{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Database schema expansion for Quick Dump queue and medical features",
        "description": "Add quick_dumps table and extend cards/notes tables to support medical list processing, partial credit, response time tracking, and connection suggestions",
        "details": "Extend database.ts to add:\n- quick_dumps table: id (UUID), content (TEXT), extractionStatus (pending/processing/completed), createdAt, processedAt\n- cards table additions: cardType (enum: 'qa'|'cloze'|'vignette'), parentListId (for medical list items), listPosition (integer)\n- review_logs additions: responseTimeMs (INTEGER), partialCreditScore (REAL 0-1)\n- connections table: id, sourceNoteId, targetNoteId, semanticScore (REAL), createdAt\n- Add indexes: idx_quick_dumps_status, idx_cards_cardType, idx_connections_sourceNoteId\n- Update DbCard, DbNote, DbReviewLog interfaces\n- Add migration logic to update existing schema without data loss\n- Create new query functions: quickDumpQueries.{getAll, insert, update, delete}, connectionQueries.{getAll, insert}\n- Update IPC handlers in ipc-handlers.ts for new operations",
        "testStrategy": "Verify schema creation with SQLite browser, test all CRUD operations, confirm indexes exist via EXPLAIN QUERY PLAN, validate migration preserves existing data",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create quick_dumps table schema and migration logic",
            "description": "Add quick_dumps table to database.ts with proper schema, types, and version migration from user_version=1 to user_version=2",
            "dependencies": [],
            "details": "In electron/database.ts: Add DbQuickDump interface with id, content, extractionStatus, createdAt, processedAt fields. Add QuickDumpRow interface and parseQuickDumpRow helper. In initDatabase(), add schema version migration check. Migration creates quick_dumps table and idx_quick_dumps_status index. Update user_version to 2.",
            "status": "pending",
            "testStrategy": "Open existing database, verify migration runs only once, confirm quick_dumps table exists with correct columns using SQLite browser, verify index via EXPLAIN QUERY PLAN",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Extend cards table with medical feature columns",
            "description": "Add cardType, parentListId, and listPosition columns to cards table through schema migration",
            "dependencies": [
              1
            ],
            "details": "In electron/database.ts: Update DbCard and CardRow interfaces to include cardType, parentListId, listPosition. In migration block, add ALTER TABLE statements for new columns. Create indexes idx_cards_cardType and idx_cards_parentListId. Update parseCardRow and cardQueries methods.",
            "status": "pending",
            "testStrategy": "Verify ALTER TABLE succeeds, confirm new columns exist with correct defaults, test cardQueries.insert with cardType='vignette', verify indexes via EXPLAIN QUERY PLAN",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Extend review_logs table with performance tracking columns",
            "description": "Add responseTimeMs and partialCreditScore columns to review_logs table for tracking review performance metrics",
            "dependencies": [
              1
            ],
            "details": "In electron/database.ts: Update DbReviewLog and ReviewLogRow interfaces to include responseTimeMs and partialCreditScore. In migration block, add ALTER TABLE statements with CHECK constraint for partialCreditScore (0-1 range). Update reviewLogQueries.insert. Update src/types/index.ts ReviewLog interface.",
            "status": "pending",
            "testStrategy": "Verify ALTER TABLE succeeds, test reviewLogQueries.insert with responseTimeMs=1500 and partialCreditScore=0.75, verify CHECK constraint rejects invalid values",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create connections table for semantic note relationships",
            "description": "Add connections table to store semantic relationships between notes with similarity scores",
            "dependencies": [
              1
            ],
            "details": "In electron/database.ts: Add DbConnection and ConnectionRow interfaces and parseConnectionRow helper. In migration block, create connections table with foreign key constraints to notes table with CASCADE delete. Create three indexes: idx_connections_sourceNoteId, idx_connections_targetNoteId, idx_connections_semanticScore.",
            "status": "pending",
            "testStrategy": "Verify connections table created with foreign key constraints, test CASCADE delete, confirm all indexes exist via EXPLAIN QUERY PLAN, verify semanticScore accepts REAL values",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement query functions and IPC handlers for new tables",
            "description": "Create quickDumpQueries and connectionQueries objects, add corresponding IPC handlers in ipc-handlers.ts, and expose via preload.ts",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "In electron/database.ts: Add quickDumpQueries with getAll, getByStatus, insert, update, delete methods. Add connectionQueries with getAll, getBySourceNote, getByTargetNote, insert, delete methods. In electron/ipc-handlers.ts: Add IPC handlers for quickDumps and connections operations. In electron/preload.ts: Expose quickDumps and connections API. In src/types/electron.d.ts: Add QuickDump and Connection interfaces and extend ElectronAPI.",
            "status": "pending",
            "testStrategy": "Test quickDumpQueries operations, verify IPC roundtrip from renderer, test connectionQueries methods, verify all handlers return IpcResult format, test error handling",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-05T00:05:06.017Z"
      },
      {
        "id": "2",
        "title": "AI service integration layer",
        "description": "Create local-first AI service with Ollama auto-detection, cloud fallbacks (OpenAI, DeepSeek, Anthropic), concept extraction, card format suggestions, and medical content processing",
        "details": "Create electron/ai-service.ts with local-first, cloud-ready architecture:\n\nPROVIDER LAYER:\n- Auto-detect Ollama on localhost:11434 (preferred - free, private)\n- OpenAI-compatible SDK with configurable baseURL (supports Ollama, OpenAI, DeepSeek)\n- Anthropic SDK as secondary provider (different API format)\n- Provider presets: { ollama, openai, deepseek, anthropic } with baseURL, model, timeout\n- Environment config: AI_PROVIDER, AI_BASE_URL, AI_API_KEY, AI_MODEL\n- Configurable timeout: 10s for local models, 3s for cloud APIs\n\nCORE FUNCTIONS:\n- extractConcepts(content: string): Promise<ExtractedConcept[]> - analyzes pasted content, returns array of {text, conceptType, confidence, suggestedFormat: 'qa'|'cloze'}\n- validateCard(card: {front, back}): Promise<ValidationResult> - checks minimum information principle, pattern-matching detection, multi-fact warnings\n- detectMedicalList(content: string): Promise<{isList: boolean, listType: 'differential'|'procedure'|'algorithm', items: string[]}>\n- convertToVignette(listItem: string, context: string): Promise<{vignette: string, cloze: string}>\n- suggestTags(content: string): Promise<string[]> - medical domain tag suggestions\n- findRelatedNotes(content: string, existingNotes: DbNote[]): Promise<{noteId: string, similarity: number}[]> - semantic similarity\n\nINFRASTRUCTURE:\n- Retry logic with exponential backoff (3 attempts max)\n- Cache results for 5 minutes to reduce API costs\n- Create src/types/ai.ts for all AI-related interfaces\n- Provider health check on startup",
        "testStrategy": "Unit tests with mock API responses for Ollama and cloud providers. Test provider auto-detection. Test timeout behavior (10s local, 3s cloud). Test fallback when Ollama unavailable. Verify <3s response for cloud, <10s for local. Test error handling and retries. Validate medical terminology recognition.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create AI provider abstraction and type definitions",
            "description": "Set up local-first provider detection, OpenAI-compatible SDK configuration, and all AI-related types",
            "dependencies": [],
            "details": "Create src/types/ai.ts with interfaces:\n- AIProviderConfig {type: 'openai-compatible'|'anthropic', baseURL, apiKey, model, timeout}\n- ProviderPreset for ollama, openai, deepseek, anthropic\n- ExtractedConcept {id, text, conceptType, confidence, suggestedFormat: 'qa'|'cloze'}\n- ValidationResult {isValid, warnings[], suggestions[]}\n- MedicalListDetection {isList, listType, items[]}\n- VignetteConversion {vignette, cloze}\n- SemanticMatch {noteId, similarity}\n\nCreate electron/ai-service.ts:\n- PROVIDER_PRESETS constant with configs for ollama (localhost:11434), openai, deepseek, anthropic\n- detectProvider(): auto-detect Ollama via fetch to localhost:11434/api/tags, fallback to env vars\n- initializeClient(): create OpenAI SDK instance with detected/configured baseURL\n- getClient(): singleton accessor for AI client\n- Environment vars: AI_PROVIDER, AI_BASE_URL, AI_API_KEY, AI_MODEL\n\nFollow patterns from fsrs-service.ts for modular exports.",
            "status": "done",
            "testStrategy": "Test Ollama auto-detection when running. Test fallback to env vars when Ollama unavailable. Test each provider preset initializes correctly. Verify type definitions compile. Mock fetch for Ollama detection tests.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T01:12:15.700Z"
          },
          {
            "id": 2,
            "title": "Implement concept extraction and card validation functions",
            "description": "Build extractConcepts and validateCard functions with provider-aware timeout and retry logic",
            "dependencies": [
              1
            ],
            "details": "In electron/ai-service.ts:\n\nHelper functions:\n- withTimeout<T>(promise, ms): Promise.race wrapper for enforcing timeouts\n- withRetry<T>(fn, maxRetries, backoffMs): exponential backoff retry wrapper\n- callAI(prompt, systemPrompt?): unified function that uses detected provider\n\nCore functions:\n- extractConcepts(content: string): Promise<ExtractedConcept[]> - send medical content to AI, parse JSON response into concept array with suggestedFormat (qa/cloze)\n- validateCard(card: {front, back}): Promise<ValidationResult> - check minimum information principle, detect pattern-matching cues, warn about multi-fact cards\n\nTimeout strategy:\n- Use provider.timeout from config (10s for Ollama, 3s for cloud)\n- Retry 3 times with exponential backoff (500ms, 1000ms, 2000ms)\n\nPrompts:\n- Create PROMPTS constant with system prompts for each function\n- Include medical context in prompts to help smaller local models",
            "status": "done",
            "testStrategy": "Unit tests with mock responses for Ollama and cloud. Test timeout respects provider config. Test retry with simulated failures. Test with medical content samples. Verify JSON parsing handles malformed responses gracefully.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T01:15:33.268Z"
          },
          {
            "id": 3,
            "title": "Implement medical list detection and vignette conversion",
            "description": "Create functions to detect medical lists and convert list items to clinical vignettes",
            "dependencies": [
              1,
              2
            ],
            "details": "In electron/ai-service.ts, implement detectMedicalList(content: string): Promise<{isList: boolean, listType: 'differential'|'procedure'|'algorithm', items: string[]}> to identify medical lists. Implement convertToVignette(listItem: string, context: string): Promise<{vignette: string, cloze: string}> to transform list items into patient scenarios with demographics, presentation, and key findings. Each vignette must be independently answerable without sibling context. Follow the timeout and retry patterns established in subtask 2.",
            "status": "done",
            "testStrategy": "Test with real medical lists: 'DDx for acute MI', '5 steps of BLS', 'Algorithm for trauma management'. Verify each vignette is clinically realistic and independently answerable. Validate list type detection accuracy.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T01:17:15.711Z"
          },
          {
            "id": 4,
            "title": "Implement tag suggestions and semantic similarity search",
            "description": "Build AI-powered tag suggestion and note similarity matching functions",
            "dependencies": [
              1,
              2
            ],
            "details": "In electron/ai-service.ts:\n\n- suggestTags(content: string): Promise<string[]> - analyze content and return medical domain tags (cardiology, pharmacology, pathophysiology, etc.). Use callAI helper with appropriate prompt.\n\n- findRelatedNotes(content: string, existingNotes: DbNote[]): Promise<SemanticMatch[]> - for MVP, use simple keyword/TF-IDF matching rather than vector embeddings (embeddings require separate API call or local model). Compare content against existing notes and return similarity scores.\n\nNote: Vector embeddings can be added post-MVP for better semantic matching. For now, keyword-based similarity is sufficient and works offline with Ollama.\n\nBoth functions use provider-aware timeout from subtask 1.",
            "status": "done",
            "testStrategy": "Test suggestTags with cardiology, procedure, pharmacology content. Test findRelatedNotes returns meaningful matches. Verify functions work with both local and cloud providers.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T01:19:43.555Z"
          },
          {
            "id": 5,
            "title": "Add caching layer and IPC handlers for AI service",
            "description": "Implement 5-minute result caching, provider status, and integrate AI service with IPC layer",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Caching layer in electron/ai-service.ts:\n- AICache class with Map<string, {result: T, expires: number}>\n- hashContent(content: string): string - simple hash for cache key\n- getCached<T>(key): T | null - return if not expired\n- setCache<T>(key, result, ttlMs = 300000): void - 5 min default\n\nIPC handlers in electron/ipc-handlers.ts:\n- ai:getProviderStatus - returns {provider, model, isLocal, isConnected}\n- ai:extractConcepts\n- ai:validateCard\n- ai:detectMedicalList\n- ai:convertToVignette\n- ai:suggestTags\n- ai:findRelatedNotes\n\nPreload API in electron/preload.ts - add ai: {...} namespace\n\nTypes in src/types/electron.d.ts - extend ElectronAPI with ai methods\n\nFollow IpcResult<T> wrapper pattern from existing handlers.",
            "status": "done",
            "testStrategy": "Test cache hit/miss with identical content. Test cache expiration after 5 min. Test ai:getProviderStatus returns correct provider info. Integration tests with mock AI. Verify all IPC returns IpcResult<T> format.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T01:22:53.351Z"
          }
        ],
        "updatedAt": "2026-01-05T01:22:53.351Z"
      },
      {
        "id": "3",
        "title": "AI-guided capture interface",
        "description": "Replace manual capture interface with AI concept extraction workflow: paste → highlight → confirm → format suggestion → save",
        "details": "Refactor src/components/capture/CaptureInterface.tsx:\n- Add state: pastedContent, extractedConcepts[], selectedConcepts (Set<id>), processingState\n- Phase 1: Paste area (Textarea) with auto-detect on paste\n- Phase 2: Call ai-service.extractConcepts, show loading (<3s), render concepts with checkboxes\n- Phase 3: For each selected concept, show AI format suggestion (cloze/Q&A) with reasoning, allow override\n- Phase 4: Real-time validation warnings using validateCard for each concept\n- Phase 5: Batch create cards + auto-create parent Note with source content\n- Medical list detection: if detectMedicalList returns true, show conversion prompt before proceeding\n- Add ConceptCheckbox component: displays concept text, checkbox, format badge, edit capability\n- Add ValidationWarning component: pattern-matching alert, minimum info violation, multi-fact warning\n- Update IPC to call window.api.ai.extractConcepts, window.api.ai.validateCard\n- Keyboard: Tab to navigate concepts, Space to toggle selection, Enter to proceed\n- Auto-save draft to localStorage every 5s (crash recovery)\n- Show 'Saved ✓' toast <500ms after successful save",
        "testStrategy": "E2E test: paste medical content → verify concepts extracted → select/deselect → override format → save → verify cards created with correct FSRS defaults. Test validation warnings appear for problematic cards. Verify <20s total workflow time.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add paste detection and content state management",
            "description": "Implement paste area UI with auto-detection and state management for pasted content, extracted concepts, selected concepts, and processing state",
            "dependencies": [],
            "details": "Refactor CaptureInterface.tsx to add new state: pastedContent (string), extractedConcepts (array of ExtractedConcept objects), selectedConcepts (Set<string>), processingState (enum: 'idle'|'extracting'|'validating'|'saving'). Replace existing multi-card draft interface with a single Textarea for paste input. Add onPaste handler to auto-trigger concept extraction. Implement localStorage auto-save every 5s for crash recovery using useEffect with debounce. Add ExtractedConcept type definition in src/types/index.ts with fields: id, text, conceptType, confidence, suggestedFormat ('qa'|'cloze'), validationWarnings.",
            "status": "done",
            "testStrategy": "Test paste event triggers extraction, verify state updates correctly, test localStorage recovery after simulated crash, verify debounced auto-save works within 5s window",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T14:29:44.663Z"
          },
          {
            "id": 2,
            "title": "Create concept extraction display with checkboxes",
            "description": "Build ConceptCheckbox component to display extracted concepts with selection, format badges, and edit capability",
            "dependencies": [
              1
            ],
            "details": "Create src/components/capture/ConceptCheckbox.tsx component that accepts props: concept (ExtractedConcept), isSelected (boolean), onToggle (callback), onEdit (callback). Display concept.text with checkbox, format badge showing 'Q&A' or 'Cloze' based on suggestedFormat, and confidence indicator. Add inline edit mode triggered by click or keyboard shortcut. Style with shadcn/ui components (Checkbox, Badge, Card). Implement keyboard navigation: Tab to move between concepts, Space to toggle selection. Update CaptureInterface to render list of ConceptCheckbox components after extraction completes, mapping extractedConcepts array.",
            "status": "done",
            "testStrategy": "Test checkbox toggles selection state, verify Tab navigation works, test Space key toggles selection, verify inline edit updates concept text, test format badge displays correct type",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T14:29:44.666Z"
          },
          {
            "id": 3,
            "title": "Implement AI concept extraction integration",
            "description": "Wire up AI service calls for concept extraction with loading states and error handling",
            "dependencies": [
              1
            ],
            "details": "Update CaptureInterface to call window.api.ai.extractConcepts(pastedContent) when paste detected. Add AI API types to electron.d.ts: extend ElectronAPI interface with ai: { extractConcepts: (content: string) => Promise<IpcResult<ExtractedConcept[]>>, validateCard: (card: {front: string, back: string}) => Promise<IpcResult<ValidationResult>>, detectMedicalList: (content: string) => Promise<IpcResult<{isList: boolean}>> }. Show loading state with spinner and 'Extracting concepts...' message (<3s timeout). Handle errors gracefully with toast notification. Add medical list detection: call detectMedicalList before extractConcepts, if true show dialog prompting conversion to clinical vignettes or overlapping cloze format.",
            "status": "done",
            "testStrategy": "Mock AI service responses, verify loading shows <3s, test error handling displays toast, verify medical list detection shows conversion prompt, test timeout behavior",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T14:29:44.669Z"
          },
          {
            "id": 4,
            "title": "Add format suggestion display and validation warnings",
            "description": "Create ValidationWarning component and integrate real-time card validation for each selected concept",
            "dependencies": [
              2,
              3
            ],
            "details": "Create src/components/capture/ValidationWarning.tsx component displaying pattern-matching alerts, minimum information violations, multi-fact warnings with warning icon and descriptive text. For each selected concept in CaptureInterface, show AI format suggestion (cloze/Q&A) with reasoning text explaining why that format was chosen. Add override dropdown to allow user to change format. Call window.api.ai.validateCard for each selected concept, passing generated {front, back} based on current format. Display ValidationWarning components inline with each concept showing real-time validation results. Use Alert component from shadcn/ui for warnings. Update ExtractedConcept type to include formatReasoning: string field.",
            "status": "done",
            "testStrategy": "Test validation warnings appear for problematic cards, verify format reasoning displays correctly, test format override changes validation results, verify real-time validation on concept selection changes",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T14:29:44.671Z"
          },
          {
            "id": 5,
            "title": "Implement batch card creation and auto-note generation",
            "description": "Create batch card creation flow with auto-generated parent Note and save feedback within 500ms",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Update CaptureInterface handleCreateCards function to: 1) Create parent Note with title from first 50 chars of pastedContent, content = full pastedContent, generate UUID noteId. 2) For each selected concept, create CardWithFSRS with proper FSRS defaults (stability=0, difficulty=0, state=0, reps=0, lapses=0, dueDate=now) and link to parent noteId. 3) Call useAppStore.addNote first, then batch call addCard for all selected concepts. 4) Show 'Saved ✓' toast with <500ms after successful save using toast.success. 5) Add Enter keyboard shortcut to proceed from concept selection to save. 6) Clear pastedContent and reset state after successful save. Ensure total workflow <20s from paste to save.",
            "status": "done",
            "testStrategy": "E2E test: paste medical content → verify concepts extracted → select/deselect multiple → override format → save → verify cards created with correct FSRS defaults and parent Note. Test save feedback appears <500ms. Verify total workflow completes <20s.",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T14:29:44.672Z"
          }
        ],
        "updatedAt": "2026-01-05T14:29:44.672Z"
      },
      {
        "id": "4",
        "title": "Medical list processing and clinical vignette generation",
        "description": "Convert medical lists (differentials, procedures) into clinical vignettes and overlapping cloze cards to prevent sibling contamination",
        "details": "Create src/lib/medical-processor.ts:\n- processMedicalList(content: string, listType): Promise<ProcessedCard[]>\n- For 'differential' type: generate individual clinical vignettes per item (e.g., '5 causes of chest pain' → 5 separate patient scenarios)\n- For 'procedure' type: create overlapping cloze deletions (step N shows context from N-1 and N+1)\n- For 'algorithm' type: similar to procedure with decision tree context\n- Each vignette includes: patient demographics, presentation, key findings\n- Use ai-service.convertToVignette for generation\n- Store parentListId to link related cards\n- Add MedicalListConverter component for preview before creation\n- Integration in CaptureInterface: detect list → show preview → confirm → batch create\n- Ensure zero sibling contamination: each card is independently answerable\n- Add cardType field to distinguish vignettes from standard Q&A",
        "testStrategy": "Test with real medical lists: 'DDx for acute MI', '5 steps of BLS'. Verify each card is independently answerable without seeing siblings. Check overlapping cloze maintains context. Validate vignettes are clinically realistic.",
        "priority": "high",
        "dependencies": [
          "2",
          "3"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement list detection and parsing in medical processor",
            "description": "Create src/lib/medical-processor.ts with list detection logic that identifies medical lists (differential diagnoses, procedures, algorithms) from pasted content and parses them into structured data",
            "dependencies": [],
            "details": "Create src/lib/medical-processor.ts with these functions:\n- detectListType(content: string): 'differential' | 'procedure' | 'algorithm' | null - Detect if content contains a medical list using pattern matching (e.g., numbered items, 'DDx', 'steps', 'causes')\n- parseListItems(content: string): string[] - Extract individual list items from the content\n- Add TypeScript types: MedicalListType, ParsedListItem { text: string, order: number }\n- Handle various list formats: numbered (1., 2.), bulleted (-, *), implicit lists\n- Return null for non-list content to bypass medical processing",
            "status": "pending",
            "testStrategy": "Test with medical list samples: '5 causes of chest pain:\\n1. MI\\n2. PE\\n3. Pneumothorax\\n4. GERD\\n5. Costochondritis'. Verify detectListType returns 'differential'. Test procedure steps, algorithm content. Verify non-list content returns null.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build differential diagnosis to clinical vignette converter",
            "description": "Implement processDifferentialList function that converts each differential diagnosis item into an independent clinical vignette with patient demographics, presentation, and key findings",
            "dependencies": [
              1
            ],
            "details": "In src/lib/medical-processor.ts, add:\n- processDifferentialList(items: string[], parentTitle: string): ProcessedCard[] - Generate clinical vignette for each diagnosis\n- Each ProcessedCard contains: { front: string (clinical scenario), back: string (diagnosis), cardType: 'vignette', parentListId: string (UUID for grouping), metadata: { demographics, presentation, keyFindings } }\n- Vignette structure: 'A [age] [gender] presents with [symptoms]. Exam shows [findings]. Most likely diagnosis?'\n- Back contains the diagnosis item text\n- Use placeholder text for now (AI integration in next task dependency)\n- Generate parentListId once per list to link all derived cards\n- Ensure each vignette is independently answerable (no 'not X from previous card' patterns)\n<info added on 2026-01-05T15:44:19.421Z>\nI need to analyze the codebase structure to provide a more informed update about this subtask's deferral. Let me explore the relevant files.Based on my codebase analysis, I can see that:\n\n1. The vignette conversion functionality already exists in `electron/ai-service.ts:771` with `convertToVignette()` function\n2. Medical list detection is implemented in `electron/ai-service.ts:715` with `detectMedicalList()` \n3. The CaptureInterface already calls `detectMedicalList()` at line 116 and shows a toast notification\n4. Both functions are fully wired through the IPC layer (preload.ts, ipc-handlers.ts, electron.d.ts)\n5. No `src/lib/medical-processor.ts` file exists yet - it was planned but not created\n6. The current implementation uses direct AI service calls rather than a dedicated medical processor module\n\nThe user wants to defer clinical vignette generation to post-MVP and instead use overlapping cloze for all list types. This means the existing `convertToVignette()` and `processDifferentialList()` functionality should NOT be implemented in the MVP, and overlapping cloze should be used uniformly for procedures, algorithms, and differentials.\n\nARCHITECTURE CHANGE: Vignette functions (convertToVignette at electron/ai-service.ts:771, detectMedicalList toast suggestion at src/components/capture/CaptureInterface.tsx:116-122) exist but will NOT be used in MVP workflow. All medical lists (differential, procedure, algorithm) will use overlapping cloze generation with shuffled context instead. This simplifies the capture flow and allows vignette enhancement to be validated post-launch. The processDifferentialList function and src/lib/medical-processor.ts module creation are cancelled for MVP scope. Task 4.3 (overlapping cloze generator) becomes the unified solution for all list types.\n</info added on 2026-01-05T15:44:19.421Z>",
            "status": "deferred",
            "testStrategy": "Test with '5 causes of chest pain' list. Verify 5 separate ProcessedCard objects generated. Check each has unique clinical vignette front, diagnosis back, same parentListId, cardType='vignette'. Verify vignettes don't reference each other (sibling independence).",
            "parentId": "undefined",
            "updatedAt": "2026-01-05T15:44:19.524Z"
          },
          {
            "id": 3,
            "title": "Create overlapping cloze generator for procedures and algorithms",
            "description": "Implement processProcedureList and processAlgorithmList functions that create overlapping cloze deletions where step N shows context from steps N-1 and N+1 to maintain procedural flow",
            "dependencies": [
              1
            ],
            "details": "In src/lib/medical-processor.ts, add:\n- processProcedureList(items: string[], parentTitle: string): ProcessedCard[] - Create overlapping cloze for procedures\n- processAlgorithmList(items: string[], parentTitle: string): ProcessedCard[] - Similar with decision tree context\n- Overlapping cloze format: front shows step N-1 (if exists) + '[...blank...]' + step N+1 (if exists), back shows step N\n- Example: Step 2 of BLS -> front: '1. Check responsiveness\\n[...?...]\\n3. Call for help', back: '2. Open airway'\n- Edge cases: first step shows only N+1, last step shows only N-1\n- Each card has cardType='cloze', parentListId for grouping\n- Ensure contextual continuity prevents 'naked' questions\n<info added on 2026-01-05T15:45:40.548Z>\nI need to analyze the codebase to understand the current implementation and provide accurate, code-specific guidance for this expanded subtask.Now let me check if there's a medical-processor.ts file or any AI concept extraction patterns:Let me check the AI types and see the extractConcepts implementation:Based on my analysis of the codebase, I can now generate the subtask update text:\n\nSCOPE CHANGE: All list types (differential, procedure, algorithm) now use unified overlapping cloze approach instead of clinical vignettes. Clinical vignettes deferred to post-MVP.\n\nOVERLAPPING CLOZE MECHANISM:\nEach list item becomes individual FSRS-scheduled card. Front shows cloze target with 2-3 shuffled sibling items as context. Shuffle happens at review time to prevent pattern matching.\n\nDATABASE SCHEMA ALREADY IN PLACE:\nSchema updates complete in electron/database.ts:28-31 - cardType, parentListId (UUID linking all cards from same list), listPosition (original index for List Health view). Indexes idx_cards_cardType and idx_cards_parentListId exist. Default cardType='qa' in migration.\n\nCREATE src/lib/medical-processor.ts:\n- processAllListTypes(items: string[], parentTitle: string, listType: 'differential' | 'procedure' | 'algorithm'): ProcessedCard[] - Unified processor for all list types\n- generateOverlappingCloze(item: string, index: number, allItems: string[], listType: string): { front: string, back: string } - Core cloze generator\n- For each item at position N, front shows: shuffled selection of 2-3 sibling items + cloze placeholder for target item. Back shows target item.\n- Shuffle logic: at review time in review interface, randomize context items order and selection to maximize anti-pattern-matching\n- Edge handling: lists with <3 items show all available siblings, single-item lists become standard Q&A\n- Each card: cardType='list-cloze', parentListId=uuid() shared across all cards from same list, listPosition=original index\n- Return ProcessedCard[] array with all FSRS defaults (state=0, reps=0, dueDate=now)\n\nINTEGRATION POINT:\nHook into existing AI concept extraction at src/components/capture/CaptureInterface.tsx:286-288 where cardType is already set. When list detected, call processAllListTypes instead of standard card creation.\n\nVALIDATION:\nEnsure cards independently answerable without seeing siblings. Verify context items shuffled across reviews. Test with 'DDx for chest pain', 'BLS steps', 'Sepsis algorithm'.\n</info added on 2026-01-05T15:45:40.548Z>",
            "status": "pending",
            "testStrategy": "Test with '5 steps of BLS' procedure list. Verify 5 cloze cards generated with overlapping context. Check step 1 shows step 2 context, step 3 shows steps 2 and 4, step 5 shows step 4. Verify cardType='cloze' and consistent parentListId.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Build MedicalListConverter preview component with batch creation",
            "description": "Create src/components/capture/MedicalListConverter.tsx component that displays preview of generated cards before creation, allows editing/removal, and handles batch card creation with parentListId tracking",
            "dependencies": [
              2,
              3
            ],
            "details": "Create MedicalListConverter component:\n- Props: { content: string, listType: MedicalListType, onConfirm: (cards: ProcessedCard[]) => void, onCancel: () => void }\n- Use medical-processor to generate preview cards on mount\n- Display cards in grid/list with Card component from shadcn/ui\n- Show card type badge (Vignette/Cloze), front preview (truncated), edit/remove buttons\n- Allow inline editing of front/back text before confirmation\n- Confirm button calls onConfirm with final ProcessedCard[] array\n- Show parentListId indicator to visualize card relationships\n- Use existing UI components: Card, Button, Badge, ScrollArea from src/components/ui/",
            "status": "pending",
            "testStrategy": "Render MedicalListConverter with differential list. Verify preview shows 5 vignette cards with front/back preview. Test edit functionality updates card text. Test remove reduces card count. Click confirm, verify onConfirm receives ProcessedCard[] with all edits applied.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate medical list workflow into CaptureInterface with database schema updates",
            "description": "Extend CaptureInterface to detect medical lists on paste, show MedicalListConverter preview, batch create cards with new cardType and parentListId fields, and update database schema to support these fields",
            "dependencies": [
              4
            ],
            "details": "Update multiple files:\n1. src/types/index.ts: Add cardType?: 'standard' | 'vignette' | 'cloze' and parentListId?: string to Card interface\n2. electron/database.ts: Add cardType and parentListId columns to cards table schema (nullable for backward compatibility)\n3. src/components/capture/CaptureInterface.tsx:\n   - Add state: detectedListType, showMedicalConverter (boolean)\n   - On paste event, call medical-processor.detectListType\n   - If list detected, set detectedListType and show MedicalListConverter modal\n   - MedicalListConverter onConfirm: batch create cards via useAppStore.addCard with cardType and parentListId fields\n   - Update handleCreateCards to preserve cardType/parentListId when provided\n   - Add parentListId to CardWithFSRS construction in CaptureInterface.tsx:63-80\n4. electron/database.ts DbCard interface: Add cardType and parentListId fields\n5. Maintain backward compatibility: cardType defaults to 'standard', parentListId defaults to null for non-list cards",
            "status": "pending",
            "testStrategy": "E2E test: Paste 'DDx for MI: 1. Unstable angina 2. PE 3. Aortic dissection' into CaptureInterface. Verify MedicalListConverter modal appears. Review generated vignettes, confirm creation. Query database, verify 3 cards created with cardType='vignette' and matching parentListId. Test normal card creation still works (cardType='standard', parentListId=null).",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-05T15:44:19.524Z"
      },
      {
        "id": "5",
        "title": "Zero-decision review interface (eliminate grading buttons)",
        "description": "Replace 4-button grading (Again/Hard/Good/Easy) with Show Answer → Continue flow, letting FSRS auto-schedule based on response time",
        "details": "Refactor src/components/review/ReviewInterface.tsx:\n- REMOVE: All 4 rating buttons (Again/Hard/Good/Easy), interval previews, keyboard shortcuts 1-4\n- NEW flow: Show Answer (Space) → Continue (Space/Enter)\n- Track response time: startTime when card shown, calculateResponseTime when Continue pressed\n- Auto-determine rating based on response time algorithm:\n  * <5s: Rating.Easy (immediate recall)\n  * 5-15s: Rating.Good (confident recall)\n  * 15-30s: Rating.Hard (struggled)\n  * >30s OR explicit 'Forgot' button: Rating.Again\n- Add optional 'I forgot this' button (visible after Show Answer) for explicit Again rating\n- Update scheduleCardReview to accept responseTimeMs instead of explicit rating\n- Store responseTimeMs in review_logs for future FSRS personalization\n- Update UI: remove all grading complexity, show only card content and single action button\n- Maintain progress bar, session stats, keyboard shortcuts (Space, Escape)\n- Update useAppStore.scheduleCardReview signature",
        "testStrategy": "Review 10 cards with varying response times, verify correct auto-rating. Test 'I forgot' button forces Again rating. Confirm <100ms FSRS calculation. Verify cleaner UI with zero user decisions except Show/Continue.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Remove grading buttons and interval preview UI",
            "description": "Remove the 4-button grading interface (Again/Hard/Good/Easy) and interval previews from ReviewInterface.tsx",
            "dependencies": [],
            "details": "In src/components/review/ReviewInterface.tsx (lines 313-371): Remove the 4 rating buttons and their interval previews. Remove the keyboard hints section (lines 375-382) showing kbd shortcuts 1-4. Remove the intervals state (line 19) and the useEffect that fetches intervals (lines 52-66). Clean up the FormattedIntervals import from types (line 5). This simplifies the UI to prepare for the zero-decision flow.",
            "status": "done",
            "testStrategy": "Verify ReviewInterface renders without grading buttons, intervals state is removed, and no interval fetching occurs on answer show",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T00:32:13.586Z"
          },
          {
            "id": 2,
            "title": "Implement Show Answer → Continue flow with response time tracking",
            "description": "Replace grading buttons with single Continue button and add response time tracking from card shown to Continue pressed",
            "dependencies": [
              1
            ],
            "details": "In ReviewInterface.tsx: Add state for responseStartTime (timestamp when card shown). In handleShowAnswer (line 68), record start time. Replace the grading buttons section with: Show Answer button (when !answerVisible), then Continue button with Space/Enter shortcuts (when answerVisible). Add 'I forgot this' button (optional, visible after answer shown, styled as secondary/warning). Update keyboard shortcuts (lines 144-188): Keep Space for Show Answer, add Space/Enter for Continue when answer visible, remove 1-4 rating shortcuts. Calculate responseTimeMs = Date.now() - responseStartTime when Continue pressed.\n<info added on 2026-01-05T22:00:47.677Z>\nI'll analyze the codebase to provide specific implementation guidance for the timeout detection requirement.Based on my analysis of the codebase, here is the new implementation requirement to add:\n\nTIMEOUT DETECTION: Add responseStartTime state (number | null) initialized to null. Add isPaused state (boolean) initialized to false. When card shown, set responseStartTime = Date.now(). Add useEffect with 60-second timer: when answerVisible === false and responseStartTime !== null, start setTimeout(60000ms). If timer fires before answer shown, set isPaused = true. When Continue pressed, calculate rawResponseTimeMs = Date.now() - responseStartTime. If isPaused === true, store responseTimeMs as null or -1 (indicating interrupted session) instead of rawResponseTimeMs. Clear timeout in cleanup. This prevents using artificially long times (user stepped away, phone call, etc.) for FSRS grading. The isPaused flag signals to auto-rating algorithm (subtask 5.3) to handle interrupted cards differently (default to Rating.Hard or prompt user).\n</info added on 2026-01-05T22:00:47.677Z>",
            "status": "done",
            "testStrategy": "Test Space shows answer, Space/Enter continues to next card, response time is accurately measured from card display to Continue press",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T00:32:13.591Z"
          },
          {
            "id": 3,
            "title": "Implement auto-rating algorithm based on response time",
            "description": "Create algorithm to automatically determine Rating based on response time: <5s=Easy, 5-15s=Good, 15-30s=Hard, >30s=Again",
            "dependencies": [
              2
            ],
            "details": "In ReviewInterface.tsx, create calculateAutoRating function: takes responseTimeMs, returns Rating value. Algorithm: if (ms < 5000) return Rating.Easy; else if (ms < 15000) return Rating.Good; else if (ms < 30000) return Rating.Hard; else return Rating.Again. Handle 'I forgot' button override: if clicked, always return Rating.Again regardless of time. Integrate into Continue button handler: calculate rating, then call scheduleCardReview with auto-determined rating.\n<info added on 2026-01-05T22:01:17.377Z>\nI'll analyze the codebase to understand the current review interface implementation before generating the subtask update.Based on my analysis, the current ReviewInterface.tsx shows the traditional 4-button grading system (Again/Hard/Good/Easy) and doesn't have response time tracking or isPaused state implemented yet. The subtask needs to add logic for handling paused states.\n\nHere's the new text to append:\n\nWhen isPaused=true (response time exceeds 60000ms), bypass auto-rating algorithm entirely. Instead, render a manual grade selector UI: display text 'Looks like you stepped away. How did this feel?' above four manual rating buttons (Easy/Good/Hard/Again). User must explicitly select a rating. Only when isPaused=false should the calculateAutoRating function execute and return the auto-determined rating based on the time thresholds. Update Continue button handler logic: check isPaused state first, if true show manual selector, if false apply auto-rating algorithm.\n</info added on 2026-01-05T22:01:17.377Z>",
            "status": "done",
            "testStrategy": "Review cards with controlled response times: <5s confirms Easy, 5-15s confirms Good, 15-30s confirms Hard, >30s confirms Again. Test 'I forgot' button forces Again rating",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T00:32:13.595Z"
          },
          {
            "id": 4,
            "title": "Update scheduleReview to accept and store responseTimeMs",
            "description": "Modify scheduleReview function and review_logs table to accept and persist response time in milliseconds",
            "dependencies": [
              3
            ],
            "details": "In electron/fsrs-service.ts (line 82), update scheduleReview signature to accept optional responseTimeMs parameter. In electron/database.ts, update DbReviewLog interface to include responseTimeMs field. Update review_logs table schema to add responseTimeMs column (INTEGER). In reviewLogQueries.insert, store responseTimeMs. Update electron/ipc-handlers.ts reviews:schedule handler to accept responseTimeMs from renderer. Update src/stores/useAppStore.ts scheduleCardReview to accept and pass responseTimeMs. Update electron/preload.ts and src/types/electron.d.ts to reflect new signature.",
            "status": "done",
            "testStrategy": "Verify review_logs table includes responseTimeMs column, scheduleCardReview persists response time, query review_logs confirms responseTimeMs stored correctly for future FSRS personalization",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T00:32:13.598Z"
          },
          {
            "id": 5,
            "title": "UI cleanup and zero-decision interface verification",
            "description": "Clean up UI after removing grading buttons, verify progress bar and session stats work, ensure FSRS performance <100ms",
            "dependencies": [
              4
            ],
            "details": "In ReviewInterface.tsx:\n- Verify progress bar (lines 254-272) remains functional with auto-grading flow\n- Verify session stats (reviewedCount) update correctly after each card\n- Ensure Continue button is prominent and clearly labeled\n- Remove ALL remaining references to manual grading (variable names, comments, unused imports)\n- Rename handleRating to handleContinue for clarity\n- Verify session completion flow (lines 224-238) works with auto-rating\n- Test FSRS calculation performance: ensure scheduleReview completes in <100ms\n- Confirm Escape key navigation to capture view still works\n- Verify 'I forgot this' button styling matches secondary/warning variant",
            "status": "done",
            "testStrategy": "Complete 5-card review session, verify: progress bar updates correctly, session stats accurate, FSRS <100ms per card, Escape returns to capture, no console errors or warnings about removed grading code",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T00:32:13.600Z"
          },
          {
            "id": 6,
            "title": "Auto-grade animation feedback display",
            "description": "Show brief visual feedback of the auto-determined grade (Easy/Good/Hard/Again) for ~1 second after Continue pressed, before advancing to next card",
            "dependencies": [
              5
            ],
            "details": "Create GradeFeedback component or inline state in ReviewInterface.tsx:\n- After Continue pressed and rating calculated, display feedback before advancing\n- Show rating label with colored icon:\n  * Easy: green checkmark, 'Easy' text\n  * Good: blue circle, 'Good' text\n  * Hard: amber warning icon, 'Hard' text\n  * Again: red X icon, 'Again' text\n- Use existing color variants from shadcn/ui (success, primary, warning, destructive)\n- Display duration: 1000ms using setTimeout\n- Position: centered below card content, near where Continue button was\n- Animation: fade-in on appear, can use Tailwind animate-fade-in or similar\n- After 1000ms, auto-advance to next card (call existing next card logic)\n- State: add showingFeedback boolean, currentGrade Rating enum value\n- Prevent user interaction during feedback display (disable buttons, ignore Space)",
            "status": "done",
            "testStrategy": "Review card, press Continue, verify: feedback appears with correct grade color/icon, displays for ~1 second, then advances to next card. Test all 4 grade levels appear correctly. Verify Space key is disabled during feedback.",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T00:32:13.603Z"
          },
          {
            "id": 7,
            "title": "Quick override mechanism during feedback display",
            "description": "Allow user to override auto-grade during the 1-second feedback window using keyboard shortcuts (1-4) or clicking alternative grades",
            "dependencies": [
              6
            ],
            "details": "Extend the feedback display phase with override capability:\n- During 1000ms feedback window, listen for keyboard shortcuts:\n  * 1 = Again, 2 = Hard, 3 = Good, 4 = Easy (matches Anki convention)\n- Show subtle override hint: small text 'Press 1-4 to adjust' below the grade display\n- Alternative: show all 4 grade options as small clickable chips/buttons, current auto-grade highlighted\n- If user presses key or clicks override:\n  * Cancel the 1000ms auto-advance timeout\n  * Apply the user-selected rating instead of auto-grade\n  * Immediately advance to next card (no second feedback display)\n- If no action within 1000ms, auto-grade applies as normal\n- IMPORTANT: Block Space/Enter during feedback window to prevent accidental skip\n- Track override usage: increment overrideCount in session stats for analytics\n- Log to console (dev only): 'Auto-grade: Good, User override: Hard' for debugging\n- Consider: store override events in review_logs for future analysis (optional field: wasOverridden boolean)",
            "status": "done",
            "testStrategy": "Review card, see feedback, press '2' to override to Hard - verify override applies and card advances. Test all 4 override keys work. Test clicking override chips. Verify Space/Enter blocked during feedback. Check override count increments in session stats.",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T00:32:13.607Z"
          }
        ],
        "updatedAt": "2026-01-06T00:32:13.607Z"
      },
      {
        "id": "6",
        "title": "Auto note creation and bidirectional linking",
        "description": "Automatically create Notes from extraction sessions and maintain bidirectional links between cards and source notes",
        "details": "Update src/components/capture/CaptureInterface.tsx:\n- On card creation, auto-generate parent Note with: title (first line or 'Extraction - [timestamp]'), content (original pasted text), cardIds (all created card IDs), tags (AI-suggested + user tags)\n- Update all created cards with noteId referencing parent Note\n- Add NoteContext component in ReviewInterface: shows source note title, 'View source' link, related cards count\n- Create src/components/notes/NoteViewer.tsx: displays note content, derived cards list, metadata (created date, tags)\n- Add navigation: clicking noteId in review → opens NoteViewer modal\n- NoteViewer shows all cards derived from note with edit/delete actions\n- Update store: when deleting card, remove cardId from parent note.cardIds; when deleting note, orphan cards (noteId = '')\n- Ensure referential integrity: transactions for create/update/delete operations\n- Add 'View source' button in ReviewInterface below card content",
        "testStrategy": "Create cards from paste → verify Note created with correct content and cardIds. Review card → click 'View source' → verify Note displayed with all derived cards. Delete card → verify cardIds updated in Note. Delete Note → verify cards orphaned.",
        "priority": "medium",
        "dependencies": [
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "7",
        "title": "Connection suggestions and semantic linking",
        "description": "AI surfaces related notes during extraction based on semantic similarity to help build knowledge connections",
        "details": "Create src/components/capture/ConnectionSuggestions.tsx:\n- During extraction, call ai-service.findRelatedNotes(pastedContent, existingNotes)\n- Display gentle highlight banner: 'Related notes found: [NoteTitle1], [NoteTitle2]' with similarity scores\n- Allow one-click linking: creates connection record in connections table\n- Show in ReviewInterface: 'Related cards' section below answer (if connections exist)\n- Implement semantic search using embeddings (OpenAI text-embedding-3-small or local model)\n- Cache embeddings in notes table (add embeddingVector TEXT field - JSON array)\n- Compute cosine similarity for matching (threshold: >0.7 for suggestions)\n- ConnectionSuggestions displays top 3 matches with preview snippet\n- Add 'Link' button to create bidirectional connection\n- Store semantic score for future ranking/filtering\n- Integration: non-intrusive, doesn't block workflow, dismissible",
        "testStrategy": "Create note about 'Acute MI pathophysiology', then paste related content about 'Chest pain differential'. Verify suggestions appear with correct similarity scores. Link notes → verify connection in DB. Review card from linked note → verify 'Related cards' appears.",
        "priority": "medium",
        "dependencies": [
          "2",
          "3",
          "6"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "8",
        "title": "Extraction queue for Quick Dump processing",
        "description": "Process Quick Dumps later with same AI-assisted extraction workflow, show queue indicator, batch processing support",
        "details": "Create src/components/queue/ExtractionQueue.tsx:\n- Fetch all quick_dumps with extractionStatus='pending' on mount\n- Display list: title (first 50 chars), timestamp, 'Process' button\n- Clicking 'Process' opens guided extraction flow (same as CaptureInterface but pre-populated with quick_dump.content)\n- After successful extraction, update quick_dump.extractionStatus='completed', processedAt=now\n- Add queue indicator in Header: badge showing pending count, click to open ExtractionQueue\n- Support batch selection: checkboxes, 'Process all selected' button\n- Update QuickDumpModal to save to quick_dumps table instead of notes\n- Add IPC handlers: quickDumps:getAll, quickDumps:updateStatus\n- Integration in AppLayout: new route/view for queue\n- Queue view shows: content preview, creation date, actions (Process/Delete)\n- Empty state: 'No pending extractions. Use Quick Dump (Cmd+Shift+S) to capture content.'",
        "testStrategy": "Save 3 Quick Dumps → verify queue shows 3 pending. Process one → verify AI extraction flow works. Complete extraction → verify count decreases. Test batch processing. Verify quick dumps don't interfere with normal notes.",
        "priority": "medium",
        "dependencies": [
          "1",
          "2",
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "9",
        "title": "Tag-based organization with AI suggestions",
        "description": "Implement tag-only organization (no folders), AI suggests medical domain tags during capture, search/filter by tags",
        "details": "Update CaptureInterface and types:\n- During extraction, call ai-service.suggestTags(content) to get medical domain tags\n- Display suggested tags as chips with '+' to add, show common tags: 'cardiology', 'pharmacology', 'pathophysiology', etc.\n- Allow manual tag entry: input field with autocomplete from existing tags\n- Create src/components/tags/TagManager.tsx: displays all tags with card counts, filter view\n- Add tag filtering to SearchBar: clicking tag shows all cards/notes with that tag\n- Store tags in cards.tags and notes.tags (JSON array)\n- Create tag index query: SELECT DISTINCT json_each.value as tag FROM cards, json_each(cards.tags)\n- Add TagChip component: displays tag with color based on domain (cardio=red, pharm=blue, etc.)\n- TagManager shows tag hierarchy (domain-based grouping)\n- No folder UI, purely tag-based navigation\n- Search supports tag:cardiology syntax",
        "testStrategy": "Paste cardiology content → verify AI suggests 'cardiology', 'acute coronary syndrome'. Add custom tag → verify autocomplete. Filter by tag → verify only matching cards shown. Test tag persistence across sessions.",
        "priority": "medium",
        "dependencies": [
          "2",
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "10",
        "title": "Global search with <200ms response time",
        "description": "Implement fast unified search across cards and notes with FTS5 full-text search, Cmd+F shortcut, instant results",
        "details": "Add to database.ts:\n- Create FTS5 virtual table: CREATE VIRTUAL TABLE cards_fts USING fts5(id, front, back, content=cards, content_rowid=rowid)\n- Create FTS5 for notes: CREATE VIRTUAL TABLE notes_fts USING fts5(id, title, content, content=notes, content_rowid=rowid)\n- Add triggers to keep FTS tables synchronized with cards/notes on INSERT/UPDATE/DELETE\n- Create searchQueries.search(query: string): {cards: DbCard[], notes: DbNote[], responseTimeMs: number}\n- Use FTS5 MATCH for fast search (ORDER BY rank)\n- Update src/components/layout/SearchBar.tsx:\n  * Real-time search as user types (debounced 150ms)\n  * Display unified results: cards (front snippet) + notes (title)\n  * Keyboard navigation: Up/Down arrows, Enter to open\n  * Cmd+F global shortcut to focus search\n  * Results dropdown with highlights, max 10 results\n  * Show 'View all X results' if >10 matches\n- Add search highlighting using FTS5 snippet() function\n- Log search performance, warn if >200ms\n- Support tag: prefix for tag filtering\n- Create SearchResults component for full results page",
        "testStrategy": "Index 1000 cards, search 'acute MI', verify <200ms response. Test special chars, partial matches. Verify Cmd+F focuses search. Test keyboard navigation. Confirm FTS tables stay synchronized on card create/update/delete.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create FTS5 virtual tables for cards, notes, and source_items with sync triggers",
            "description": "Implement FTS5 full-text search tables with automatic synchronization triggers for all searchable content",
            "dependencies": [],
            "details": "In electron/database.ts, create three FTS5 virtual tables:\n\n1. cards_fts: CREATE VIRTUAL TABLE cards_fts USING fts5(id UNINDEXED, front, back, content=cards, content_rowid=rowid)\n2. notes_fts: CREATE VIRTUAL TABLE notes_fts USING fts5(id UNINDEXED, title, content, content=notes, content_rowid=rowid)\n3. source_items_fts: CREATE VIRTUAL TABLE source_items_fts USING fts5(id UNINDEXED, title, rawContent, sourceName, content=source_items, content_rowid=rowid)\n\nAdd sync triggers for each table (INSERT, UPDATE, DELETE) to keep FTS tables synchronized:\n- cards_fts_insert, cards_fts_update, cards_fts_delete\n- notes_fts_insert, notes_fts_update, notes_fts_delete\n- source_items_fts_insert, source_items_fts_update, source_items_fts_delete\n\nImplement in initializeDatabase() function and add to schema migration (v4). Test with existing data to verify initial population and trigger synchronization.",
            "status": "pending",
            "testStrategy": "Create test card, verify FTS entry exists. Update card front text, verify FTS updated. Delete card, verify FTS entry removed. Repeat for notes and source_items. Query SELECT * FROM cards_fts WHERE cards_fts MATCH 'test' to verify indexing.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement searchQueries.search() with filter support and performance tracking",
            "description": "Create unified search function that queries all FTS tables with filter options (all/cards/notes/inbox) and tracks response time",
            "dependencies": [
              1
            ],
            "details": "In electron/database.ts, create new searchQueries object with search() method:\n\ninterface SearchResult {\n  cards: DbCard[];\n  notes: DbNote[];\n  sourceItems: DbSourceItem[];\n  responseTimeMs: number;\n  totalMatches: number;\n}\n\ntype SearchFilter = 'all' | 'cards' | 'notes' | 'inbox';\n\nsearchQueries.search(query: string, filter: SearchFilter = 'all', limit: number = 10): SearchResult\n\nImplementation:\n1. Start performance timer (Date.now())\n2. Sanitize query (escape FTS5 special chars unless using tag: prefix)\n3. Handle tag: prefix → convert to tags LIKE query\n4. Handle # prefix → convert to tag: automatically\n5. Use FTS5 MATCH with ORDER BY rank for relevance\n6. Query based on filter:\n   - 'all': Query all three FTS tables\n   - 'cards': Only cards_fts\n   - 'notes': Only notes_fts  \n   - 'inbox': Only source_items_fts WHERE status='inbox'\n7. Join FTS results with main tables to get full records\n8. Calculate responseTimeMs, log warning if >200ms\n9. Return SearchResult with results + metadata\n\nUse snippet() function for highlighted excerpts in future subtasks.",
            "status": "pending",
            "testStrategy": "Index 1000 cards. Search 'acute MI' with filter='all', verify <200ms. Search with filter='cards', verify only cards returned. Search 'tag:cardiology', verify tag filtering. Search '#nephrology', verify # converts to tag. Test special characters (quotes, parentheses). Verify responseTimeMs accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add IPC handler, preload exposure, and TypeScript types for search",
            "description": "Wire up search functionality through IPC layer with proper type definitions",
            "dependencies": [
              2
            ],
            "details": "1. In electron/database.ts, export SearchResult interface and searchQueries object\n\n2. In electron/ipc-handlers.ts, add handler:\nipcMain.handle('search:query', async (_, query: string, filter: SearchFilter, limit?: number): Promise<IpcResult<SearchResult>> => {\n  try {\n    const startTime = Date.now();\n    const results = searchQueries.search(query, filter, limit);\n    return success(results);\n  } catch (error) {\n    return failure(error);\n  }\n});\n\n3. In electron/preload.ts, expose search API:\nsearch: {\n  query: (query: string, filter: SearchFilter, limit?: number) => ipcRenderer.invoke('search:query', query, filter, limit)\n}\n\n4. In src/types/index.ts, add:\nexport interface SearchResult {\n  cards: Card[];\n  notes: Note[];\n  sourceItems: SourceItem[];\n  responseTimeMs: number;\n  totalMatches: number;\n}\nexport type SearchFilter = 'all' | 'cards' | 'notes' | 'inbox';\n\n5. In src/types/electron.d.ts, add to ElectronAPI:\nsearch: {\n  query: (query: string, filter: SearchFilter, limit?: number) => Promise<IpcResult<SearchResult>>;\n}\n\nTest by calling window.api.search.query() from browser console.",
            "status": "pending",
            "testStrategy": "Open DevTools console, call await window.api.search.query('test', 'all'). Verify IpcResult<SearchResult> returned. Test with filter='cards', verify type safety. Confirm TypeScript compilation with no errors.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Build basic SearchBar component with debounced query and results dropdown",
            "description": "Replace placeholder SearchBar with functional component that performs real-time search with debounced input and displays top 10 results in dropdown",
            "dependencies": [
              3
            ],
            "details": "Update src/components/layout/SearchBar.tsx:\n\n1. Add state:\n   - query: string (search input)\n   - results: SearchResult | null\n   - isOpen: boolean (dropdown visibility)\n   - activeFilter: SearchFilter (default 'all')\n   - isLoading: boolean\n\n2. Implement debounced search (150ms) using useEffect:\n   - Clear timeout on query change\n   - If query.length >= 2, call window.api.search.query(query, activeFilter, 10)\n   - Update results state, set isOpen=true\n   - If query empty, clear results, set isOpen=false\n\n3. Results dropdown UI:\n   - Position absolute below input\n   - Max height with scroll\n   - Show cards section (if results.cards.length > 0)\n   - Show notes section (if results.notes.length > 0)\n   - Show source items section (if results.sourceItems.length > 0)\n   - Each result shows:\n     * Type icon (Card/Note/Inbox indicator)\n     * Title/front text (truncated)\n     * onClick → navigate to item\n   - Show 'No results' if totalMatches === 0\n   - Close dropdown on click outside (useEffect with document listener)\n\n4. Performance display:\n   - Show responseTimeMs in dropdown footer\n   - Warn if >200ms (yellow indicator)\n\nDefer keyboard navigation and filter chips to later subtasks. Focus on core search functionality.",
            "status": "pending",
            "testStrategy": "Type 'acu' in SearchBar, wait 150ms, verify API called with 'acu'. Type 'acute', verify only one API call made (debounce working). Click result, verify navigation. Click outside dropdown, verify closes. Test with empty query, verify dropdown hidden. Check DevTools Network tab for response time display accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add horizontal filter chips UI with toggleable selection and count badges",
            "description": "Implement filter chip buttons (All/Cards/Notes/Inbox) above search results with active state and result counts",
            "dependencies": [
              4
            ],
            "details": "Update src/components/layout/SearchBar.tsx to add filter chips:\n\n1. Create FilterChip component:\n   - Props: label, count, isActive, onClick\n   - Visual: horizontal pill button with badge\n   - Active state: purple background (bg-primary)\n   - Inactive state: gray background (bg-secondary/20)\n   - Count badge: small circle with number (only show if count > 0)\n\n2. Filter chips container:\n   - Position: inside results dropdown, above results list\n   - Layout: flex row, gap-2, horizontal scroll if needed\n   - Chips: ['All', 'Cards', 'Notes', 'Inbox']\n   - Display counts from SearchResult:\n     * All: results.totalMatches\n     * Cards: results.cards.length\n     * Notes: results.notes.length\n     * Inbox: results.sourceItems.length (filtered by status='inbox')\n\n3. Click behavior:\n   - Set activeFilter state to clicked filter\n   - Trigger new search with updated filter\n   - Update active chip visual state\n   - Only one chip active at a time\n\n4. Styling:\n   - Use Tailwind classes for consistent look\n   - Match DougHub purple theme for active state\n   - Smooth transition on filter change\n   - Mobile responsive (horizontal scroll on small screens)\n\n5. Update search logic:\n   - Pass activeFilter to window.api.search.query()\n   - Re-render results based on filter\n   - Preserve scroll position when switching filters",
            "status": "pending",
            "testStrategy": "Search 'test', verify 'All' chip active and shows total count. Click 'Cards' chip, verify only cards displayed and chip highlights purple. Verify count badges update correctly. Click 'All' again, verify all results return. Test with query that has 0 cards, verify Cards chip shows count=0. Test horizontal scroll on mobile viewport.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement keyboard navigation and Ctrl+F global shortcut",
            "description": "Add full keyboard support for search including Up/Down/Enter/Escape navigation and global Ctrl+F hotkey to focus search bar",
            "dependencies": [
              5
            ],
            "details": "Update src/components/layout/SearchBar.tsx for keyboard controls:\n\n1. Global Ctrl+F shortcut:\n   - Use useEffect with document keydown listener\n   - Check if (e.ctrlKey && e.key === 'f')\n   - e.preventDefault() to override browser search\n   - Focus SearchBar input using ref\n   - Show dropdown if results exist\n   - Cleanup listener on unmount\n\n2. Dropdown navigation state:\n   - Add selectedIndex: number state (-1 = none selected)\n   - Calculate flattened results array (cards + notes + sourceItems)\n   - Highlight selected result with different background color\n\n3. Arrow key navigation (when dropdown open):\n   - ArrowDown: increment selectedIndex, wrap to 0 if at end, prevent default scroll\n   - ArrowUp: decrement selectedIndex, wrap to last if at -1, prevent default\n   - Auto-scroll selected item into view (element.scrollIntoView({ block: 'nearest' }))\n\n4. Enter key behavior:\n   - If dropdown open && selectedIndex >= 0: navigate to selected item, close dropdown\n   - If dropdown closed: open dropdown (trigger search if query exists)\n\n5. Escape key:\n   - First press: clear selection (selectedIndex = -1)\n   - Second press: close dropdown (isOpen = false)\n   - Third press: blur input and clear query\n\n6. Mouse interaction:\n   - onMouseEnter on result item: update selectedIndex\n   - Preserve keyboard selection on mouse move\n\n7. Accessibility:\n   - Add aria-activedescendant to input\n   - Add role='listbox' to dropdown\n   - Add role='option' to each result",
            "status": "pending",
            "testStrategy": "Press Ctrl+F from review view, verify SearchBar focused. Type query, press ArrowDown 3 times, verify 3rd result highlighted. Press Enter, verify navigated to that item. Press Escape, verify dropdown closes. Test ArrowUp wrapping to last item. Test from any view (Capture, Review, Settings). Verify browser default Ctrl+F prevented. Test mouse hover updates selectedIndex.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Add search term highlighting in result snippets",
            "description": "Implement visual highlighting of matched search terms in result text using FTS5 snippet() function",
            "dependencies": [
              4
            ],
            "details": "Implement search term highlighting:\n\n1. Update searchQueries.search() in electron/database.ts:\n   - Use FTS5 snippet() function to generate highlighted excerpts:\n     snippet(cards_fts, 1, '<mark>', '</mark>', '...', 32)\n   - snippet params: (table, column_idx, start_tag, end_tag, ellipsis, max_tokens)\n   - For cards: snippet front (col 1) and back (col 2)\n   - For notes: snippet title (col 1) and content (col 2)\n   - For source_items: snippet title (col 1) and rawContent (col 2)\n   - Add snippet fields to SearchResult interface:\n     cards: Array<DbCard & { frontSnippet?: string, backSnippet?: string }>\n     notes: Array<DbNote & { titleSnippet?: string, contentSnippet?: string }>\n     sourceItems: Array<DbSourceItem & { titleSnippet?: string, contentSnippet?: string }>\n\n2. Update SearchBar.tsx result rendering:\n   - Create HighlightedText component:\n     * Takes html string with <mark> tags\n     * Uses dangerouslySetInnerHTML with sanitization\n     * Styles <mark> with yellow background (bg-yellow-200 dark:bg-yellow-800)\n   - Render frontSnippet instead of plain front text\n   - Show backSnippet in smaller gray text below\n   - Fallback to original text if snippet undefined\n\n3. Security:\n   - Sanitize HTML before rendering (use DOMPurify or strip all tags except <mark>)\n   - Only allow <mark> tags through sanitization\n\n4. Styling:\n   - Highlighted terms: bg-yellow-200/50 for light mode, bg-yellow-700/30 for dark\n   - Bold text for emphasis\n   - Smooth visual distinction from surrounding text",
            "status": "pending",
            "testStrategy": "Search 'myocardial', verify 'myocardial' highlighted in yellow in card results. Search 'acute MI', verify both terms highlighted. Search special characters like 'C. difficile', verify proper highlighting. Test XSS by searching '<script>alert(1)</script>', verify script tags stripped but search works. Verify ellipsis (...) appears for long content.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Create full SearchResults page component with sections and view-all navigation",
            "description": "Build dedicated full-page search results view with categorized sections, showing all matches beyond the 10-result dropdown limit",
            "dependencies": [
              7
            ],
            "details": "Create src/components/search/SearchResultsPage.tsx:\n\n1. Page structure:\n   - Full-height layout with SearchBar at top (pre-filled with query)\n   - Tabbed sections: All / Cards / Notes / Inbox\n   - Each section shows paginated results (20 per page)\n   - Performance metrics in header (X results in Yms)\n\n2. Result sections:\n   - Cards section:\n     * Grid layout (2 columns on desktop, 1 on mobile)\n     * Card preview with front/back text\n     * Due date badge, tag pills\n     * Click → navigate to ReviewInterface with that card\n   - Notes section:\n     * List layout with title + content preview\n     * Click → navigate to note detail view\n   - Inbox section:\n     * Source items with type icon, title, source name\n     * Status badge (inbox/processed/curated)\n     * Click → navigate to source item detail\n\n3. Empty states:\n   - No results: 'No matches found for \"query\"'\n   - Suggestions: 'Try different keywords' / 'Use tag:name to filter'\n\n4. View-all link in SearchBar dropdown:\n   - Show at bottom if totalMatches > 10\n   - Text: 'View all {totalMatches} results →'\n   - onClick: navigate to /search?q={query}&filter={activeFilter}\n   - Preserve filter state in URL\n\n5. URL state management:\n   - Read query params: ?q=searchterm&filter=cards\n   - Update URL on filter/query change (pushState)\n   - Allow browser back/forward navigation\n\n6. Performance:\n   - Virtualized list for >100 results (react-window)\n   - Debounced query updates (same 150ms as SearchBar)\n   - Show loading skeleton while searching\n\n7. Routing:\n   - Add /search route to App.tsx router\n   - Make SearchResultsPage accessible from SearchBar 'View all' link",
            "status": "pending",
            "testStrategy": "Search query with 50 results, click 'View all 50 results' in dropdown. Verify navigation to /search?q=... page. Verify all 50 results displayed in sections. Click 'Cards' tab, verify filter applied and URL updates. Click browser back button, verify returns to previous view. Test pagination with 100+ results. Verify performance <200ms even on full results page. Test empty state with nonsense query.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "11",
        "title": "Command palette (Cmd+K)",
        "description": "Keyboard-first command palette for search, create, navigate with fuzzy matching and recent actions",
        "details": "Update src/components/modals/CommandPalette.tsx:\n- Cmd+K global shortcut (prevent default browser behavior)\n- Fuzzy search using Fuse.js or similar\n- Command categories:\n  * Navigation: 'Go to Capture', 'Go to Review', 'Go to Queue', 'Go to Settings'\n  * Actions: 'Create new card', 'Quick Dump', 'Process queue item'\n  * Search: 'Search cards...', 'Search notes...', 'Filter by tag...'\n- Recent actions history (last 5), stored in localStorage\n- Keyboard-only operation: Up/Down to navigate, Enter to execute, Esc to close\n- Display keyboard shortcuts next to commands\n- Integrate with AppView navigation via useAppStore.setCurrentView\n- Add command execution handlers for each action\n- Show command descriptions and shortcuts\n- Support typeahead: typing narrows results\n- Visual: centered modal, 500px width, max 8 results visible",
        "testStrategy": "Press Cmd+K → verify palette opens. Type 'cap' → verify 'Go to Capture' suggested. Press Enter → verify navigation. Test all commands execute correctly. Verify recent actions persist across sessions. Test keyboard navigation.",
        "priority": "low",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "12",
        "title": "Response time tracking and FSRS personalization",
        "description": "Track response times during review, store in review_logs, use for interval adjustments and domain-specific scheduling",
        "details": "Extend FSRS integration in electron/fsrs-service.ts:\n- Add responseTimeMs parameter to scheduleReview()\n- Store in DbReviewLog.responseTimeMs (already added in task 1)\n- Create analytics module: electron/fsrs-analytics.ts\n  * calculatePersonalParams(cardId: string): Promise<FSRSParams> - analyzes review history for card\n  * getDomainStats(domain: string): {avgResponseTime: number, accuracy: number}\n  * getOptimizedRetention(userId: string): number - personalized retention rate based on performance\n- Track domain from card tags (first tag = primary domain)\n- Implement forgetting curve optimization: adjust retention target per domain based on historical performance\n- Add settings UI to display stats: average response time by domain, accuracy trends\n- Use response time intelligence to detect cards needing reformulation (consistently >30s)\n- Create IPC handlers for analytics queries\n- Privacy: all data stays local, no external tracking",
        "testStrategy": "Review 20 cards with varying response times. Query analytics → verify response times stored. Check domain stats → verify cardiology vs pharmacology tracked separately. Test personalized params applied to new reviews.",
        "priority": "low",
        "dependencies": [
          "1",
          "5"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "13",
        "title": "Partial credit tracking for medical list cards",
        "description": "Track individual list item recall for clinical vignettes (3/5 correct schedules differently than 1/5)",
        "details": "Update ReviewInterface for cardType='vignette':\n- After showing answer, display checklist of list items (e.g., 5 causes)\n- User checks items they recalled correctly\n- Calculate partialCreditScore = recalled / total (0.0 to 1.0)\n- Store in review_logs.partialCreditScore\n- Modify FSRS rating calculation:\n  * score >= 0.8: Rating.Good\n  * score 0.5-0.79: Rating.Hard\n  * score < 0.5: Rating.Again\n- Add PartialCreditInput component: renders checkboxes for each list item\n- Extract list items from card.back (parse numbered list or use stored listItems metadata)\n- Update card update flow to preserve individual item scheduling\n- Alternative: split list cards into individual cards with parentListId for granular tracking\n- Add UI indicator in review: 'Recalled 3/5 items'\n- Store itemScores as JSON in review_log for detailed analytics",
        "testStrategy": "Review vignette card with 5 causes. Check 3/5 recalled → verify score=0.6, rating=Hard. Review same card later → verify interval reflects partial credit. Test edge cases: 0/5, 5/5.",
        "priority": "low",
        "dependencies": [
          "4",
          "5"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "14",
        "title": "Data persistence and crash recovery",
        "description": "Auto-save on every action, session restoration, hourly backups retained 7 days, zero data loss guarantee",
        "details": "Enhance electron/database.ts:\n- All mutations already use WAL mode (enabled in initDatabase)\n- Add backup system: electron/backup-service.ts\n  * Hourly backup: copy doughub.db → backups/doughub-[timestamp].db\n  * Retention: delete backups older than 7 days\n  * Use electron's app.getPath('userData') + '/backups/'\n  * Backup on app close (before quit)\n- Session state persistence:\n  * Store currentView, reviewQueue, currentQueueIndex in localStorage\n  * Restore on app launch via useAppStore.initialize()\n- Auto-save indicators:\n  * Show 'Saved ✓' toast <500ms after DB write\n  * Status bar indicator: 'Last saved: 2s ago'\n- Crash recovery:\n  * On app launch, check for incomplete transactions (SQLite handles this)\n  * Restore draft from localStorage if exists\n- Test data integrity: simulate crash (kill process), verify no data loss\n- Add 'Restore from backup' option in Settings",
        "testStrategy": "Create card, kill app process immediately → restart → verify card saved. Simulate power loss during review → verify queue restored. Test hourly backup creation. Verify 7-day retention cleanup. Test restore from backup.",
        "priority": "medium",
        "dependencies": [
          "1"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create backup-service.ts for automated database backups",
            "description": "Implement hourly backup system with 7-day retention policy",
            "dependencies": [],
            "details": "Create electron/backup-service.ts module with functions for: 1) createBackup() - copies doughub.db to backups/doughub-[ISO timestamp].db using fs.copyFileSync, 2) cleanupOldBackups() - deletes backups older than 7 days using fs.readdirSync/fs.statSync/fs.unlinkSync, 3) initBackupService() - sets up hourly interval (setInterval 3600000ms) and returns cleanup function, 4) backupOnQuit() - one-time backup before app close. Use app.getPath('userData') for paths. Add proper error handling and logging for all operations.",
            "status": "pending",
            "testStrategy": "Unit tests: verify backup file creation with correct timestamp format, test retention cleanup removes files >7 days old while preserving recent ones. Integration test: run app for test period, verify hourly backups created, simulate 8-day period and verify only 7 days retained.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate backup service into main.ts lifecycle",
            "description": "Hook backup service into Electron app initialization and shutdown",
            "dependencies": [
              1
            ],
            "details": "Update electron/main.ts to: 1) Import initBackupService and backupOnQuit from backup-service.ts, 2) Call initBackupService() after database initialization in app.whenReady() and store cleanup function, 3) Add IPC handler 'backup:restore' in ipc-handlers.ts that takes backup filename, closes current DB, copies backup to doughub.db, and reinitializes DB, 4) Update preload.ts to expose backup.restore(filename) API, 5) Modify before-quit handler to call backupOnQuit() before closeDatabase(). Ensure proper sequencing and error handling.",
            "status": "pending",
            "testStrategy": "Test app startup creates hourly backup timer, verify before-quit creates final backup. Test restore: create backup, add test card, restore backup, verify card doesn't exist. Test error handling: attempt restore with invalid filename.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement session state persistence with localStorage",
            "description": "Persist and restore currentView and review queue state across app restarts",
            "dependencies": [],
            "details": "Update src/stores/useAppStore.ts: 1) Add persistSessionState() function that saves currentView, current card ID during review to localStorage under 'doughub-session' key as JSON, 2) Add restoreSessionState() function to read from localStorage and return parsed session object or null, 3) Modify initialize() method to call restoreSessionState() and apply to store if exists, 4) Add middleware to useAppStore that calls persistSessionState() whenever currentView changes or review progresses, 5) Add clearSessionState() for clean shutdown. Use try-catch for localStorage access with fallback for quota errors.",
            "status": "pending",
            "testStrategy": "Test persistence: navigate to review, close app (kill process), restart, verify returns to review at same position. Test with no saved state (first launch). Test localStorage quota exceeded scenario. Test invalid JSON in localStorage handled gracefully.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add auto-save toast indicators with <500ms feedback",
            "description": "Show 'Saved ✓' toast after successful DB writes within 500ms",
            "dependencies": [
              3
            ],
            "details": "Update useAppStore.ts actions (addCard, updateCard, deleteCard, addNote, updateNote, deleteNote): 1) Import { toast } from '@/hooks/use-toast', 2) After successful DB operation (result.data exists), call toast({ title: 'Saved ✓', duration: 2000 }) - ensure this happens <500ms after write completes, 3) On error, show error toast with result.error message, 4) Optional: Add lastSaved timestamp to store state and display 'Last saved: Xs ago' in a status indicator component. Use performance.now() to measure DB operation timing in development mode and log warning if >500ms.",
            "status": "pending",
            "testStrategy": "Test toast appears within 500ms of card creation (use performance.now() to measure). Test error toast on DB failure (simulate by passing invalid data). Verify toast doesn't block UI interaction. Test rapid successive saves don't create toast spam (TOAST_LIMIT=1 helps).",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add Settings UI for backup restoration",
            "description": "Create Settings interface with 'Restore from backup' functionality",
            "dependencies": [
              2
            ],
            "details": "Create src/components/settings/SettingsView.tsx (or update existing): 1) Add IPC handler 'backup:list' in ipc-handlers.ts that returns array of { filename, timestamp, size } from backups directory, 2) Expose window.api.backup.list() in preload.ts, 3) In Settings UI, create 'Data & Backup' section with: list of available backups (fetch via api.backup.list()), each showing formatted date/time and size, 'Restore' button per backup, 4) On restore click, show confirmation dialog (AlertDialog from shadcn/ui) warning data loss, 5) Call window.api.backup.restore(filename) and reload app state via store.initialize(), 6) Add currentView === 'settings' case in App.tsx router.",
            "status": "pending",
            "testStrategy": "Test backup list displays correctly with multiple backups. Test restore confirmation dialog appears. Test successful restore loads correct data (create card, backup, create another card, restore first backup, verify only first card exists). Test error handling for corrupted backup file.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "15",
        "title": "Evidence-based card validation system",
        "description": "Real-time warnings for pattern-matching cards, multi-fact violations, and minimum information principle enforcement",
        "details": "Create src/lib/card-validation.ts:\n- validateMinimumInformation(card): checks if card tests one concept only\n- detectPatternMatching(card): detects format recognition cues (e.g., 'Which of the following...', multiple choice artifacts)\n- detectMultiFact(card): warns if answer contains multiple independent facts\n- detectMedicalList(card): checks for list-style answers without clinical context\n- Create ValidationWarning component: displays warnings with explanation and fix suggestions\n- Integrate in CaptureInterface: show warnings as user confirms concepts\n- Use ai-service.validateCard for AI-powered detection\n- Validation categories:\n  * ERROR (blocks save): pattern-matching detected, empty fields\n  * WARNING (allows save with confirm): multi-fact, complex answer, list without context\n  * INFO: suggestions for improvement\n- Add 'Fix automatically' option for some violations (e.g., split multi-fact into multiple cards)\n- Store validation history for analytics (which warnings users dismiss most)\n- Settings: enable/disable specific validation rules",
        "testStrategy": "Create card with 'What are the 5 causes of X?' → verify list warning. Create pattern-matching card → verify error blocks save. Create multi-fact card → verify warning with split suggestion. Test auto-fix splits card correctly.",
        "priority": "medium",
        "dependencies": [
          "2",
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "16",
        "title": "Global floating capture window (Ctrl+Enter)",
        "description": "Implement floating capture window accessible via Ctrl+Enter hotkey from anywhere in the app, allowing quick card creation without breaking workflow",
        "details": "Create floating capture modal that can be triggered globally:\n- Global Ctrl+Enter keyboard shortcut that works from any view (Capture, Review, Queue, Settings)\n- Create src/components/modals/FloatingCaptureModal.tsx component\n- Modal contains full CaptureInterface with AI-guided extraction workflow\n- After successful card creation, modal automatically closes and returns user to previous view\n- Keyboard shortcuts:\n  * Ctrl+Enter: Open floating capture window\n  * Escape: Close modal without saving (with confirmation if content exists)\n  * Enter (after card creation): Save and close\n- Modal styling: centered overlay, 800px width, max-height 90vh, scrollable content\n- Preserve user's current context (e.g., review session continues where it left off)\n- Auto-focus on paste/input area when modal opens\n- Show subtle indicator in Header that Ctrl+Enter is available\n- Integration: Add global keyboard listener in App.tsx or AppLayout\n- State management: Add floatingCaptureOpen to useAppStore\n- Visual feedback: smooth fade-in/out transition (200ms)",
        "testStrategy": "From Review view, press Ctrl+Enter → verify modal opens with focus on input. Paste content → create cards → verify modal closes and returns to review at same position. Test Escape cancellation. Verify works from all views. Test keyboard navigation within modal. Confirm no duplicate shortcuts conflict.",
        "priority": "low",
        "dependencies": [
          "3"
        ],
        "status": "deferred",
        "subtasks": []
      },
      {
        "id": "17",
        "title": "Auto-start Ollama from Electron on first AI request",
        "description": "Automatically start Ollama when the app needs AI services, eliminating the friction of manually starting Ollama before using the app",
        "details": "Update electron/ai-service.ts to auto-start Ollama:\n\n1. Create ensureOllamaRunning() function:\n   - Check if Ollama is already running via detectOllamaAvailable()\n   - If not running, spawn('ollama', ['serve']) with detached: true, stdio: 'ignore'\n   - Wait for Ollama to be ready (retry detection up to 10 times with 500ms intervals)\n   - Return true if started successfully, false if failed\n\n2. Update getClient() or initializeClient():\n   - Before returning client, call ensureOllamaRunning() if provider is 'ollama'\n   - Log startup status to console\n   - If Ollama can't start, fall back to cloud provider and notify user via IPC\n\n3. Add IPC handler for AI status notifications:\n   - ai:ollamaStarted - notify renderer when Ollama auto-started\n   - ai:ollamaFailed - notify renderer when Ollama couldn't start (show toast)\n\n4. Handle Windows/macOS/Linux differences:\n   - Windows: 'ollama.exe' may need full path or PATH lookup\n   - macOS/Linux: 'ollama' should work if installed via standard methods\n   - Check for ollama in PATH first, then common install locations\n\n5. Graceful degradation:\n   - If Ollama not installed, detect and inform user\n   - Offer to use cloud API as fallback (requires API key)\n   - Don't block app startup, just disable AI features gracefully",
        "testStrategy": "Test with Ollama installed but not running → verify auto-start works. Test with Ollama already running → verify no duplicate process. Test with Ollama not installed → verify graceful fallback message. Test on Windows specifically. Verify AI extraction works after auto-start.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement ensureOllamaRunning() function with process spawning",
            "description": "Create function to check Ollama status and spawn it if not running",
            "dependencies": [],
            "details": "In electron/ai-service.ts, add:\n- Import { spawn } from 'child_process'\n- Create async function ensureOllamaRunning(): Promise<boolean>\n- First check if already running via existing detectOllamaAvailable() logic\n- If not running, spawn('ollama', ['serve'], { detached: true, stdio: 'ignore', shell: true })\n- Use process.unref() to allow parent to exit independently\n- Implement retry loop: 10 attempts, 500ms between each, checking detectOllamaAvailable()\n- Return true on success, false on failure after all retries\n- Add proper error handling for spawn failures (e.g., ollama not in PATH)",
            "status": "pending",
            "testStrategy": "Test with Ollama not running → verify spawn is called. Test with Ollama already running → verify no spawn. Test retry logic with mock delayed startup. Test error handling when ollama executable not found.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate auto-start into AI client initialization",
            "description": "Update initializeClient() to call ensureOllamaRunning() when Ollama provider is detected",
            "dependencies": [
              1
            ],
            "details": "In electron/ai-service.ts:\n- Update initializeClient() to be async if not already\n- Before creating OpenAI client for Ollama provider, call ensureOllamaRunning()\n- Log result: 'Ollama auto-started successfully' or 'Failed to start Ollama, falling back to...'\n- If ensureOllamaRunning() returns false and no cloud API key configured, set a flag for graceful degradation\n- Update getClient() to handle async initialization properly\n- Ensure first AI request waits for Ollama to be ready",
            "status": "pending",
            "testStrategy": "Test first extractConcepts() call triggers Ollama startup. Verify subsequent calls don't retry startup. Test fallback when Ollama fails to start. Verify logs show startup status.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add user notification for Ollama status via IPC",
            "description": "Notify renderer process about Ollama auto-start status so user sees feedback",
            "dependencies": [
              2
            ],
            "details": "In electron/ipc-handlers.ts:\n- Add IPC send (not handle) for 'ai:ollamaStatus' that sends { status: 'starting' | 'started' | 'failed', message: string }\n- In ai-service.ts, emit status updates during ensureOllamaRunning()\n- In renderer (CaptureInterface or App.tsx), listen for ai:ollamaStatus via window.api\n- Show toast: 'Starting local AI...' (info), 'Local AI ready' (success), 'Could not start local AI' (warning)\n- Update preload.ts to expose ai.onStatus(callback) listener\n- Update electron.d.ts with new IPC types",
            "status": "pending",
            "testStrategy": "Verify toast appears when Ollama auto-starts. Test 'failed' toast shows when Ollama unavailable. Verify no duplicate toasts on subsequent AI calls. Test listener cleanup on component unmount.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Handle cross-platform Ollama executable detection",
            "description": "Ensure Ollama can be found and started on Windows, macOS, and Linux",
            "dependencies": [
              1
            ],
            "details": "In electron/ai-service.ts:\n- Create findOllamaExecutable(): string | null function\n- Windows: Check PATH, then common locations like %LOCALAPPDATA%\\Programs\\Ollama\\ollama.exe\n- macOS: Check PATH, then /usr/local/bin/ollama, ~/.ollama/ollama\n- Linux: Check PATH, then /usr/local/bin/ollama, ~/.local/bin/ollama\n- Use which/where command as fallback to find in PATH\n- Update spawn() call to use full path when available\n- Add shell: true option for Windows compatibility\n- Log found path or 'Ollama not found in PATH or common locations'",
            "status": "pending",
            "testStrategy": "Test on Windows with Ollama installed via official installer. Test PATH detection. Test with Ollama not installed → verify appropriate error message. Verify spawn works with full path.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "18",
        "title": "Implement Quick Dump modal with Cmd+Shift+S shortcut",
        "description": "Zero-friction emergency capture modal for saving content when too exhausted to process, with global keyboard shortcut and auto-save to quick_dumps queue",
        "details": "Update src/components/modals/QuickDumpModal.tsx:\n- Add global Cmd+Shift+S (Ctrl+Shift+S Windows) keyboard listener in App.tsx or AppLayout\n- Modal contains single Textarea for paste/type with auto-focus on open\n- One-click 'Save to Queue' button (Enter shortcut)\n- No format decisions, no AI processing at capture time\n- Calls window.api.quickDumps.insert({ content, extractionStatus: 'pending' })\n- Shows 'Saved to queue ✓' toast <500ms after successful save\n- Escape to close modal\n- No validation or processing during save - pure capture\n- Modal stays open after save for rapid successive captures\n- Add 'Save & Close' vs 'Save & Continue' button options\n- Update useAppStore to track quick dump count for badge display\n- Integration: Modal component already exists, needs IPC wiring and keyboard shortcut\n\nImplementation pseudo-code:\n```typescript\n// App.tsx global listener\nuseEffect(() => {\n  const handleKeyDown = (e: KeyboardEvent) => {\n    if ((e.metaKey || e.ctrlKey) && e.shiftKey && e.key === 's') {\n      e.preventDefault();\n      setQuickDumpOpen(true);\n    }\n  };\n  window.addEventListener('keydown', handleKeyDown);\n  return () => window.removeEventListener('keydown', handleKeyDown);\n}, []);\n\n// QuickDumpModal save handler\nconst handleSave = async () => {\n  const result = await window.api.quickDumps.insert({\n    id: crypto.randomUUID(),\n    content,\n    extractionStatus: 'pending',\n    createdAt: new Date().toISOString(),\n    processedAt: null\n  });\n  if (result.data) {\n    toast.success('Saved to queue ✓');\n    setContent('');\n  }\n};\n```",
        "testStrategy": "Test Cmd+Shift+S opens modal from any view. Paste content, press Enter, verify saved to quick_dumps table with status='pending'. Verify toast appears <500ms. Test Escape closes modal. Test rapid successive saves. Verify modal focus management. Test both 'Save & Close' and 'Save & Continue' workflows.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.483Z"
      },
      {
        "id": "19",
        "title": "Build extraction queue UI with guided processing workflow",
        "description": "Display pending quick dumps with process button that opens AI-guided extraction flow, batch processing support, queue indicator in header",
        "details": "Create src/components/queue/ExtractionQueue.tsx component:\n- Fetch all quick_dumps with extractionStatus='pending' on mount via window.api.quickDumps.getAll()\n- Display list using Card components: title (first 50 chars), timestamp (relative, e.g., '2 hours ago'), content preview\n- Each item shows 'Process' button and 'Delete' button\n- 'Process' button opens AI-guided extraction flow in modal or inline:\n  * Pre-populate CaptureInterface-like flow with quick_dump.content\n  * Same AI extraction workflow: extractConcepts → checkboxes → validation → save\n  * After successful card creation, update quick_dump: extractionStatus='completed', processedAt=now\n  * Remove from pending list or move to 'Processed' section\n- Batch processing:\n  * Add checkboxes for selection\n  * 'Process Selected' button processes items sequentially\n  * Progress indicator during batch processing (Item X of Y)\n- Header badge integration:\n  * Add pending count badge to QuickActionsBar 'Queue' button\n  * Update count on quick dump create/process/delete\n  * Badge shows number, clicking navigates to queue view\n- Empty state: 'No pending extractions. Use Quick Dump (Cmd+Shift+S) when you need to capture quickly.'\n- Add view to AppLayout routing: case 'queue' in currentView switch\n\nImplementation pseudo-code:\n```typescript\nfunction ExtractionQueue() {\n  const [quickDumps, setQuickDumps] = useState<QuickDump[]>([]);\n  const [processing, setProcessing] = useState<string | null>(null);\n\n  useEffect(() => {\n    loadQuickDumps();\n  }, []);\n\n  const loadQuickDumps = async () => {\n    const result = await window.api.quickDumps.getAll();\n    if (result.data) {\n      setQuickDumps(result.data.filter(qd => qd.extractionStatus === 'pending'));\n    }\n  };\n\n  const handleProcess = async (qd: QuickDump) => {\n    setProcessing(qd.id);\n    // Open extraction modal with pre-populated content\n    // After extraction completes:\n    await window.api.quickDumps.update(qd.id, { \n      extractionStatus: 'completed',\n      processedAt: new Date().toISOString()\n    });\n    loadQuickDumps(); // Refresh list\n  };\n\n  return (\n    <div className=\"p-6\">\n      <h1>Extraction Queue</h1>\n      {quickDumps.length === 0 ? (\n        <EmptyState />\n      ) : (\n        quickDumps.map(qd => (\n          <QuickDumpItem key={qd.id} data={qd} onProcess={() => handleProcess(qd)} />\n        ))\n      )}\n    </div>\n  );\n}\n```",
        "testStrategy": "Save 3 quick dumps via Quick Dump modal. Navigate to queue view, verify 3 pending items shown. Click 'Process' on first item, verify AI extraction flow opens with pre-populated content. Complete extraction, verify item removed from pending list and status updated to 'completed'. Test batch selection with checkboxes, process 2 items, verify sequential processing. Verify header badge shows correct pending count. Test delete functionality. Verify empty state displays when queue is clear.",
        "priority": "high",
        "dependencies": [
          "18"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.514Z"
      },
      {
        "id": "20",
        "title": "Enhance review interface with source context and partial credit UI",
        "description": "Show source note context during review with 'View source' link, implement partial credit checklist for vignette cards, display related cards count",
        "details": "Update src/components/review/ReviewInterface.tsx:\n\nSOURCE CONTEXT DISPLAY:\n- Below card content, add NoteContext component showing:\n  * Source note title (or 'No source note' if orphaned)\n  * 'View source' button that opens NoteViewer modal\n  * Related cards count from same note (e.g., '3 more cards from this note')\n- Fetch note data via window.api.notes.getById(card.noteId) when card displayed\n- NoteContext styled with subtle background, small text, non-intrusive\n\nPARTIAL CREDIT FOR VIGNETTES:\n- After showing answer for cardType='vignette' cards:\n  * If card.parentListId exists, fetch all sibling cards\n  * Display checklist: 'Which items did you recall?' with checkbox per item\n  * User checks recalled items before clicking Continue\n  * Calculate partialCreditScore = recalled / total\n  * Pass to scheduleCardReview along with responseTimeMs\n- Modify auto-rating algorithm to use partial credit:\n  * If partialCreditScore provided:\n    - score >= 0.8: Rating.Good\n    - score 0.5-0.79: Rating.Hard  \n    - score < 0.5: Rating.Again\n  * Otherwise use response time algorithm from task 5\n- Add PartialCreditChecklist component:\n  * Displays list items extracted from card.back or fetched from siblings\n  * Checkboxes for each item\n  * Shows count: 'Recalled X of Y items'\n\nRELATED CARDS SECTION:\n- If card has connections (from connections table), show 'Related cards' section\n- Fetch via window.api.connections.getByCard(card.id)\n- Display related card titles with link to view\n- Gentle highlight, dismissible, doesn't block workflow\n\nImplementation pseudo-code:\n```typescript\n// NoteContext component\nfunction NoteContext({ noteId }: { noteId: string }) {\n  const [note, setNote] = useState<Note | null>(null);\n  \n  useEffect(() => {\n    if (noteId) {\n      window.api.notes.getById(noteId).then(result => {\n        if (result.data) setNote(result.data);\n      });\n    }\n  }, [noteId]);\n\n  return (\n    <div className=\"mt-4 p-2 bg-muted/50 rounded text-sm\">\n      <p>Source: {note?.title || 'Unknown'}</p>\n      {note && (\n        <>\n          <Button variant=\"link\" size=\"sm\" onClick={() => openNoteViewer(note)}>\n            View source\n          </Button>\n          <span className=\"text-muted-foreground\">\n            {note.cardIds.length - 1} more cards from this note\n          </span>\n        </>\n      )}\n    </div>\n  );\n}\n\n// Partial credit for vignettes\nconst [partialCreditScore, setPartialCreditScore] = useState<number | null>(null);\nconst [showPartialCredit, setShowPartialCredit] = useState(false);\n\nuseEffect(() => {\n  if (answerVisible && currentCard.cardType === 'vignette' && currentCard.parentListId) {\n    setShowPartialCredit(true);\n  }\n}, [answerVisible, currentCard]);\n\nconst handlePartialCreditChange = (recalled: number, total: number) => {\n  setPartialCreditScore(recalled / total);\n};\n\nconst handleContinue = () => {\n  const rating = partialCreditScore \n    ? calculatePartialCreditRating(partialCreditScore)\n    : calculateAutoRating(responseTimeMs);\n  \n  scheduleCardReview(currentCard.id, rating, responseTimeMs, partialCreditScore);\n  // ... continue to next card\n};\n```",
        "testStrategy": "Review a standard card, verify source note context displays with correct title and related card count. Click 'View source', verify NoteViewer modal opens. Review a vignette card with parentListId, verify partial credit checklist appears after showing answer. Check 3 of 5 items, verify score=0.6 calculated, rating=Hard applied. Test orphaned card (no noteId), verify graceful 'No source note' display. Test card with connections, verify 'Related cards' section appears.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.517Z"
      },
      {
        "id": "21",
        "title": "Create NoteViewer modal for source navigation",
        "description": "Build modal component to display full note content, all derived cards, metadata, and enable navigation between notes and cards during review",
        "details": "Create src/components/notes/NoteViewer.tsx modal component:\n\nDISPLAY SECTIONS:\n- Header: Note title, creation date, edit/delete buttons\n- Content section: Full note.content displayed in formatted text\n- Tags section: Badge chips for all tags with color coding\n- Derived cards section: List of all cards created from this note\n  * Show card.front preview (truncated to 100 chars)\n  * Card type badge (Standard/Vignette/Cloze)\n  * Due date indicator\n  * 'Review now' button to jump to card review\n  * 'Edit' and 'Delete' buttons per card\n- Metadata footer: Created date, last modified, card count\n\nINTERACTION:\n- Modal triggered by clicking 'View source' in ReviewInterface NoteContext\n- Can be opened from search results, card lists, anywhere noteId is available\n- 'Close' button and Escape key to dismiss\n- Navigation: clicking derived card opens it in review or edit mode\n- Edit note: opens inline editor for title and content\n- Delete note: confirmation dialog, handles orphaning cards (sets noteId='')\n\nSTATE MANAGEMENT:\n- Accept noteId prop or note object\n- Fetch full note data if only ID provided\n- Real-time updates: if note/cards modified elsewhere, reflect changes\n- Loading and error states\n\nSTYLING:\n- Large modal: 800px width, max-height 80vh, scrollable content\n- Sections clearly separated with dividers\n- Derived cards in grid layout (2 columns)\n- Use existing shadcn/ui components: Dialog, Card, Badge, ScrollArea\n\nImplementation pseudo-code:\n```typescript\ninterface NoteViewerProps {\n  noteId?: string;\n  note?: Note;\n  onClose: () => void;\n}\n\nfunction NoteViewer({ noteId, note: initialNote, onClose }: NoteViewerProps) {\n  const [note, setNote] = useState<Note | null>(initialNote || null);\n  const [cards, setCards] = useState<CardWithFSRS[]>([]);\n  const [loading, setLoading] = useState(!initialNote);\n\n  useEffect(() => {\n    const loadNote = async () => {\n      if (noteId && !initialNote) {\n        const result = await window.api.notes.getById(noteId);\n        if (result.data) {\n          setNote(result.data);\n          // Fetch all derived cards\n          const cardResults = await Promise.all(\n            result.data.cardIds.map(id => window.api.cards.getById(id))\n          );\n          setCards(cardResults.filter(r => r.data).map(r => r.data!));\n        }\n        setLoading(false);\n      }\n    };\n    loadNote();\n  }, [noteId, initialNote]);\n\n  const handleDeleteNote = async () => {\n    if (note && confirm('Delete this note? Cards will be orphaned.')) {\n      await window.api.notes.delete(note.id);\n      onClose();\n    }\n  };\n\n  return (\n    <Dialog open onOpenChange={onClose}>\n      <DialogContent className=\"max-w-3xl max-h-[80vh]\">\n        <DialogHeader>\n          <DialogTitle>{note?.title}</DialogTitle>\n          <span className=\"text-sm text-muted-foreground\">\n            {note?.createdAt && formatDate(note.createdAt)}\n          </span>\n        </DialogHeader>\n        \n        <ScrollArea className=\"h-full\">\n          {/* Content section */}\n          <div className=\"prose\">\n            <p>{note?.content}</p>\n          </div>\n\n          {/* Tags */}\n          <div className=\"flex gap-2 mt-4\">\n            {note?.tags.map(tag => <Badge key={tag}>{tag}</Badge>)}\n          </div>\n\n          {/* Derived cards */}\n          <div className=\"mt-6\">\n            <h3>Derived Cards ({cards.length})</h3>\n            <div className=\"grid grid-cols-2 gap-4\">\n              {cards.map(card => (\n                <Card key={card.id}>\n                  <CardHeader>\n                    <Badge>{card.cardType}</Badge>\n                  </CardHeader>\n                  <CardContent>\n                    <p className=\"truncate\">{card.front}</p>\n                    <Button onClick={() => reviewCard(card)}>Review now</Button>\n                  </CardContent>\n                </Card>\n              ))}\n            </div>\n          </div>\n        </ScrollArea>\n\n        <DialogFooter>\n          <Button variant=\"destructive\" onClick={handleDeleteNote}>Delete Note</Button>\n          <Button onClick={onClose}>Close</Button>\n        </DialogFooter>\n      </DialogContent>\n    </Dialog>\n  );\n}\n```",
        "testStrategy": "Open NoteViewer from review interface 'View source' link, verify full note content displays. Verify all derived cards shown in grid with correct previews. Click 'Review now' on a card, verify navigation to review at that card. Test edit note inline, save, verify changes persist. Test delete note, verify confirmation dialog, confirm deletion, verify cards orphaned (noteId=''). Test with note containing 10+ cards, verify scrollable layout. Test Escape key closes modal.",
        "priority": "medium",
        "dependencies": [
          "20"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.520Z"
      },
      {
        "id": "22",
        "title": "Implement FTS5 full-text search with <200ms performance",
        "description": "Add SQLite FTS5 virtual tables for cards and notes, create synchronized triggers, build unified search function with performance tracking",
        "details": "This is the complete implementation of task 10 which was previously defined with subtasks but not started.\n\nDATABASE LAYER (electron/database.ts):\n1. Create FTS5 virtual tables in initDatabase():\n```sql\nCREATE VIRTUAL TABLE IF NOT EXISTS cards_fts USING fts5(\n  id UNINDEXED,\n  front,\n  back,\n  content=cards,\n  content_rowid=rowid\n);\n\nCREATE VIRTUAL TABLE IF NOT EXISTS notes_fts USING fts5(\n  id UNINDEXED,\n  title,\n  content,\n  content=notes,\n  content_rowid=rowid\n);\n```\n\n2. Create synchronization triggers:\n```sql\n-- Cards INSERT trigger\nCREATE TRIGGER IF NOT EXISTS cards_fts_insert AFTER INSERT ON cards BEGIN\n  INSERT INTO cards_fts(rowid, id, front, back)\n  VALUES(NEW.rowid, NEW.id, NEW.front, NEW.back);\nEND;\n\n-- Cards UPDATE trigger\nCREATE TRIGGER IF NOT EXISTS cards_fts_update AFTER UPDATE ON cards BEGIN\n  UPDATE cards_fts SET front=NEW.front, back=NEW.back WHERE rowid=NEW.rowid;\nEND;\n\n-- Cards DELETE trigger  \nCREATE TRIGGER IF NOT EXISTS cards_fts_delete AFTER DELETE ON cards BEGIN\n  DELETE FROM cards_fts WHERE rowid=OLD.rowid;\nEND;\n\n-- Similar triggers for notes_fts\n```\n\n3. Implement searchQueries object:\n```typescript\nexport const searchQueries = {\n  search(query: string): { cards: DbCard[]; notes: DbNote[]; responseTimeMs: number } {\n    const startTime = performance.now();\n    \n    // Handle tag: prefix filtering\n    if (query.startsWith('tag:')) {\n      const tag = query.substring(4);\n      const cards = db.prepare(\n        'SELECT * FROM cards WHERE json_extract(tags, ?) IS NOT NULL'\n      ).all(`$[${tag}]`);\n      const notes = db.prepare(\n        'SELECT * FROM notes WHERE json_extract(tags, ?) IS NOT NULL'\n      ).all(`$[${tag}]`);\n      return { cards, notes, responseTimeMs: performance.now() - startTime };\n    }\n    \n    // FTS5 MATCH query\n    const cardResults = db.prepare(`\n      SELECT cards.* FROM cards\n      JOIN cards_fts ON cards.rowid = cards_fts.rowid\n      WHERE cards_fts MATCH ?\n      ORDER BY rank\n      LIMIT 50\n    `).all(query);\n    \n    const noteResults = db.prepare(`\n      SELECT notes.* FROM notes\n      JOIN notes_fts ON notes.rowid = notes_fts.rowid\n      WHERE notes_fts MATCH ?\n      ORDER BY rank\n      LIMIT 50\n    `).all(query);\n    \n    const responseTimeMs = performance.now() - startTime;\n    \n    if (responseTimeMs > 200) {\n      console.warn(`Search took ${responseTimeMs}ms - exceeds 200ms target`);\n    }\n    \n    return {\n      cards: cardResults.map(parseCardRow),\n      notes: noteResults.map(parseNoteRow),\n      responseTimeMs\n    };\n  }\n};\n```\n\nIPC LAYER:\n- Add ipcMain.handle('search:query', ...) in ipc-handlers.ts\n- Expose window.api.search.query() in preload.ts\n- Update electron.d.ts with search types\n\nUI LAYER (src/components/layout/SearchBar.tsx):\n- Add debounced search (150ms) with useMemo\n- Display unified dropdown with up to 10 results\n- Keyboard navigation: Up/Down arrows, Enter to open, Escape to close\n- Global Cmd+F (Ctrl+F) shortcut to focus search\n- Highlight search terms in results\n- 'View all X results' link if >10 matches\n\nSEARCH RESULTS PAGE (src/components/search/SearchResults.tsx):\n- Full paginated results view\n- Separate sections for cards and notes\n- Search term highlighting with <mark> tags\n- Tag chip filtering\n- Navigation back to search\n\nImplementation pseudo-code in SearchBar:\n```typescript\nfunction SearchBar() {\n  const [query, setQuery] = useState('');\n  const [results, setResults] = useState<SearchResult | null>(null);\n  const [focusedIndex, setFocusedIndex] = useState(0);\n\n  // Debounced search\n  useEffect(() => {\n    if (!query.trim()) {\n      setResults(null);\n      return;\n    }\n    \n    const timeoutId = setTimeout(async () => {\n      const result = await window.api.search.query(query);\n      if (result.data) {\n        setResults(result.data);\n      }\n    }, 150);\n    \n    return () => clearTimeout(timeoutId);\n  }, [query]);\n\n  // Global Cmd+F shortcut\n  useEffect(() => {\n    const handleKeyDown = (e: KeyboardEvent) => {\n      if ((e.metaKey || e.ctrlKey) && e.key === 'f') {\n        e.preventDefault();\n        searchInputRef.current?.focus();\n      }\n    };\n    window.addEventListener('keydown', handleKeyDown);\n    return () => window.removeEventListener('keydown', handleKeyDown);\n  }, []);\n\n  // Keyboard navigation\n  const handleKeyDown = (e: React.KeyboardEvent) => {\n    if (!results) return;\n    const totalResults = results.cards.length + results.notes.length;\n    \n    if (e.key === 'ArrowDown') {\n      e.preventDefault();\n      setFocusedIndex((prev) => (prev + 1) % totalResults);\n    } else if (e.key === 'ArrowUp') {\n      e.preventDefault();\n      setFocusedIndex((prev) => (prev - 1 + totalResults) % totalResults);\n    } else if (e.key === 'Enter' && focusedIndex >= 0) {\n      // Navigate to selected result\n      navigateToResult(focusedIndex);\n    }\n  };\n\n  return (\n    <div className=\"relative w-full max-w-lg\">\n      <Input\n        ref={searchInputRef}\n        value={query}\n        onChange={(e) => setQuery(e.target.value)}\n        onKeyDown={handleKeyDown}\n        placeholder=\"Search cards and notes... (Cmd+F)\"\n      />\n      {results && (\n        <SearchDropdown\n          results={results}\n          focusedIndex={focusedIndex}\n          query={query}\n        />\n      )}\n    </div>\n  );\n}\n```",
        "testStrategy": "Index 1000+ cards via seed data. Search 'acute MI', verify <200ms response time using DevTools Performance tab. Test special characters (quotes, parentheses, medical symbols). Test partial matches ('acu' matches 'acute'). Test tag:cardiology prefix filtering, verify only tagged cards returned. Test Cmd+F focuses search input. Type 'chest', press Down arrow twice, verify second result highlighted, press Enter, verify navigation to that card. Click 'View all results' link, verify SearchResults page opens with full result set. Test FTS synchronization: create new card with text 'pneumothorax', search for it immediately, verify appears in results. Update card text, verify FTS reflects change. Delete card, verify removed from FTS. Test empty query handling. Verify search works offline.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.523Z"
      },
      {
        "id": "23",
        "title": "Build command palette with fuzzy search and recent actions",
        "description": "Keyboard-first Cmd+K command palette for navigation, actions, and search with fuzzy matching, recent history, and keyboard-only operation",
        "details": "Update existing src/components/modals/CommandPalette.tsx component:\n\nGLOBAL SHORTCUT:\n- Cmd+K (Ctrl+K on Windows) opens palette\n- Prevent default browser search behavior\n- Works from any view (capture, review, queue, settings)\n\nCOMMAND CATEGORIES:\n1. Navigation:\n   - 'Go to Capture' → setCurrentView('capture')\n   - 'Go to Review' → setCurrentView('review')\n   - 'Go to Queue' → setCurrentView('queue')\n   - 'Go to Settings' → setCurrentView('settings')\n\n2. Actions:\n   - 'Create new card' → open capture interface\n   - 'Quick Dump' → open Quick Dump modal\n   - 'Start daily reviews' → navigate to review with today's queue\n   - 'Process queue item' → navigate to queue\n\n3. Search:\n   - 'Search cards...' → focus search bar with prefix 'cards:'\n   - 'Search notes...' → focus search bar with prefix 'notes:'\n   - 'Filter by tag...' → focus search bar with prefix 'tag:'\n\nFUZZY SEARCH:\n- Install fuse.js: npm install fuse.js\n- Fuzzy match user input against command names\n- Rank by relevance score\n- Highlight matching characters\n\nRECENT ACTIONS:\n- Store last 5 executed commands in localStorage under 'doughub-recent-commands'\n- Display at top of palette (above search results)\n- Format: 'Recently: Go to Review, Create new card, ...'\n- Update on every command execution\n\nKEYBOARD OPERATION:\n- Up/Down arrows: Navigate results\n- Enter: Execute selected command\n- Escape: Close palette\n- Type: Filter commands (no mouse required)\n- Tab: Cycle through categories\n\nVISUAL DESIGN:\n- Centered modal overlay: 500px width, max 400px height\n- Dark overlay background (50% opacity)\n- Command list: max 8 visible results with scroll\n- Each command shows: icon, name, description, keyboard shortcut hint\n- Selected command highlighted with accent background\n- Empty state: 'No commands found' when no matches\n\nImplementation pseudo-code:\n```typescript\nimport Fuse from 'fuse.js';\nimport { useEffect, useState } from 'react';\nimport { useAppStore } from '@/stores/useAppStore';\n\ninterface Command {\n  id: string;\n  name: string;\n  description: string;\n  shortcut?: string;\n  category: 'navigation' | 'action' | 'search';\n  execute: () => void;\n}\n\nconst COMMANDS: Command[] = [\n  {\n    id: 'nav-capture',\n    name: 'Go to Capture',\n    description: 'Navigate to capture interface',\n    shortcut: 'Escape (from review)',\n    category: 'navigation',\n    execute: () => setCurrentView('capture')\n  },\n  // ... more commands\n];\n\nfunction CommandPalette() {\n  const [open, setOpen] = useState(false);\n  const [query, setQuery] = useState('');\n  const [selectedIndex, setSelectedIndex] = useState(0);\n  const [recentCommands, setRecentCommands] = useState<string[]>([]);\n  const { setCurrentView } = useAppStore();\n\n  // Fuzzy search\n  const fuse = new Fuse(COMMANDS, {\n    keys: ['name', 'description'],\n    threshold: 0.3\n  });\n\n  const filteredCommands = query\n    ? fuse.search(query).map(result => result.item)\n    : COMMANDS;\n\n  // Global Cmd+K shortcut\n  useEffect(() => {\n    const handleKeyDown = (e: KeyboardEvent) => {\n      if ((e.metaKey || e.ctrlKey) && e.key === 'k') {\n        e.preventDefault();\n        setOpen(true);\n      }\n    };\n    window.addEventListener('keydown', handleKeyDown);\n    return () => window.removeEventListener('keydown', handleKeyDown);\n  }, []);\n\n  // Load recent commands from localStorage\n  useEffect(() => {\n    const stored = localStorage.getItem('doughub-recent-commands');\n    if (stored) {\n      setRecentCommands(JSON.parse(stored));\n    }\n  }, []);\n\n  const executeCommand = (command: Command) => {\n    command.execute();\n    \n    // Update recent commands\n    const updated = [command.id, ...recentCommands.filter(id => id !== command.id)].slice(0, 5);\n    setRecentCommands(updated);\n    localStorage.setItem('doughub-recent-commands', JSON.stringify(updated));\n    \n    setOpen(false);\n    setQuery('');\n  };\n\n  const handleKeyDown = (e: React.KeyboardEvent) => {\n    if (e.key === 'ArrowDown') {\n      e.preventDefault();\n      setSelectedIndex((prev) => (prev + 1) % filteredCommands.length);\n    } else if (e.key === 'ArrowUp') {\n      e.preventDefault();\n      setSelectedIndex((prev) => (prev - 1 + filteredCommands.length) % filteredCommands.length);\n    } else if (e.key === 'Enter') {\n      e.preventDefault();\n      executeCommand(filteredCommands[selectedIndex]);\n    } else if (e.key === 'Escape') {\n      setOpen(false);\n    }\n  };\n\n  return (\n    <Dialog open={open} onOpenChange={setOpen}>\n      <DialogContent className=\"max-w-lg\">\n        <Input\n          value={query}\n          onChange={(e) => setQuery(e.target.value)}\n          onKeyDown={handleKeyDown}\n          placeholder=\"Type a command or search...\"\n          autoFocus\n        />\n        \n        {recentCommands.length > 0 && !query && (\n          <div className=\"mb-2\">\n            <p className=\"text-xs text-muted-foreground mb-1\">Recent</p>\n            {recentCommands.slice(0, 3).map(id => {\n              const cmd = COMMANDS.find(c => c.id === id);\n              return cmd ? (\n                <CommandItem\n                  key={id}\n                  command={cmd}\n                  selected={false}\n                  onClick={() => executeCommand(cmd)}\n                />\n              ) : null;\n            })}\n          </div>\n        )}\n        \n        <ScrollArea className=\"max-h-96\">\n          {filteredCommands.map((command, index) => (\n            <CommandItem\n              key={command.id}\n              command={command}\n              selected={index === selectedIndex}\n              onClick={() => executeCommand(command)}\n            />\n          ))}\n          {filteredCommands.length === 0 && (\n            <p className=\"text-center text-muted-foreground py-8\">No commands found</p>\n          )}\n        </ScrollArea>\n      </DialogContent>\n    </Dialog>\n  );\n}\n```",
        "testStrategy": "Press Cmd+K from capture view, verify palette opens. Type 'rev', verify 'Go to Review' appears in results. Press Down arrow, verify next result highlighted. Press Enter, verify navigation to review. Test recent commands: execute 'Go to Capture' twice, then 'Quick Dump', open palette, verify 'Quick Dump' and 'Go to Capture' in recent section. Test Escape closes palette. Test fuzzy matching: type 'gocap', verify 'Go to Capture' matches. Test keyboard-only operation: open palette, navigate and execute without mouse. Verify shortcut hints display correctly. Test from all views (capture, review, queue, settings). Test empty query shows all commands. Test no results shows 'No commands found'.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.526Z"
      },
      {
        "id": "24",
        "title": "Implement tag system with AI suggestions and filtering",
        "description": "Tag-based organization with AI medical domain suggestions during capture, manual tag entry with autocomplete, tag filtering in search, and TagManager UI",
        "details": "TAG SUGGESTIONS DURING CAPTURE:\n- In CaptureInterface, after extractConcepts completes, call window.api.ai.suggestTags(pastedContent)\n- Display suggested tags as Badge chips with '+' button to add\n- Common medical domains: 'cardiology', 'pulmonology', 'pharmacology', 'pathophysiology', 'emergency-medicine', 'procedures', 'diagnostics'\n- Show max 5 suggested tags, most relevant first\n- Add TagSuggestions component displaying chips in flex row\n\nMANUAL TAG ENTRY:\n- Add tag input field below suggestions: Input with autocomplete\n- Autocomplete fetches existing tags via window.api.tags.getAll()\n- As user types, filter existing tags and show dropdown\n- Enter or comma adds tag to selected list\n- Selected tags shown as dismissible Badge chips\n- Tag validation: lowercase, alphanumeric + hyphens only, max 30 chars\n\nTAG STORAGE:\n- Tags stored in cards.tags and notes.tags as JSON arrays\n- When creating cards, attach both AI-suggested and manually-added tags\n- Tag extraction query for TagManager:\n```sql\nSELECT DISTINCT json_each.value as tag, \n  COUNT(*) as count\nFROM cards, json_each(cards.tags)\nGROUP BY tag\nORDER BY count DESC;\n```\n\nTAG MANAGER UI (src/components/tags/TagManager.tsx):\n- Displays all tags with card counts\n- Grouped by domain (auto-detect common prefixes or manual categorization)\n- Click tag to filter view: shows all cards/notes with that tag\n- Tag statistics: total usage, last used date\n- Edit tag: rename across all cards (bulk update)\n- Delete tag: remove from all cards (confirmation required)\n- Color coding: cardiology=red, pulmonology=blue, pharmacology=green, etc.\n- Integration: Add 'Tags' view to AppLayout navigation\n\nTAG FILTERING:\n- SearchBar supports 'tag:cardiology' syntax (already implemented in task 22)\n- Click tag chip anywhere → filters to that tag\n- Multiple tag filtering: 'tag:cardiology,pharmacology' (OR logic)\n- Tag filter UI: add tag chips to search bar when active\n- Clear filter button to reset\n\nTAGCHIP COMPONENT (src/components/tags/TagChip.tsx):\n- Displays tag with domain-based color\n- Size variants: sm, md, lg\n- Dismissible variant with X button\n- Click handler for filtering\n- Hover shows tag statistics (optional)\n\nImplementation pseudo-code:\n```typescript\n// In CaptureInterface after extraction\nconst [suggestedTags, setSuggestedTags] = useState<string[]>([]);\nconst [selectedTags, setSelectedTags] = useState<string[]>([]);\n\nuseEffect(() => {\n  if (extractedConcepts.length > 0 && pastedContent) {\n    window.api.ai.suggestTags(pastedContent).then(result => {\n      if (result.data) {\n        setSuggestedTags(result.data);\n      }\n    });\n  }\n}, [extractedConcepts.length]);\n\nconst handleAddTag = (tag: string) => {\n  if (!selectedTags.includes(tag)) {\n    setSelectedTags([...selectedTags, tag]);\n  }\n};\n\n// TagManager component\nfunction TagManager() {\n  const [tags, setTags] = useState<Array<{tag: string, count: number}>>([]);\n\n  useEffect(() => {\n    loadTags();\n  }, []);\n\n  const loadTags = async () => {\n    const result = await window.api.tags.getAll();\n    if (result.data) {\n      setTags(result.data);\n    }\n  };\n\n  const handleTagClick = (tag: string) => {\n    // Navigate to filtered view\n    setCurrentView('search');\n    setSearchQuery(`tag:${tag}`);\n  };\n\n  const domainGroups = {\n    cardiology: tags.filter(t => t.tag.includes('cardio') || t.tag.includes('heart')),\n    pulmonology: tags.filter(t => t.tag.includes('pulm') || t.tag.includes('lung')),\n    pharmacology: tags.filter(t => t.tag.includes('pharm') || t.tag.includes('drug')),\n    other: tags.filter(t => !['cardio', 'pulm', 'pharm'].some(d => t.tag.includes(d)))\n  };\n\n  return (\n    <div className=\"p-6\">\n      <h1>Tags</h1>\n      {Object.entries(domainGroups).map(([domain, domainTags]) => (\n        <div key={domain} className=\"mb-6\">\n          <h2 className=\"capitalize\">{domain}</h2>\n          <div className=\"flex flex-wrap gap-2\">\n            {domainTags.map(({ tag, count }) => (\n              <TagChip\n                key={tag}\n                tag={tag}\n                count={count}\n                onClick={() => handleTagClick(tag)}\n                domain={domain}\n              />\n            ))}\n          </div>\n        </div>\n      ))}\n    </div>\n  );\n}\n\n// TagChip component\ninterface TagChipProps {\n  tag: string;\n  count?: number;\n  domain?: string;\n  dismissible?: boolean;\n  onDismiss?: () => void;\n  onClick?: () => void;\n}\n\nfunction TagChip({ tag, count, domain, dismissible, onDismiss, onClick }: TagChipProps) {\n  const colorMap = {\n    cardiology: 'bg-red-100 text-red-800',\n    pulmonology: 'bg-blue-100 text-blue-800',\n    pharmacology: 'bg-green-100 text-green-800',\n    other: 'bg-gray-100 text-gray-800'\n  };\n\n  return (\n    <Badge\n      className={`${colorMap[domain || 'other']} cursor-pointer`}\n      onClick={onClick}\n    >\n      {tag}\n      {count && <span className=\"ml-1 text-xs\">({count})</span>}\n      {dismissible && (\n        <button onClick={(e) => { e.stopPropagation(); onDismiss?.(); }} className=\"ml-1\">\n          ×\n        </button>\n      )}\n    </Badge>\n  );\n}\n```\n\nIPC HANDLERS (electron/ipc-handlers.ts):\n```typescript\n// Add tag-related handlers\nipcMain.handle('tags:getAll', async () => {\n  try {\n    const tags = db.prepare(`\n      SELECT DISTINCT json_each.value as tag, COUNT(*) as count\n      FROM cards, json_each(cards.tags)\n      GROUP BY tag\n      ORDER BY count DESC\n    `).all();\n    return success(tags);\n  } catch (error) {\n    return failure(error);\n  }\n});\n\nipcMain.handle('tags:rename', async (_, oldTag: string, newTag: string) => {\n  try {\n    // Update all cards with old tag to use new tag\n    const cards = db.prepare('SELECT * FROM cards').all();\n    for (const card of cards) {\n      const tags = JSON.parse(card.tags);\n      if (tags.includes(oldTag)) {\n        const updated = tags.map(t => t === oldTag ? newTag : t);\n        db.prepare('UPDATE cards SET tags = ? WHERE id = ?')\n          .run(JSON.stringify(updated), card.id);\n      }\n    }\n    return success(true);\n  } catch (error) {\n    return failure(error);\n  }\n});\n```",
        "testStrategy": "Paste cardiology content in CaptureInterface, verify AI suggests 'cardiology', 'acute-coronary-syndrome' tags. Click '+' on suggested tag, verify added to selected. Type 'pharm' in tag input, verify autocomplete shows existing pharmacology tags. Add manual tag, create cards, verify tags saved to cards.tags. Navigate to TagManager, verify all tags displayed with counts grouped by domain. Click 'cardiology' tag, verify filtered view shows only cardiology cards. Test tag:cardiology in SearchBar, verify same filtering. Test tag rename: rename 'cardio' to 'cardiology', verify all cards updated. Test TagChip color coding matches domains. Test dismissible TagChip removes tag from selection.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.528Z"
      },
      {
        "id": "25",
        "title": "Build connection suggestions and semantic linking system",
        "description": "AI surfaces related notes during extraction using semantic similarity, displays gentle suggestion banner, enables one-click linking, shows related cards during review",
        "details": "CONNECTION SUGGESTIONS DURING CAPTURE:\n- In CaptureInterface, after extractConcepts and before save, call window.api.ai.findRelatedNotes(pastedContent)\n- Display ConnectionSuggestions banner if matches found (similarity >0.7)\n- Banner shows: 'Related notes found: [NoteTitle1] (85% match), [NoteTitle2] (72% match)'\n- Non-intrusive: collapsible, dismissible, doesn't block workflow\n- Each suggestion shows: note title, similarity percentage, content snippet (first 100 chars)\n- 'Link' button per suggestion creates connection in connections table\n- 'Dismiss all' button hides suggestions\n\nCONNECTIONS TABLE OPERATIONS:\n- When 'Link' clicked, insert into connections table:\n  * sourceNoteId: newly created note\n  * targetNoteId: suggested note\n  * semanticScore: similarity value (0.0-1.0)\n  * createdAt: timestamp\n- Bidirectional: also insert reverse connection (target→source) for both-way navigation\n\nSEMANTIC SIMILARITY IMPLEMENTATION:\n- For MVP, use keyword-based TF-IDF matching (implemented in ai-service.ts findRelatedNotes)\n- Extract keywords from content using simple term frequency\n- Calculate cosine similarity between term vectors\n- Future enhancement: OpenAI embeddings API (text-embedding-3-small) for true semantic matching\n- Threshold: only suggest if similarity >0.7\n\nRELATED CARDS IN REVIEW:\n- In ReviewInterface, fetch connections for current card's note via window.api.connections.getByNote(card.noteId)\n- Display 'Related cards' section below answer (similar to NoteContext)\n- Show linked note titles with similarity scores\n- 'View' button opens NoteViewer for linked note\n- Expandable/collapsible section, doesn't distract from review\n\nCONNECTION SUGGESTIONS COMPONENT (src/components/capture/ConnectionSuggestions.tsx):\n```typescript\ninterface ConnectionSuggestionsProps {\n  matches: SemanticMatch[];\n  onLink: (noteId: string, score: number) => void;\n  onDismiss: () => void;\n}\n\nfunction ConnectionSuggestions({ matches, onLink, onDismiss }: ConnectionSuggestionsProps) {\n  const [expanded, setExpanded] = useState(true);\n  const [notes, setNotes] = useState<Note[]>([]);\n\n  useEffect(() => {\n    // Fetch full note details for matches\n    Promise.all(matches.map(m => window.api.notes.getById(m.noteId)))\n      .then(results => {\n        setNotes(results.filter(r => r.data).map(r => r.data!));\n      });\n  }, [matches]);\n\n  if (matches.length === 0 || !expanded) {\n    return null;\n  }\n\n  return (\n    <Card className=\"border-blue-200 bg-blue-50\">\n      <CardHeader className=\"flex flex-row items-center justify-between\">\n        <div className=\"flex items-center gap-2\">\n          <Sparkles className=\"h-4 w-4 text-blue-600\" />\n          <h3 className=\"text-sm font-medium\">Related notes found</h3>\n        </div>\n        <div className=\"flex gap-2\">\n          <Button variant=\"ghost\" size=\"sm\" onClick={() => setExpanded(false)}>Collapse</Button>\n          <Button variant=\"ghost\" size=\"sm\" onClick={onDismiss}>Dismiss</Button>\n        </div>\n      </CardHeader>\n      <CardContent>\n        <div className=\"space-y-2\">\n          {notes.map((note, idx) => {\n            const match = matches[idx];\n            return (\n              <div key={note.id} className=\"flex items-start justify-between p-2 rounded bg-white\">\n                <div className=\"flex-1\">\n                  <p className=\"font-medium\">{note.title}</p>\n                  <p className=\"text-xs text-muted-foreground truncate\">\n                    {note.content.substring(0, 100)}...\n                  </p>\n                  <Badge variant=\"secondary\" className=\"mt-1\">\n                    {Math.round(match.similarity * 100)}% match\n                  </Badge>\n                </div>\n                <Button\n                  size=\"sm\"\n                  onClick={() => onLink(note.id, match.similarity)}\n                >\n                  Link\n                </Button>\n              </div>\n            );\n          })}\n        </div>\n      </CardContent>\n    </Card>\n  );\n}\n```\n\nINTEGRATION IN CAPTUREINTERFACE:\n```typescript\n// After extractConcepts\nconst [connectionMatches, setConnectionMatches] = useState<SemanticMatch[]>([]);\n\nuseEffect(() => {\n  if (extractedConcepts.length > 0 && pastedContent) {\n    // Get existing notes\n    window.api.notes.getAll().then(notesResult => {\n      if (notesResult.data) {\n        // Find related notes\n        window.api.ai.findRelatedNotes(pastedContent, notesResult.data)\n          .then(matchResult => {\n            if (matchResult.data) {\n              setConnectionMatches(matchResult.data.filter(m => m.similarity > 0.7));\n            }\n          });\n      }\n    });\n  }\n}, [extractedConcepts.length]);\n\nconst handleLinkNote = async (targetNoteId: string, score: number) => {\n  // Create bidirectional connection after note is saved\n  const sourceNoteId = createdNote.id; // from handleCreateCards\n  \n  await window.api.connections.insert({\n    id: crypto.randomUUID(),\n    sourceNoteId,\n    targetNoteId,\n    semanticScore: score,\n    createdAt: new Date().toISOString()\n  });\n  \n  // Create reverse connection\n  await window.api.connections.insert({\n    id: crypto.randomUUID(),\n    sourceNoteId: targetNoteId,\n    targetNoteId: sourceNoteId,\n    semanticScore: score,\n    createdAt: new Date().toISOString()\n  });\n  \n  toast.success('Notes linked!');\n  // Remove from suggestions\n  setConnectionMatches(prev => prev.filter(m => m.noteId !== targetNoteId));\n};\n```\n\nRELATED CARDS IN REVIEWINTERFACE:\n```typescript\nconst [relatedNotes, setRelatedNotes] = useState<Array<{note: Note, score: number}>>([]);\n\nuseEffect(() => {\n  if (currentCard?.noteId) {\n    window.api.connections.getByNote(currentCard.noteId).then(result => {\n      if (result.data) {\n        // Fetch full note details for each connection\n        Promise.all(\n          result.data.map(async conn => {\n            const noteResult = await window.api.notes.getById(conn.targetNoteId);\n            return noteResult.data ? { note: noteResult.data, score: conn.semanticScore } : null;\n          })\n        ).then(notes => {\n          setRelatedNotes(notes.filter(Boolean));\n        });\n      }\n    });\n  }\n}, [currentCard?.noteId]);\n\n// In render, after NoteContext\n{relatedNotes.length > 0 && (\n  <Collapsible className=\"mt-4\">\n    <CollapsibleTrigger className=\"text-sm flex items-center gap-2\">\n      <span>Related cards ({relatedNotes.length})</span>\n      <ChevronDown className=\"h-4 w-4\" />\n    </CollapsibleTrigger>\n    <CollapsibleContent>\n      <div className=\"space-y-2 mt-2\">\n        {relatedNotes.map(({ note, score }) => (\n          <div key={note.id} className=\"flex items-center justify-between p-2 bg-muted/50 rounded\">\n            <span className=\"text-sm\">{note.title}</span>\n            <div className=\"flex items-center gap-2\">\n              <Badge variant=\"outline\">{Math.round(score * 100)}%</Badge>\n              <Button size=\"sm\" variant=\"ghost\" onClick={() => openNoteViewer(note)}>\n                View\n              </Button>\n            </div>\n          </div>\n        ))}\n      </div>\n    </CollapsibleContent>\n  </Collapsible>\n)}\n```\n\nIPC HANDLERS (electron/ipc-handlers.ts):\n```typescript\nipcMain.handle('connections:getByNote', async (_, noteId: string) => {\n  try {\n    const connections = db.prepare(\n      'SELECT * FROM connections WHERE sourceNoteId = ? ORDER BY semanticScore DESC'\n    ).all(noteId);\n    return success(connections);\n  } catch (error) {\n    return failure(error);\n  }\n});\n\nipcMain.handle('connections:insert', async (_, connection: DbConnection) => {\n  try {\n    db.prepare(`\n      INSERT INTO connections (id, sourceNoteId, targetNoteId, semanticScore, createdAt)\n      VALUES (?, ?, ?, ?, ?)\n    `).run(\n      connection.id,\n      connection.sourceNoteId,\n      connection.targetNoteId,\n      connection.semanticScore,\n      connection.createdAt\n    );\n    return success(connection);\n  } catch (error) {\n    return failure(error);\n  }\n});\n```",
        "testStrategy": "Create note about 'Acute MI pathophysiology' with content mentioning 'coronary occlusion, troponin elevation'. Paste related content about 'Chest pain differential diagnosis including MI'. Verify ConnectionSuggestions banner appears with first note as match with similarity >70%. Click 'Link' button, verify connection created in connections table (check both directions). Verify toast 'Notes linked!' appears. Navigate to review, load card from second note, verify 'Related cards' section appears showing first note. Click 'View', verify NoteViewer opens with correct note. Test 'Dismiss' button hides suggestions. Test with unrelated content (e.g., dermatology after cardiology), verify no suggestions appear (similarity <0.7). Test expandable/collapsible related cards section in review.",
        "priority": "low",
        "dependencies": [
          "21"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.531Z"
      },
      {
        "id": "26",
        "title": "Implement FSRS analytics and response time intelligence",
        "description": "Track response times and domain-specific performance, create analytics module for personalized FSRS parameters, forgetting curve optimization, and card reformulation detection",
        "details": "ANALYTICS MODULE (electron/fsrs-analytics.ts):\nCreate comprehensive analytics system for FSRS personalization:\n\n1. calculatePersonalParams(cardId: string): Promise<FSRSParams>\n   - Analyzes review history for specific card\n   - Computes personalized stability and difficulty parameters\n   - Uses ts-fsrs optimal retention calculation\n   - Returns adjusted FSRS params for next review\n\n2. getDomainStats(domain: string): Promise<DomainStats>\n   - Aggregates performance across cards with given tag/domain\n   - Returns: { avgResponseTime, accuracy, cardCount, lastReviewed }\n   - Identifies weak domains for targeted study\n\n3. getOptimizedRetention(userId?: string): Promise<number>\n   - Calculates personalized retention rate based on historical performance\n   - Default: 0.89 (medical education optimal from PRD)\n   - Adjusts up/down based on accuracy trends\n   - Returns value between 0.80 and 0.95\n\n4. detectCardsNeedingReformulation(): Promise<CardWithFSRS[]>\n   - Identifies cards with consistently high response times (>30s avg)\n   - Flags cards with high lapse rate (>3 lapses)\n   - Suggests split into multiple cards\n   - Returns array of problematic cards with metrics\n\n5. getForgettingCurve(domain?: string): Promise<ForgettingCurveData>\n   - Computes forgetting curve for domain or all cards\n   - Returns data points for visualization\n   - Format: { days: number[], retention: number[] }\n\nDOMAIN TRACKING:\n- Domain derived from card's first tag (card.tags[0])\n- Common domains: cardiology, pulmonology, pharmacology, pathophysiology, procedures\n- Store in review_logs for historical analysis\n- Query: GROUP BY domain to aggregate statistics\n\nPERSONALIZED FSRS:\n- Use response time to adjust difficulty:\n  * Fast responses (<5s avg) → decrease difficulty parameter\n  * Slow responses (>20s avg) → increase difficulty parameter\n- Adjust stability based on lapse rate:\n  * Low lapses (<1) → increase stability\n  * High lapses (>2) → decrease stability\n- Apply per-domain optimizations:\n  * Weak domains: decrease intervals (more frequent reviews)\n  * Strong domains: increase intervals (less frequent reviews)\n\nSETTINGS UI INTEGRATION:\nCreate src/components/settings/AnalyticsView.tsx:\n- Display domain statistics table:\n  * Domain name, card count, avg response time, accuracy %\n  * Sort by accuracy (ascending) to show weak domains first\n- Forgetting curve chart using recharts:\n  * X-axis: days since last review\n  * Y-axis: retention %\n  * Multiple lines for different domains\n- Cards needing reformulation section:\n  * List problematic cards with metrics\n  * 'Edit card' button for each\n  * Suggestion to split multi-fact cards\n- Personalized retention rate display:\n  * Current value with explanation\n  * Recommendation based on performance\n  * Manual override slider (0.80-0.95)\n\nImplementation pseudo-code:\n```typescript\n// electron/fsrs-analytics.ts\nimport { db } from './database';\nimport { FSRS, FSRSParameters } from 'ts-fsrs';\n\nexport interface DomainStats {\n  domain: string;\n  cardCount: number;\n  avgResponseTime: number;\n  accuracy: number;\n  lastReviewed: string | null;\n}\n\nexport interface ForgettingCurveData {\n  days: number[];\n  retention: number[];\n}\n\nexport const fsrsAnalytics = {\n  /**\n   * Calculate personalized FSRS parameters for a card based on review history.\n   */\n  async calculatePersonalParams(cardId: string): Promise<FSRSParameters> {\n    const reviews = db.prepare(\n      'SELECT * FROM review_logs WHERE cardId = ? ORDER BY createdAt DESC LIMIT 10'\n    ).all(cardId);\n\n    if (reviews.length === 0) {\n      // Return default params\n      return new FSRS().params;\n    }\n\n    // Analyze review patterns\n    const avgResponseTime = reviews.reduce((sum, r) => \n      sum + (r.responseTimeMs || 0), 0) / reviews.length;\n    const lapseCount = reviews.filter(r => r.rating === 1).length;\n    const lapseRate = lapseCount / reviews.length;\n\n    // Adjust difficulty based on response time\n    const baseDifficulty = 5; // Default from ts-fsrs\n    const difficultyAdjustment = avgResponseTime > 20000 ? 1 : (avgResponseTime < 5000 ? -1 : 0);\n    const difficulty = Math.max(1, Math.min(10, baseDifficulty + difficultyAdjustment));\n\n    // Adjust stability based on lapse rate\n    const baseStability = 1;\n    const stabilityAdjustment = lapseRate > 0.3 ? -0.2 : (lapseRate < 0.1 ? 0.2 : 0);\n    const stability = Math.max(0.1, baseStability + stabilityAdjustment);\n\n    // Return personalized params\n    const params = new FSRS().params;\n    return {\n      ...params,\n      w: params.w.map((val, idx) => {\n        // Adjust specific weight parameters based on analysis\n        if (idx === 4) return difficulty; // Difficulty weight\n        if (idx === 0) return stability;  // Initial stability\n        return val;\n      })\n    };\n  },\n\n  /**\n   * Get performance statistics for a medical domain.\n   */\n  async getDomainStats(domain: string): Promise<DomainStats> {\n    // Get all cards with domain tag\n    const cards = db.prepare(`\n      SELECT c.*, rl.responseTimeMs, rl.rating\n      FROM cards c\n      LEFT JOIN review_logs rl ON rl.cardId = c.id\n      WHERE json_extract(c.tags, '$[0]') = ?\n    `).all(domain);\n\n    if (cards.length === 0) {\n      return {\n        domain,\n        cardCount: 0,\n        avgResponseTime: 0,\n        accuracy: 0,\n        lastReviewed: null\n      };\n    }\n\n    const uniqueCards = new Set(cards.map(c => c.id)).size;\n    const reviewsWithTime = cards.filter(c => c.responseTimeMs !== null);\n    const avgResponseTime = reviewsWithTime.length > 0\n      ? reviewsWithTime.reduce((sum, c) => sum + c.responseTimeMs, 0) / reviewsWithTime.length\n      : 0;\n\n    // Accuracy: % of reviews rated Good or Easy (3 or 4)\n    const reviewsWithRating = cards.filter(c => c.rating !== null);\n    const accuracy = reviewsWithRating.length > 0\n      ? reviewsWithRating.filter(c => c.rating >= 3).length / reviewsWithRating.length\n      : 0;\n\n    const lastReviewed = cards.reduce((latest, c) => {\n      return c.lastReview && (!latest || c.lastReview > latest) ? c.lastReview : latest;\n    }, null as string | null);\n\n    return {\n      domain,\n      cardCount: uniqueCards,\n      avgResponseTime: Math.round(avgResponseTime),\n      accuracy: Math.round(accuracy * 100) / 100,\n      lastReviewed\n    };\n  },\n\n  /**\n   * Calculate optimized retention rate based on user performance.\n   */\n  async getOptimizedRetention(): Promise<number> {\n    const allReviews = db.prepare(\n      'SELECT rating FROM review_logs ORDER BY createdAt DESC LIMIT 100'\n    ).all();\n\n    if (allReviews.length < 20) {\n      return 0.89; // Default for medical education\n    }\n\n    const accuracy = allReviews.filter(r => r.rating >= 3).length / allReviews.length;\n\n    // Adjust retention target based on accuracy\n    if (accuracy > 0.92) {\n      return 0.85; // User is doing well, can reduce review frequency\n    } else if (accuracy < 0.80) {\n      return 0.93; // User struggling, increase review frequency\n    }\n\n    return 0.89; // Keep default\n  },\n\n  /**\n   * Detect cards that need reformulation (high response time, high lapses).\n   */\n  async detectCardsNeedingReformulation(): Promise<Array<{ card: DbCard; metrics: any }>> {\n    const problematicCards = db.prepare(`\n      SELECT c.*, \n        AVG(rl.responseTimeMs) as avgResponseTime,\n        SUM(CASE WHEN rl.rating = 1 THEN 1 ELSE 0 END) as lapseCount,\n        COUNT(rl.id) as reviewCount\n      FROM cards c\n      JOIN review_logs rl ON rl.cardId = c.id\n      GROUP BY c.id\n      HAVING avgResponseTime > 30000 OR lapseCount > 3\n      ORDER BY avgResponseTime DESC\n      LIMIT 20\n    `).all();\n\n    return problematicCards.map(row => ({\n      card: parseCardRow(row),\n      metrics: {\n        avgResponseTime: row.avgResponseTime,\n        lapseCount: row.lapseCount,\n        reviewCount: row.reviewCount,\n        suggestion: row.avgResponseTime > 30000\n          ? 'Card takes too long to answer - consider splitting into multiple cards'\n          : 'High lapse rate - card may need clarification or simplification'\n      }\n    }));\n  },\n\n  /**\n   * Calculate forgetting curve for visualization.\n   */\n  async getForgettingCurve(domain?: string): Promise<ForgettingCurveData> {\n    const query = domain\n      ? `SELECT c.*, rl.elapsedDays, rl.rating\n         FROM cards c\n         JOIN review_logs rl ON rl.cardId = c.id\n         WHERE json_extract(c.tags, '$[0]') = ?\n         ORDER BY rl.elapsedDays`\n      : `SELECT rl.elapsedDays, rl.rating\n         FROM review_logs rl\n         ORDER BY rl.elapsedDays`;\n\n    const reviews = domain\n      ? db.prepare(query).all(domain)\n      : db.prepare(query).all();\n\n    // Group by elapsed days, calculate retention rate\n    const dayGroups = new Map<number, { total: number; recalled: number }>();\n\n    reviews.forEach(r => {\n      const day = Math.floor(r.elapsedDays);\n      if (!dayGroups.has(day)) {\n        dayGroups.set(day, { total: 0, recalled: 0 });\n      }\n      const group = dayGroups.get(day)!;\n      group.total++;\n      if (r.rating >= 3) {\n        group.recalled++;\n      }\n    });\n\n    const days: number[] = [];\n    const retention: number[] = [];\n\n    dayGroups.forEach((group, day) => {\n      days.push(day);\n      retention.push(group.recalled / group.total);\n    });\n\n    return { days, retention };\n  }\n};\n```\n\nIPC HANDLERS:\n```typescript\n// In electron/ipc-handlers.ts\nipcMain.handle('analytics:getDomainStats', async (_, domain: string) => {\n  try {\n    const stats = await fsrsAnalytics.getDomainStats(domain);\n    return success(stats);\n  } catch (error) {\n    return failure(error);\n  }\n});\n\nipcMain.handle('analytics:getOptimizedRetention', async () => {\n  try {\n    const retention = await fsrsAnalytics.getOptimizedRetention();\n    return success(retention);\n  } catch (error) {\n    return failure(error);\n  }\n});\n\nipcMain.handle('analytics:getProblematicCards', async () => {\n  try {\n    const cards = await fsrsAnalytics.detectCardsNeedingReformulation();\n    return success(cards);\n  } catch (error) {\n    return failure(error);\n  }\n});\n\nipcMain.handle('analytics:getForgettingCurve', async (_, domain?: string) => {\n  try {\n    const data = await fsrsAnalytics.getForgettingCurve(domain);\n    return success(data);\n  } catch (error) {\n    return failure(error);\n  }\n});\n```\n\nSETTINGS UI (src/components/settings/AnalyticsView.tsx):\n```typescript\nimport { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';\n\nfunction AnalyticsView() {\n  const [domainStats, setDomainStats] = useState<DomainStats[]>([]);\n  const [forgettingCurve, setForgettingCurve] = useState<ForgettingCurveData | null>(null);\n  const [problematicCards, setProblematicCards] = useState([]);\n  const [optimizedRetention, setOptimizedRetention] = useState(0.89);\n\n  useEffect(() => {\n    loadAnalytics();\n  }, []);\n\n  const loadAnalytics = async () => {\n    // Load domain stats for common medical domains\n    const domains = ['cardiology', 'pulmonology', 'pharmacology', 'pathophysiology'];\n    const stats = await Promise.all(\n      domains.map(d => window.api.analytics.getDomainStats(d))\n    );\n    setDomainStats(stats.filter(s => s.data).map(s => s.data!));\n\n    // Load forgetting curve\n    const curveResult = await window.api.analytics.getForgettingCurve();\n    if (curveResult.data) {\n      setForgettingCurve(curveResult.data);\n    }\n\n    // Load problematic cards\n    const problematicResult = await window.api.analytics.getProblematicCards();\n    if (problematicResult.data) {\n      setProblematicCards(problematicResult.data);\n    }\n\n    // Load optimized retention\n    const retentionResult = await window.api.analytics.getOptimizedRetention();\n    if (retentionResult.data) {\n      setOptimizedRetention(retentionResult.data);\n    }\n  };\n\n  return (\n    <div className=\"p-6 space-y-8\">\n      <div>\n        <h1 className=\"text-2xl font-bold mb-4\">Analytics & Performance</h1>\n      </div>\n\n      {/* Domain Statistics */}\n      <Card>\n        <CardHeader>\n          <CardTitle>Domain Performance</CardTitle>\n        </CardHeader>\n        <CardContent>\n          <table className=\"w-full\">\n            <thead>\n              <tr>\n                <th>Domain</th>\n                <th>Cards</th>\n                <th>Avg Response Time</th>\n                <th>Accuracy</th>\n              </tr>\n            </thead>\n            <tbody>\n              {domainStats.sort((a, b) => a.accuracy - b.accuracy).map(stat => (\n                <tr key={stat.domain}>\n                  <td>{stat.domain}</td>\n                  <td>{stat.cardCount}</td>\n                  <td>{(stat.avgResponseTime / 1000).toFixed(1)}s</td>\n                  <td>{(stat.accuracy * 100).toFixed(0)}%</td>\n                </tr>\n              ))}\n            </tbody>\n          </table>\n        </CardContent>\n      </Card>\n\n      {/* Forgetting Curve */}\n      {forgettingCurve && (\n        <Card>\n          <CardHeader>\n            <CardTitle>Forgetting Curve</CardTitle>\n          </CardHeader>\n          <CardContent>\n            <LineChart width={600} height={300} data={\n              forgettingCurve.days.map((day, idx) => ({\n                day,\n                retention: forgettingCurve.retention[idx] * 100\n              }))\n            }>\n              <CartesianGrid strokeDasharray=\"3 3\" />\n              <XAxis dataKey=\"day\" label={{ value: 'Days', position: 'insideBottom' }} />\n              <YAxis label={{ value: 'Retention %', angle: -90, position: 'insideLeft' }} />\n              <Tooltip />\n              <Line type=\"monotone\" dataKey=\"retention\" stroke=\"#8884d8\" />\n            </LineChart>\n          </CardContent>\n        </Card>\n      )}\n\n      {/* Optimized Retention */}\n      <Card>\n        <CardHeader>\n          <CardTitle>Personalized Retention Rate</CardTitle>\n        </CardHeader>\n        <CardContent>\n          <p>Current: {(optimizedRetention * 100).toFixed(0)}%</p>\n          <p className=\"text-sm text-muted-foreground\">\n            Based on your performance, this retention rate balances review frequency with long-term retention.\n          </p>\n        </CardContent>\n      </Card>\n\n      {/* Problematic Cards */}\n      {problematicCards.length > 0 && (\n        <Card>\n          <CardHeader>\n            <CardTitle>Cards Needing Attention</CardTitle>\n          </CardHeader>\n          <CardContent>\n            <div className=\"space-y-2\">\n              {problematicCards.map(({ card, metrics }) => (\n                <div key={card.id} className=\"border p-2 rounded\">\n                  <p className=\"font-medium truncate\">{card.front}</p>\n                  <p className=\"text-sm text-muted-foreground\">\n                    Avg response: {(metrics.avgResponseTime / 1000).toFixed(1)}s | \n                    Lapses: {metrics.lapseCount}\n                  </p>\n                  <p className=\"text-xs text-orange-600\">{metrics.suggestion}</p>\n                  <Button size=\"sm\" variant=\"outline\" className=\"mt-1\">\n                    Edit Card\n                  </Button>\n                </div>\n              ))}\n            </div>\n          </CardContent>\n        </Card>\n      )}\n    </div>\n  );\n}\n```",
        "testStrategy": "Review 20+ cards with varying response times and domains (cardiology, pharmacology). Navigate to Analytics view in Settings. Verify domain statistics table shows correct card counts, avg response times, accuracy percentages. Verify weak domains (low accuracy) appear at top of sorted table. Test forgetting curve chart displays correctly with retention % on Y-axis and days on X-axis. Create cards with >30s response time or >3 lapses, verify they appear in 'Cards Needing Attention' section with appropriate suggestions. Test optimized retention calculation: review with high accuracy (>92%), verify retention decreases toward 0.85; review with low accuracy (<80%), verify retention increases toward 0.93. Test domain filtering in forgetting curve. Verify 'Edit Card' button navigates to card editor.",
        "priority": "low",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.533Z"
      },
      {
        "id": "27",
        "title": "Implement evidence-based card validation with auto-fix suggestions",
        "description": "Real-time validation warnings for pattern-matching, multi-fact violations, minimum information principle with auto-fix options and validation history tracking",
        "details": "VALIDATION LIBRARY (src/lib/card-validation.ts):\nCreate comprehensive client-side validation module:\n\n1. validateMinimumInformation(card: {front: string, back: string}): ValidationResult\n   - Checks if card tests one concept only\n   - Detects multiple facts in answer (e.g., 'and', 'also', numbered lists)\n   - Severity: WARNING\n   - Fix suggestion: 'Split into multiple cards'\n\n2. detectPatternMatching(card): ValidationResult\n   - Detects format recognition cues:\n     * 'Which of the following...'\n     * Multiple choice artifacts (A), B), C))\n     * 'Select the correct answer'\n     * List format in question without clinical context\n   - Severity: ERROR (blocks save)\n   - Fix suggestion: 'Rephrase as direct question'\n\n3. detectMultiFact(card): ValidationResult\n   - Warns if answer contains >1 independent fact\n   - Checks for conjunctions, semicolons, numbered lists\n   - Severity: WARNING\n   - Fix suggestion: 'Split into X separate cards' (provides split)\n\n4. detectMedicalList(card): ValidationResult\n   - Checks for list-style answers without clinical context\n   - Patterns: '1.', '2.', 'causes:', 'symptoms:'\n   - Severity: WARNING\n   - Fix suggestion: 'Convert to clinical vignettes'\n\n5. detectEmptyFields(card): ValidationResult\n   - Checks for empty front or back\n   - Severity: ERROR\n   - Fix suggestion: None (must fill)\n\nVALIDATION RESULT INTERFACE:\n```typescript\ninterface ValidationIssue {\n  type: 'minimum-info' | 'pattern-matching' | 'multi-fact' | 'medical-list' | 'empty-field';\n  severity: 'error' | 'warning' | 'info';\n  message: string;\n  suggestion?: string;\n  autoFix?: () => CardPair[]; // Returns split cards if applicable\n}\n\ninterface ValidationResult {\n  isValid: boolean; // False if any ERROR severity\n  issues: ValidationIssue[];\n}\n```\n\nAUTO-FIX FUNCTIONALITY:\n- For multi-fact cards: split into multiple cards\n  * Parse numbered lists or sentences with 'and'\n  * Generate separate {front, back} pairs\n  * Preview split before applying\n- For medical lists: suggest clinical vignette conversion\n  * Use ai-service.detectMedicalList and convertToVignette\n  * Show preview of converted cards\n- For pattern-matching: provide rephrasing suggestions\n  * Use AI to rephrase as direct question\n  * User can accept or edit suggestion\n\nVALIDATION INTEGRATION IN CAPTUREINTERFACE:\n- Run validation on each extracted concept after AI processing\n- Display ValidationWarning component inline with concept\n- Block save if any ERROR severity issues\n- Allow save with WARNING severity after confirmation\n- Show 'Fix automatically' button for fixable issues\n- Track which warnings users dismiss (analytics)\n\nVALIDATION WARNING COMPONENT (src/components/capture/ValidationWarning.tsx):\n```typescript\ninterface ValidationWarningProps {\n  issue: ValidationIssue;\n  onFix?: () => void;\n  onDismiss?: () => void;\n}\n\nfunction ValidationWarning({ issue, onFix, onDismiss }: ValidationWarningProps) {\n  const severityConfig = {\n    error: { icon: AlertTriangle, color: 'text-red-600', bg: 'bg-red-50' },\n    warning: { icon: AlertTriangle, color: 'text-orange-600', bg: 'bg-orange-50' },\n    info: { icon: Info, color: 'text-blue-600', bg: 'bg-blue-50' }\n  };\n\n  const config = severityConfig[issue.severity];\n  const Icon = config.icon;\n\n  return (\n    <Alert className={`${config.bg} border-l-4 border-${config.color.split('-')[1]}-${config.color.split('-')[2]}`}>\n      <Icon className={`h-4 w-4 ${config.color}`} />\n      <AlertTitle>{issue.type.replace('-', ' ').toUpperCase()}</AlertTitle>\n      <AlertDescription>\n        <p>{issue.message}</p>\n        {issue.suggestion && (\n          <p className=\"mt-1 text-sm italic\">{issue.suggestion}</p>\n        )}\n        <div className=\"flex gap-2 mt-2\">\n          {issue.autoFix && onFix && (\n            <Button size=\"sm\" variant=\"outline\" onClick={onFix}>\n              Fix automatically\n            </Button>\n          )}\n          {issue.severity !== 'error' && onDismiss && (\n            <Button size=\"sm\" variant=\"ghost\" onClick={onDismiss}>\n              Dismiss\n            </Button>\n          )}\n        </div>\n      </AlertDescription>\n    </Alert>\n  );\n}\n```\n\nVALIDATION LIBRARY IMPLEMENTATION:\n```typescript\n// src/lib/card-validation.ts\nexport function validateCard(card: { front: string; back: string }): ValidationResult {\n  const issues: ValidationIssue[] = [];\n\n  // Check empty fields\n  if (!card.front.trim() || !card.back.trim()) {\n    issues.push({\n      type: 'empty-field',\n      severity: 'error',\n      message: 'Both question and answer must be filled',\n      suggestion: undefined\n    });\n  }\n\n  // Check pattern matching\n  const patternMatching = detectPatternMatching(card);\n  if (patternMatching) {\n    issues.push(patternMatching);\n  }\n\n  // Check multi-fact\n  const multiFact = detectMultiFact(card);\n  if (multiFact) {\n    issues.push(multiFact);\n  }\n\n  // Check medical list\n  const medicalList = detectMedicalList(card);\n  if (medicalList) {\n    issues.push(medicalList);\n  }\n\n  // Check minimum information\n  const minInfo = validateMinimumInformation(card);\n  if (minInfo) {\n    issues.push(minInfo);\n  }\n\n  return {\n    isValid: !issues.some(i => i.severity === 'error'),\n    issues\n  };\n}\n\nfunction detectPatternMatching(card: { front: string; back: string }): ValidationIssue | null {\n  const patterns = [\n    /which of the following/i,\n    /select the correct/i,\n    /choose the best/i,\n    /\\([A-D]\\)/,  // Multiple choice markers\n    /^[A-D]\\./m   // Multiple choice format\n  ];\n\n  const hasPattern = patterns.some(pattern => pattern.test(card.front));\n\n  if (hasPattern) {\n    return {\n      type: 'pattern-matching',\n      severity: 'error',\n      message: 'Card contains pattern-matching cues (multiple choice format)',\n      suggestion: 'Rephrase as direct question without multiple choice format',\n      autoFix: undefined // Could add AI-powered rephrasing\n    };\n  }\n\n  return null;\n}\n\nfunction detectMultiFact(card: { front: string; back: string }): ValidationIssue | null {\n  // Check for multiple facts in answer\n  const indicators = [\n    /\\d+\\.\\s/g,        // Numbered lists\n    /;/g,              // Semicolons\n    / and /gi,         // 'and' conjunction\n    /also/gi,          // 'also'\n    /additionally/gi   // 'additionally'\n  ];\n\n  const matches = indicators.reduce((count, pattern) => {\n    const found = card.back.match(pattern);\n    return count + (found ? found.length : 0);\n  }, 0);\n\n  if (matches >= 2) {\n    // Suggest split\n    const autoFix = () => {\n      // Simple split by sentences or numbered items\n      const sentences = card.back.split(/[.!?]/).filter(s => s.trim());\n      return sentences.map((sentence, idx) => ({\n        front: `${card.front} (Part ${idx + 1})`,\n        back: sentence.trim()\n      }));\n    };\n\n    return {\n      type: 'multi-fact',\n      severity: 'warning',\n      message: `Card appears to test multiple facts (${matches} indicators found)`,\n      suggestion: `Consider splitting into ${Math.min(matches, 5)} separate cards`,\n      autoFix\n    };\n  }\n\n  return null;\n}\n\nfunction detectMedicalList(card: { front: string; back: string }): ValidationIssue | null {\n  const listPatterns = [\n    /\\d+\\.\\s.+/,      // Numbered list\n    /causes?:/i,      // 'causes:'\n    /symptoms?:/i,    // 'symptoms:'\n    /signs?:/i,       // 'signs:'\n    /differential/i   // 'differential'\n  ];\n\n  const isList = listPatterns.some(pattern => pattern.test(card.back));\n  const hasContext = /patient|presents|history|exam/i.test(card.front);\n\n  if (isList && !hasContext) {\n    return {\n      type: 'medical-list',\n      severity: 'warning',\n      message: 'Medical list detected without clinical context',\n      suggestion: 'Convert to clinical vignettes for better retention',\n      autoFix: undefined // Handled by medical list processor\n    };\n  }\n\n  return null;\n}\n\nfunction validateMinimumInformation(card: { front: string; back: string }): ValidationIssue | null {\n  // This is covered by multi-fact detection\n  return null;\n}\n```\n\nINTEGRATION IN CAPTUREINTERFACE:\n```typescript\n// After AI extraction\nconst validateConcepts = () => {\n  const validated = extractedConcepts.map(concept => {\n    const card = { front: concept.text, back: generateAnswer(concept) };\n    const validation = validateCard(card);\n    return { ...concept, validation };\n  });\n  setExtractedConcepts(validated);\n};\n\n// Before save\nconst handleCreateCards = () => {\n  const hasErrors = extractedConcepts.some(c => c.validation && !c.validation.isValid);\n  \n  if (hasErrors) {\n    toast.error('Please fix validation errors before saving');\n    return;\n  }\n\n  const hasWarnings = extractedConcepts.some(c => \n    c.validation?.issues.some(i => i.severity === 'warning')\n  );\n\n  if (hasWarnings && !confirm('Some cards have warnings. Continue anyway?')) {\n    return;\n  }\n\n  // Proceed with save...\n};\n\n// Auto-fix handler\nconst handleAutoFix = (conceptId: string, issue: ValidationIssue) => {\n  if (!issue.autoFix) return;\n\n  const splitCards = issue.autoFix();\n  // Show preview modal with split cards\n  // Allow user to review and confirm\n  // Replace single concept with multiple concepts\n};\n```\n\nSETTINGS FOR VALIDATION RULES:\nAdd to settings UI:\n- Enable/disable specific validation rules\n- Adjust severity levels (warning vs error)\n- Store preferences in localStorage\n\nVALIDATION HISTORY TRACKING:\n- Track which warnings users dismiss in review_logs or separate validation_logs table\n- Analytics: which validation rules are most often dismissed\n- Helps refine validation logic over time",
        "testStrategy": "Test pattern-matching detection: create card with 'Which of the following is a cause of chest pain? A) MI B) PE', verify ERROR validation blocks save. Test multi-fact: create card with 'Acute MI is caused by coronary occlusion and presents with chest pain, diaphoresis, and elevated troponins', verify WARNING appears suggesting split into 3 cards. Click 'Fix automatically', verify split preview shows 3 separate cards. Test medical list: create card with 'Causes of chest pain: 1. MI 2. PE 3. Pneumothorax', verify WARNING suggests clinical vignettes. Test empty field: attempt to save card with empty back, verify ERROR blocks save. Test warning dismissal: dismiss multi-fact warning, verify save proceeds. Test auto-fix split: accept split suggestion, verify multiple cards created. Test settings: disable pattern-matching validation, verify rule no longer enforced. Test validation history tracking after implementation.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-05T15:14:47.536Z"
      },
      {
        "id": "28",
        "title": "AI Settings Configuration UI",
        "description": "Create Settings page section for AI provider configuration with provider selection, API keys, model dropdowns, connection testing, and persistence",
        "details": "Create AI configuration UI in Settings:\n\n1. PROVIDER SELECTION:\n- Radio group: Ollama (local), OpenAI, Anthropic, DeepSeek\n- Show provider status indicator (connected/disconnected)\n- Auto-detect Ollama on page load\n\n2. API KEY INPUTS:\n- Secure password inputs for cloud provider API keys\n- Keys stored in electron-store (encrypted) not SQLite\n- Show/hide toggle for each key\n- Validation on blur (format check)\n\n3. MODEL SELECTION:\n- Dropdown per provider with available models\n- Ollama: fetch from localhost:11434/api/tags\n- OpenAI: gpt-4o, gpt-4o-mini, gpt-3.5-turbo\n- Anthropic: claude-3-5-sonnet, claude-3-haiku\n- DeepSeek: deepseek-chat, deepseek-coder\n\n4. CONNECTION TEST:\n- 'Test Connection' button per provider\n- Shows success/failure with response time\n- Validates API key works\n\n5. TIMEOUT/RETRY SETTINGS:\n- Timeout slider: 3s-30s (default 10s local, 3s cloud)\n- Max retries: 1-5 (default 3)\n\nSTORAGE:\n- Use electron-store for settings persistence\n- Create electron/settings-store.ts\n- IPC handlers: settings:get, settings:set\n\nINTEGRATION:\n- Update ai-service.ts to read from settings-store\n- Restart AI client when settings change",
        "testStrategy": "Open Settings, verify AI section displays. Select OpenAI, enter API key, click Test Connection, verify success/failure. Change model, verify persists after app restart. Test Ollama auto-detection. Verify invalid API key shows error.",
        "priority": "medium",
        "dependencies": [
          "2"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create electron-store settings persistence layer",
            "description": "Install electron-store and create settings-store.ts for encrypted AI settings storage, separate from SQLite database",
            "dependencies": [],
            "details": "Install electron-store package via npm. Create electron/settings-store.ts module that initializes Store instance with schema for AI provider settings (provider type, API keys, models, timeout, maxRetries). Keys must be encrypted using electron-store's built-in encryption. Define TypeScript interfaces for AISettings matching the settings structure from task description. Export functions: getSettings(), updateSettings(partial), resetSettings(). Settings schema should include: provider ('ollama' | 'openai' | 'anthropic' | 'deepseek'), apiKeys (object with keys per provider), selectedModels (object with model per provider), timeout (number, 3-30s), maxRetries (number, 1-5). Use file location: electron/settings-store.ts. Reference existing ai-service.ts at electron/ai-service.ts:1-959 for provider types and configuration structure.",
            "status": "pending",
            "testStrategy": "Unit test getSettings returns default values on first load. Test updateSettings persists changes and getSettings retrieves them. Verify API keys are encrypted in the store file. Test resetSettings returns to defaults.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add settings IPC handlers for renderer communication",
            "description": "Extend electron IPC handlers and preload script with settings:get and settings:set operations for secure settings access from React UI",
            "dependencies": [
              1
            ],
            "details": "In electron/ipc-handlers.ts, add two new IPC handlers: ipcMain.handle('settings:get', () => getSettings()) and ipcMain.handle('settings:set', (_, updates) => updateSettings(updates)). Import getSettings and updateSettings from settings-store.ts. In electron/preload.ts:87, add settings object to api with get() and set(updates) methods that invoke the IPC channels. In src/types/electron.d.ts:52, add settings interface to ElectronAPI type with matching method signatures returning Promise<IpcResult<AISettings>>. Follow existing IPC pattern seen in electron/preload.ts:62-86 for ai handlers and src/types/electron.d.ts:106-130 for AI type definitions. Ensure proper typing for settings updates (Partial<AISettings>).",
            "status": "pending",
            "testStrategy": "Test window.api.settings.get() returns settings from main process. Test window.api.settings.set() updates and persists settings. Verify IPC communication works bidirectionally. Test type safety in TypeScript.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Build AI Settings UI component with provider selection and configuration",
            "description": "Create SettingsInterface.tsx React component with tabbed layout, AI provider selection radio group, API key inputs, model dropdowns, and connection testing UI",
            "dependencies": [
              2
            ],
            "details": "Create src/components/settings/SettingsInterface.tsx component. Use shadcn/ui components: Tabs for sections (AI, FSRS, Data), RadioGroup (src/components/ui/radio-group.tsx) for provider selection (Ollama, OpenAI, Anthropic, DeepSeek), Input for API keys with type='password' and show/hide toggle (Eye icon from lucide-react), Select for model dropdowns, Button for test connection, Slider (src/components/ui/slider.tsx) for timeout (3-30s) and max retries (1-5). Layout: AI tab contains provider selection at top, conditional API key input (hidden for Ollama), model dropdown populated based on provider (Ollama: fetch from localhost:11434/api/tags, OpenAI: hardcoded list [gpt-4o, gpt-4o-mini, gpt-3.5-turbo], Anthropic: [claude-3-5-sonnet, claude-3-haiku], DeepSeek: [deepseek-chat, deepseek-coder]), Test Connection button with loading state and success/error toast feedback, timeout/retry sliders at bottom. Use useAppStore or create local state for form values. Call window.api.settings.get() on mount, window.api.settings.set() on save. Display provider status indicator (green dot for connected, red for disconnected) using window.api.ai.getProviderStatus() from electron/preload.ts:63. Reference existing CaptureInterface at src/components/capture/CaptureInterface.tsx:1-289 for component structure patterns.",
            "status": "pending",
            "testStrategy": "Render component, verify AI tab displays with all UI elements. Select different providers, verify correct model options shown. Enter API key for OpenAI, click Test Connection, verify success/failure toast. Change timeout slider, verify value updates. Save settings, reload app, verify settings persisted. Test Ollama auto-detection on page load.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate settings with ai-service and enable dynamic reconfiguration",
            "description": "Update ai-service.ts to read configuration from settings-store instead of environment variables, and implement settings change handlers to reinitialize AI client",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Modify electron/ai-service.ts:217-232 getProviderConfig() to call getSettings() from settings-store and use stored values as priority over env vars (settings > env > presets). Update electron/ai-service.ts:251-277 initializeClient() to accept optional forceReinit parameter that clears the singleton and recreates client with new config. In electron/ipc-handlers.ts, create handler for settings:set that calls updateSettings() and then aiService.initializeClient(undefined, true) to restart client. Update electron/ai-service.ts:159-203 detectProvider() to check settings-store first before trying auto-detection. Add settings change watcher in settings-store.ts that emits event when settings change (use EventEmitter pattern). Settings should include defaults: timeout: 10s for Ollama, 3s for cloud providers, maxRetries: 3. Ensure existing ai-service.ts exports (lines 949-958) remain unchanged. Follow existing PROVIDER_PRESETS pattern at electron/ai-service.ts:111-144.",
            "status": "pending",
            "testStrategy": "Change AI provider in settings UI, verify ai-service reinitializes with new provider. Test API calls use new settings (timeout, model). Verify Ollama detection still works when settings have no provider set. Test settings persist after app restart and ai-service loads them correctly. Verify env vars still work as fallback when settings are empty.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "29",
        "title": "Quick Model Switcher in Capture Interface",
        "description": "Add model selector dropdown to CaptureInterface header for quick switching between configured AI models without navigating to Settings",
        "details": "Add compact model switcher to CaptureInterface:\n\n1. UI PLACEMENT:\n- Small dropdown in top-right of capture card header\n- Shows current model name (truncated)\n- Chevron indicator for dropdown\n\n2. DROPDOWN CONTENTS:\n- List available models from current provider\n- Show provider icon next to each\n- Checkmark on currently selected\n- 'Configure in Settings' link at bottom\n\n3. QUICK SWITCH:\n- Selecting model updates ai-service immediately\n- No page reload required\n- Toast confirmation: 'Switched to [model]'\n\n4. PROVIDER INDICATOR:\n- Small badge showing provider (Ollama/OpenAI/etc)\n- Color-coded: green=local, blue=cloud\n\n5. OFFLINE HANDLING:\n- If Ollama not running, show 'Ollama offline' with retry button\n- If cloud key missing, show 'Configure API key' link\n\nCOMPONENT:\n- Create src/components/capture/ModelSwitcher.tsx\n- Uses shadcn Select or DropdownMenu\n- Fetches models via window.api.ai.getAvailableModels()",
        "testStrategy": "Open Capture view, verify model switcher visible in header. Click dropdown, verify shows available models. Select different model, verify toast appears and subsequent extractions use new model. Test with Ollama offline, verify appropriate message.",
        "priority": "low",
        "dependencies": [
          "28"
        ],
        "status": "deferred",
        "subtasks": []
      },
      {
        "id": "30",
        "title": "Recommended Ollama Models for Medical Extraction",
        "description": "Research, document, and configure optimal Ollama models for medical concept extraction - one GPU-optimized model and one lightweight CPU model",
        "details": "Research and configure medical-optimized models:\n\n1. GPU MODEL (Heavy Hitter):\n- Research options: meditron:70b, medllama2:13b, llama3:70b, mixtral:8x22b\n- Evaluate on medical terminology extraction accuracy\n- Document VRAM requirements\n- Add to PROVIDER_PRESETS as 'ollama-medical-gpu'\n\n2. CPU MODEL (Lightweight):\n- Research options: phi3:mini, llama3.2:3b, qwen2:1.5b, gemma2:2b\n- Must run on 8GB RAM laptop\n- Acceptable accuracy for basic extraction\n- Add to PROVIDER_PRESETS as 'ollama-medical-cpu'\n\n3. MODEL DOWNLOAD HELPER:\n- Add 'Download recommended models' button in Settings\n- Executes: ollama pull [model]\n- Shows download progress\n- IPC handler: ai:downloadModel(modelName)\n\n4. AUTO-SELECT LOGIC:\n- Detect available VRAM via Electron\n- If >8GB VRAM: default to GPU model\n- If <8GB VRAM: default to CPU model\n- User can override in settings\n\n5. DOCUMENTATION:\n- Add docs/AI-MODELS.md with recommendations\n- Include benchmark results\n- Installation instructions\n\nPROMPT OPTIMIZATION:\n- Test prompts with each model\n- Adjust extractConcepts prompt for smaller models\n- May need more explicit instructions for lightweight models",
        "testStrategy": "Download recommended GPU model, run extraction on medical text, verify quality. Download CPU model, run same test, compare. Test on machine with <8GB VRAM, verify CPU model auto-selected. Test download progress UI.",
        "priority": "medium",
        "dependencies": [
          "28"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Research and document recommended Ollama models for medical extraction",
            "description": "Research GPU-optimized and CPU-optimized Ollama models suitable for medical concept extraction, documenting VRAM requirements, accuracy benchmarks, and installation instructions",
            "dependencies": [],
            "details": "Create comprehensive documentation file docs/AI-MODELS.md with:\n\n1. GPU MODEL RECOMMENDATIONS:\n- Research meditron:70b, medllama2:13b, llama3:70b, mixtral:8x22b\n- Document VRAM requirements for each model\n- Test extraction quality on sample medical text (e.g., 'DDx for chest pain')\n- Benchmark accuracy for medical terminology extraction\n- Recommend best GPU model based on accuracy vs. resource tradeoffs\n\n2. CPU MODEL RECOMMENDATIONS:\n- Research phi3:mini, llama3.2:3b, qwen2:1.5b, gemma2:2b\n- Verify each model runs on 8GB RAM laptop\n- Test extraction quality with same sample medical text\n- Document acceptable accuracy degradation vs. GPU models\n- Recommend best lightweight CPU model\n\n3. DOCUMENTATION STRUCTURE:\n- Overview of why model selection matters\n- GPU model section: name, VRAM requirement, accuracy rating, use case\n- CPU model section: name, RAM requirement, accuracy rating, use case\n- Installation instructions: ollama pull commands\n- Benchmark results table comparing extraction quality\n- Prompt optimization notes for each model type\n\nFile location: docs/AI-MODELS.md\nReference existing PROVIDER_PRESETS in electron/ai-service.ts:111-144",
            "status": "pending",
            "testStrategy": "Review docs/AI-MODELS.md for completeness. Verify all recommended models exist in Ollama registry. Test installation commands work. Validate VRAM/RAM requirements are accurate.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add GPU and CPU model presets to PROVIDER_PRESETS configuration",
            "description": "Extend PROVIDER_PRESETS in ai-service.ts with two new Ollama configurations: ollama-medical-gpu for high-performance systems and ollama-medical-cpu for resource-constrained systems",
            "dependencies": [
              1
            ],
            "details": "Update electron/ai-service.ts PROVIDER_PRESETS object:\n\n1. ADD GPU PRESET (based on research from subtask 1):\n```typescript\n'ollama-medical-gpu': {\n  type: 'openai-compatible',\n  baseURL: 'http://localhost:11434/v1',\n  apiKey: 'ollama',\n  model: '[selected-gpu-model]', // e.g., 'llama3:70b' or 'meditron:70b'\n  timeout: 60000, // 60s for large models\n  isLocal: true,\n}\n```\n\n2. ADD CPU PRESET (based on research from subtask 1):\n```typescript\n'ollama-medical-cpu': {\n  type: 'openai-compatible',\n  baseURL: 'http://localhost:11434/v1',\n  apiKey: 'ollama',\n  model: '[selected-cpu-model]', // e.g., 'phi3:mini' or 'llama3.2:3b'\n  timeout: 45000, // 45s for lightweight models\n  isLocal: true,\n}\n```\n\n3. UPDATE TYPE DEFINITIONS:\n- Update AIProviderType in src/types/ai.ts:13 to include 'ollama-medical-gpu' | 'ollama-medical-cpu'\n- Ensure backward compatibility with existing 'ollama' preset\n\n4. ADJUST PROMPTS IF NEEDED:\n- Test PROMPTS.conceptExtraction (line 363) with lightweight models\n- Add model-specific prompt variations if smaller models need more explicit instructions\n\nFiles to modify:\n- electron/ai-service.ts (PROVIDER_PRESETS and possibly PROMPTS)\n- src/types/ai.ts (AIProviderType)",
            "status": "pending",
            "testStrategy": "Verify TypeScript compiles without errors. Test both presets with sample medical content extraction. Confirm GPU preset works with high VRAM model, CPU preset works with lightweight model. Validate timeout values are appropriate.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement model download helper with IPC handlers and progress UI",
            "description": "Create IPC handler for downloading Ollama models via 'ollama pull' command, add download button to settings interface, and show real-time download progress to user",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement model download functionality across Electron main and renderer processes:\n\n1. ELECTRON IPC HANDLER (electron/ipc-handlers.ts):\n- Add handler: ipcMain.handle('ai:downloadModel', async (_, modelName: string))\n- Execute: spawn('ollama', ['pull', modelName])\n- Stream stdout/stderr to track download progress\n- Parse Ollama progress output (e.g., 'downloading layer X/Y')\n- Return progress updates via IPC streaming or polling pattern\n- Handle errors: Ollama not installed, network issues, invalid model name\n\n2. PRELOAD BRIDGE (electron/preload.ts):\n- Add ai.downloadModel(modelName: string): Promise<void>\n- Add ai.onDownloadProgress(callback: (progress: number) => void)\n\n3. TYPE DEFINITIONS (src/types/electron.d.ts):\n- Extend ElectronAPI interface with downloadModel method\n- Define DownloadProgress type with percentage and status\n\n4. SETTINGS UI (create src/components/settings/ModelDownloader.tsx):\n- Display recommended GPU and CPU models from docs/AI-MODELS.md\n- 'Download' button next to each model\n- Progress bar showing download percentage\n- Toast notifications for success/failure\n- Disable button while downloading\n- Show installed models with checkmark\n\n5. INTEGRATE INTO APP:\n- Add ModelDownloader component to settings/preferences view\n- Position near AI provider configuration\n\nFiles to create/modify:\n- electron/ipc-handlers.ts (new handler)\n- electron/preload.ts (new API method)\n- src/types/electron.d.ts (type definitions)\n- src/components/settings/ModelDownloader.tsx (new component)",
            "status": "pending",
            "testStrategy": "Test download flow: click 'Download' button for recommended GPU model, verify progress bar updates, confirm model appears in Ollama after completion. Test error cases: Ollama not running, invalid model name, network disconnection. Verify toast messages show appropriate feedback.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement auto-select logic based on available system VRAM",
            "description": "Detect available GPU VRAM at startup using Electron system APIs, automatically select GPU or CPU model preset based on memory availability, and allow user override in settings",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Add intelligent model selection based on system capabilities:\n\n1. VRAM DETECTION (electron/ai-service.ts):\n- Create async function detectSystemVRAM(): Promise<number>\n- Use Electron's screen.getPrimaryDisplay() for basic GPU info\n- For Windows: execute 'wmic path win32_VideoController get AdapterRAM' via child_process\n- For macOS: execute 'system_profiler SPDisplaysDataType' and parse VRAM\n- For Linux: read /proc/driver/nvidia/gpus/*/information or use lspci\n- Return VRAM in GB (0 if detection fails)\n- Cache result to avoid repeated system calls\n\n2. AUTO-SELECT LOGIC (electron/ai-service.ts):\n- Update detectProvider() or create detectOptimalProvider()\n- Call detectSystemVRAM() on first initialization\n- If VRAM >= 8GB: default to 'ollama-medical-gpu'\n- If VRAM < 8GB: default to 'ollama-medical-cpu'\n- If detection fails: default to existing 'ollama' preset\n- Log selection reasoning to console\n\n3. USER OVERRIDE (add to settings UI):\n- Display detected VRAM amount in settings\n- Show auto-selected model preset with explanation\n- Dropdown to manually override: 'Auto-detect' | 'GPU Model' | 'CPU Model'\n- Save preference to localStorage or config file\n- Respect user override in getProviderConfig()\n\n4. IPC HANDLERS:\n- Add 'ai:getSystemInfo' handler returning { vram: number, autoSelectedProvider: string }\n- Add 'ai:setProviderOverride' handler for user preference\n\n5. SETTINGS DISPLAY:\n- Show: 'Detected VRAM: X GB → Using [model-name]'\n- Allow override with explanation of tradeoffs\n\nFiles to modify:\n- electron/ai-service.ts (detection and auto-select logic)\n- electron/ipc-handlers.ts (new handlers)\n- src/components/settings/ModelDownloader.tsx (display system info and override option)",
            "status": "pending",
            "testStrategy": "Test on high-VRAM machine (>8GB): verify GPU model auto-selected. Test on low-VRAM machine (<8GB): verify CPU model auto-selected. Test manual override: select CPU model on GPU machine, verify subsequent extractions use CPU model. Test VRAM detection across Windows/macOS/Linux if possible.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "31",
        "title": "BUG FIX: Quick Dump saves to notes table instead of quick_dumps table",
        "description": "QuickDumpModal incorrectly saves content as a regular note instead of to the quick_dumps table, breaking the extraction queue workflow",
        "details": "CURRENT BEHAVIOR:\n- QuickDumpModal.tsx line 74 calls window.api.notes.create()\n- Shows 'Saved to queue' toast but actually saves as regular note with 'quick-dump' tag\n- This breaks Task 8 (Extraction Queue) which expects data in quick_dumps table\n\nEXPECTED BEHAVIOR:\n- Should save to quick_dumps table with extractionStatus='pending'\n- Extraction Queue (Task 8) can then process it later\n\nFIX REQUIRED:\n1. Update handleSave() in src/components/modals/QuickDumpModal.tsx\n2. Change window.api.notes.create(note) to window.api.quickDumps.create(quickDump)\n3. Structure: { id: uuid, content: trimmedContent, extractionStatus: 'pending', createdAt: now }\n4. Remove the misleading Note creation with cardIds/tags\n5. Update success toast to accurately reflect what was saved\n\nDEPENDENCIES:\n- Requires Task 1 (Database schema) to be complete for quick_dumps table\n- Blocks Task 8 (Extraction queue) functionality",
        "testStrategy": "Open Quick Dump modal, paste content, click Save. Query quick_dumps table directly - verify row exists with status='pending'. Verify notes table does NOT have new 'Quick Dump' entry. Open app again, verify extraction queue shows the saved quick dump.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-06T02:11:06.148Z"
      },
      {
        "id": "32",
        "title": "POST-MVP: Space bar continues to next card after showing answer (auto-grade as Good)",
        "description": "Add convenience feature where pressing Space after answer is visible auto-grades as 'Good' and moves to next card, reducing friction in review flow",
        "details": "CURRENT BEHAVIOR:\n- Space bar shows answer when answerVisible=false\n- Space bar does nothing when answerVisible=true (lines 154-158 in ReviewInterface.tsx)\n- User must press 1-4 keys or click buttons to grade\n\nREQUESTED BEHAVIOR:\n- Space shows answer (unchanged)\n- Space again after answer visible = auto-grade as Rating.Good and move to next card\n- This is a convenience shortcut for 'I knew it' flow\n\nIMPLEMENTATION:\n1. Update keyboard handler in ReviewInterface.tsx lines 154-158\n2. Add else branch: if (answerVisible && !isSubmitting) { handleRating(Rating.Good); }\n3. Consider: should this be configurable? Some users may want Space=Again for harder study\n\nUX CONSIDERATION:\n- This changes Space from 'show answer only' to 'show answer OR grade good'\n- Could cause accidental grading if user double-taps Space\n- Consider adding small delay or different key (Enter) for grade-and-continue\n\nALTERNATIVE APPROACH (Task 5 conflict):\n- Task 5 removes grading buttons entirely and auto-grades based on response time\n- This task may be superseded by Task 5's implementation\n- Mark as deferred until Task 5 design is finalized\n\nNOTE: Marking as post-MVP since Task 5 (zero-decision review) may make this unnecessary.",
        "testStrategy": "Review card, press Space to show answer, press Space again, verify card grades as Good and moves to next. Test rapid double-Space doesn't skip cards. Verify works with keyboard only (no mouse).",
        "priority": "low",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "33",
        "title": "Back button with navigation history persistence",
        "description": "Add a back button allowing users to return to their previous screen with state persistence",
        "details": "REQUIREMENTS:\n- Add back button to header/navigation that returns user to previous view\n- Maintain navigation history stack in useAppStore\n- Persist navigation state so users can go back after actions\n\nIMPLEMENTATION:\n1. Add navigationHistory: ViewType[] to useAppStore\n2. Add pushView(view: ViewType) action that pushes current view before navigating\n3. Add goBack() action that pops and navigates to previous view\n4. Update setCurrentView to use pushView internally\n5. Add BackButton component to Header (src/components/layout/Header.tsx)\n6. Show back button only when history.length > 0\n7. Keyboard shortcut: Alt+Left or Backspace (when not in input)\n\nSTATE PERSISTENCE:\n- Consider persisting history to localStorage for session continuity\n- Clear history on app restart or limit to last 10 views\n- Don't persist if same view pushed twice consecutively\n\nEDGE CASES:\n- Review session in progress: going back should preserve review position\n- Unsaved changes in Capture: prompt before navigating away\n- Modal open: back should close modal first, not navigate",
        "testStrategy": "Navigate Capture → Review → Settings, click back, verify returns to Review. Click back again, verify returns to Capture. Test keyboard shortcut. Test that review session position is preserved when returning.",
        "priority": "low",
        "dependencies": [],
        "status": "deferred",
        "subtasks": []
      },
      {
        "id": "34",
        "title": "Modern cloze UI with click-based creation and cleaner review display",
        "description": "Replace {{c1::}} syntax with intuitive click/select-based cloze creation and modern review rendering",
        "details": "PROBLEM:\n- Current cloze syntax {{c1::answer}} is clunky to type\n- Requires memorizing syntax format\n- Review display shows raw {{}} markers which look dated\n\nCLOZE CREATION UX:\n1. User selects text in capture textarea\n2. Click 'Make Cloze' button or press Ctrl+Shift+C\n3. Selected text is wrapped in cloze markers automatically\n4. Visual indicator shows cloze regions (highlight/underline)\n5. Multiple clozes: subsequent selections become c2, c3, etc.\n6. Click existing cloze to remove/edit\n\nCOMPONENTS:\n- src/components/capture/ClozeEditor.tsx - Rich text editor with cloze support\n- Toolbar with cloze button (or context menu on selection)\n- Visual preview showing blanks vs answers\n\nREVIEW DISPLAY:\n- Clean blank rendering: underlined space or pill-shaped blank\n- No visible {{}} markers during review\n- Answer reveal: smooth transition, highlighted answer\n- Already partially implemented in cloze-renderer.tsx\n\nSTORAGE:\n- Keep {{c1::}} format in database for compatibility\n- Transform on display only\n- ClozeEditor converts selection to syntax on save\n\nINSPIRATION:\n- Anki's cloze creation workflow\n- Notion's inline formatting\n- Modern quiz apps with tap-to-reveal",
        "testStrategy": "Create card, select text 'mitochondria', click Make Cloze, verify {{c1::mitochondria}} in storage. Review card, verify clean blank display. Reveal answer, verify smooth highlight. Test multiple clozes per card.",
        "priority": "medium",
        "dependencies": [],
        "status": "deferred",
        "subtasks": []
      },
      {
        "id": "35",
        "title": "[v2-T1] Data model v3: SourceItem, CanonicalTopic, NotebookTopicPage, SmartView schemas",
        "description": "Implement the v2 architecture data model with new tables for 3-layer system: Knowledge Bank (SourceItem), Notebook (NotebookTopicPage, NotebookBlock), and enhanced Cards with provenance. Add CanonicalTopic for topic normalization and SmartView for filtered access.",
        "details": "NEW TABLES IN electron/database.ts:\n\n1. source_items table:\n- id (UUID PRIMARY KEY)\n- sourceType ('qbank'|'article'|'pdf'|'image'|'audio'|'quickcapture'|'manual')\n- sourceName (TEXT) - e.g., 'UWorld', 'UpToDate'\n- sourceUrl (TEXT nullable)\n- title (TEXT) - AI-suggested, user-editable\n- rawContent (TEXT) - Original text/markdown\n- mediaPath (TEXT nullable) - For binary files\n- transcription (TEXT nullable) - OCR/audio deferred\n- canonicalTopicIds (TEXT) - JSON array of topic IDs\n- tags (TEXT) - JSON array\n- questionId (TEXT nullable) - For QBank sources\n- status ('inbox'|'processed'|'curated')\n- createdAt (TEXT)\n- processedAt (TEXT nullable)\n\n2. canonical_topics table:\n- id (UUID PRIMARY KEY)\n- canonicalName (TEXT UNIQUE) - e.g., 'Hypertrophic Cardiomyopathy'\n- aliases (TEXT) - JSON array ['HOCM', 'HCM']\n- domain (TEXT) - e.g., 'cardiology'\n- parentTopicId (TEXT nullable FK)\n- createdAt (TEXT)\n\n3. notebook_topic_pages table:\n- id (UUID PRIMARY KEY)\n- canonicalTopicId (TEXT FK to canonical_topics)\n- cardIds (TEXT) - JSON array of card IDs\n- createdAt (TEXT)\n- updatedAt (TEXT)\n\n4. notebook_blocks table:\n- id (UUID PRIMARY KEY)\n- notebookTopicPageId (TEXT FK)\n- sourceItemId (TEXT FK to source_items)\n- content (TEXT) - Excerpt text\n- annotations (TEXT nullable)\n- mediaPath (TEXT nullable)\n- position (INTEGER)\n\n5. smart_views table:\n- id (UUID PRIMARY KEY)\n- name (TEXT)\n- icon (TEXT)\n- filter (TEXT) - JSON SmartViewFilter\n- sortBy (TEXT)\n- isSystem (INTEGER boolean)\n\nUPDATE cards table:\n- ADD notebookTopicPageId (TEXT FK) - REQUIRED for new cards\n- ADD sourceBlockId (TEXT nullable FK to notebook_blocks)\n\nINDEXES:\n- idx_source_items_status\n- idx_source_items_sourceType\n- idx_canonical_topics_domain\n- idx_notebook_blocks_page\n- idx_cards_notebook_page\n\nTYPES in src/types/index.ts:\n- SourceItem, CanonicalTopic, NotebookTopicPage, NotebookBlock, SmartView, SmartViewFilter interfaces\n\nMIGRATION:\n- Increment user_version\n- Create all new tables\n- Add new columns to cards (nullable for existing cards)",
        "testStrategy": "Verify all tables created via SQLite browser. Test CRUD for each table. Verify indexes via EXPLAIN QUERY PLAN. Test foreign key constraints. Verify existing cards unaffected by migration.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define TypeScript interfaces for v3 data model",
            "description": "Create TypeScript interfaces in src/types/index.ts for SourceItem, CanonicalTopic, NotebookTopicPage, NotebookBlock, SmartView, and SmartViewFilter. Update Card interface with optional provenance fields.",
            "dependencies": [],
            "details": "Add to src/types/index.ts:\n\n1. SourceType = 'qbank' | 'article' | 'pdf' | 'image' | 'audio' | 'quickcapture' | 'manual'\n2. SourceItemStatus = 'inbox' | 'processed' | 'curated'\n3. SourceItem interface with all fields (id, sourceType, sourceName, sourceUrl, title, rawContent, mediaPath, transcription, canonicalTopicIds as string[], tags as string[], questionId, status, createdAt, processedAt, updatedAt)\n4. CanonicalTopic interface (id, canonicalName, aliases as string[], domain, parentTopicId, createdAt)\n5. NotebookTopicPage interface (id, canonicalTopicId, cardIds as string[], createdAt, updatedAt)\n6. NotebookBlock interface (id, notebookTopicPageId, sourceItemId, content, annotations, mediaPath, position)\n7. SmartViewFilter interface with domain, tags, status, createdAfter, createdBefore fields\n8. SmartView interface (id, name, icon, filter as SmartViewFilter, sortBy, isSystem)\n9. Update Card interface: add notebookTopicPageId?: string and sourceBlockId?: string (optional for backwards compat)\n\nAll new types exported from index.ts. Follow existing pattern with JSON array fields stored as string[] in TypeScript layer.",
            "status": "pending",
            "testStrategy": "Verify TypeScript compilation passes. Ensure all new types are properly exported. Check that Card interface extension maintains backwards compatibility with existing card objects.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create database schema v3 with migration from v2",
            "description": "Implement new tables (source_items, canonical_topics, notebook_topic_pages, notebook_blocks, smart_views) in electron/database.ts with migration from schema v2 to v3.",
            "dependencies": [
              1
            ],
            "details": "In electron/database.ts:\n\n1. Create migrateToV3() function following migrateToV2 pattern with backup/restore safety\n2. Add source_items table: id TEXT PRIMARY KEY, sourceType TEXT, sourceName TEXT, sourceUrl TEXT, title TEXT, rawContent TEXT, mediaPath TEXT, transcription TEXT, canonicalTopicIds TEXT (JSON), tags TEXT (JSON), questionId TEXT, status TEXT DEFAULT 'inbox', createdAt TEXT, processedAt TEXT, updatedAt TEXT\n3. Add canonical_topics table: id TEXT PRIMARY KEY, canonicalName TEXT UNIQUE, aliases TEXT (JSON), domain TEXT, parentTopicId TEXT FK, createdAt TEXT\n4. Add notebook_topic_pages table: id TEXT PRIMARY KEY, canonicalTopicId TEXT FK, cardIds TEXT (JSON), createdAt TEXT, updatedAt TEXT\n5. Add notebook_blocks table: id TEXT PRIMARY KEY, notebookTopicPageId TEXT FK, sourceItemId TEXT FK, content TEXT, annotations TEXT, mediaPath TEXT, position INTEGER\n6. Add smart_views table: id TEXT PRIMARY KEY, name TEXT, icon TEXT, filter TEXT (JSON), sortBy TEXT, isSystem INTEGER\n7. ALTER TABLE cards ADD COLUMN notebookTopicPageId TEXT, ADD COLUMN sourceBlockId TEXT (nullable for existing cards)\n8. Create indexes: idx_source_items_status, idx_source_items_sourceType, idx_canonical_topics_domain, idx_notebook_blocks_page, idx_cards_notebook_page\n9. Migrate existing quick_dumps to source_items with sourceType='quickcapture', status='inbox', map content→rawContent, createdAt→createdAt\n10. Call migrateToV3 in initDatabase() when version < 3, set version to 3 after successful migration\n\nUse transactions for safety, follow existing columnExists/tableExists pattern.",
            "status": "pending",
            "testStrategy": "Open database in SQLite browser after migration, verify all 5 new tables exist with correct schemas. Check indexes via EXPLAIN QUERY PLAN. Verify existing cards table has new nullable columns. Verify existing quick_dumps migrated to source_items with correct field mappings. Test migration rollback on simulated failure.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement CRUD functions for new tables",
            "description": "Add database query functions for SourceItem, NotebookTopicPage, NotebookBlock, CanonicalTopic, and SmartView in electron/database.ts following existing pattern.",
            "dependencies": [
              2
            ],
            "details": "In electron/database.ts, add query objects following cardQueries/noteQueries pattern:\n\n1. sourceItemQueries:\n   - getAll(): DbSourceItem[]\n   - getByStatus(status: SourceItemStatus): DbSourceItem[]\n   - getById(id: string): DbSourceItem | null\n   - insert(item: DbSourceItem): void\n   - update(id: string, updates: Partial<DbSourceItem>): void\n   - delete(id: string): void\n   Include parseSourceItemRow() to handle JSON fields (canonicalTopicIds, tags)\n\n2. notebookTopicPageQueries:\n   - getAll(): DbNotebookTopicPage[]\n   - getById(id: string): DbNotebookTopicPage | null\n   - insert(page: DbNotebookTopicPage): void\n   - update(id: string, updates: Partial<DbNotebookTopicPage>): void\n   Include parseNotebookTopicPageRow() for cardIds JSON array\n\n3. notebookBlockQueries:\n   - getByPage(pageId: string): DbNotebookBlock[]\n   - insert(block: DbNotebookBlock): void\n   - update(id: string, updates: Partial<DbNotebookBlock>): void\n   - delete(id: string): void\n\n4. canonicalTopicQueries:\n   - getAll(): DbCanonicalTopic[]\n   - getByDomain(domain: string): DbCanonicalTopic[]\n   Include parseCanonicalTopicRow() for aliases JSON array\n\n5. smartViewQueries:\n   - getAll(): DbSmartView[]\n   - getSystemViews(): DbSmartView[]\n   Include parseSmartViewRow() for filter JSON object\n\n6. seedSystemSmartViews() function called in initDatabase():\n   Create 7 system views: Inbox (status=inbox), Today (dueDate=today), Queue (state=0), Notebook (all notebook pages), Topics (group by topic), Stats (analytics), Weak Topics (low accuracy)\n\nDefine DbSourceItem, DbNotebookTopicPage, DbNotebookBlock, DbCanonicalTopic, DbSmartView interfaces matching db schema.",
            "status": "pending",
            "testStrategy": "Write unit tests for each CRUD operation. Test sourceItemQueries with all status values. Test notebookBlockQueries.getByPage with multiple blocks. Test canonicalTopicQueries.getByDomain filtering. Verify JSON fields correctly serialized/deserialized. Test seedSystemSmartViews creates exactly 7 views on fresh db.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Wire IPC handlers for new CRUD operations",
            "description": "Register IPC handlers in ipc-handlers.ts, expose via preload.ts, and declare types in electron.d.ts for all new database operations.",
            "dependencies": [
              3
            ],
            "details": "1. In electron/ipc-handlers.ts, add handlers following existing pattern:\n   - sourceItems:getAll, sourceItems:getByStatus, sourceItems:getById, sourceItems:create, sourceItems:update, sourceItems:remove\n   - notebookPages:getAll, notebookPages:getById, notebookPages:create, notebookPages:update\n   - notebookBlocks:getByPage, notebookBlocks:create, notebookBlocks:update, notebookBlocks:remove\n   - canonicalTopics:getAll, canonicalTopics:getByDomain\n   - smartViews:getAll, smartViews:getSystem\n   All handlers wrap in IpcResult<T> using success/failure helpers\n\n2. In electron/preload.ts, expose via contextBridge:\n   - window.api.sourceItems object with all methods\n   - window.api.notebookPages object\n   - window.api.notebookBlocks object\n   - window.api.canonicalTopics object\n   - window.api.smartViews object\n   Each method invokes corresponding IPC channel using ipcRenderer.invoke\n\n3. In src/types/electron.d.ts, extend ElectronAPI interface:\n   - Add sourceItems, notebookPages, notebookBlocks, canonicalTopics, smartViews with full method signatures\n   - Return types use IpcResult<SourceItem>, IpcResult<NotebookTopicPage>, etc.\n   - Import new types from './index'\n\nFollow exact pattern from existing cards/notes/quickDumps handlers for consistency.",
            "status": "pending",
            "testStrategy": "Build application, verify TypeScript compilation. Test each IPC call from renderer DevTools console (window.api.sourceItems.getAll(), etc.). Verify error handling returns proper IpcResult with error field. Test status filtering for sourceItems. Verify all methods properly typed in electron.d.ts with autocomplete working.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Update test helpers with new schema",
            "description": "Add new table creation to tests/helpers/db-helpers.ts ensuring test database schema matches production v3 schema.",
            "dependencies": [
              2
            ],
            "details": "If tests/helpers/db-helpers.ts exists, update it:\n\n1. Add source_items table creation in test schema initialization\n2. Add canonical_topics table creation\n3. Add notebook_topic_pages table creation  \n4. Add notebook_blocks table creation\n5. Add smart_views table creation\n6. Update cards table schema to include notebookTopicPageId and sourceBlockId columns\n7. Add all indexes matching production schema\n8. Ensure foreign key constraints match production\n9. Add helper functions: createTestSourceItem(), createTestNotebookPage(), createTestCanonicalTopic() for test data fixtures\n10. Set schema version to 3 in test initialization\n\nIf db-helpers.ts doesn't exist, create basic structure with schema matching electron/database.ts after v3 migration.\n\nTest schema must be identical to production to catch schema mismatch bugs early.",
            "status": "pending",
            "testStrategy": "Run existing test suite, verify all tests pass with new schema. Compare test schema with production schema field-by-field. Test helper fixture functions create valid test data. Verify foreign key constraints work in test environment. Check that test database version pragma shows 3.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "36",
        "title": "[v2-T1.1] Topic normalization: alias table, matching rules, dedupe prevention",
        "description": "Implement topic alias matching system that normalizes user input to canonical topics. 'HOCM', 'HCM', 'hypertrophic cardiomyopathy' should all resolve to the same CanonicalTopic.",
        "details": "CORE FUNCTIONS in electron/topic-service.ts:\n\n1. resolveTopicAlias(input: string): Promise<CanonicalTopic | null>\n- Normalize input (lowercase, trim)\n- Check canonicalName exact match first\n- Then check aliases array (case-insensitive)\n- Return matched CanonicalTopic or null\n\n2. createOrGetTopic(name: string, domain?: string): Promise<CanonicalTopic>\n- Try resolveTopicAlias first\n- If found, return existing topic\n- If not found, create new CanonicalTopic with name as canonicalName\n- Auto-generate common aliases (lowercase, acronyms)\n\n3. suggestTopicMatches(input: string): Promise<CanonicalTopic[]>\n- Fuzzy search across canonicalName and aliases\n- Return top 5 matches sorted by similarity\n- Use for autocomplete/typeahead\n\n4. addTopicAlias(topicId: string, alias: string): Promise<void>\n- Add alias to existing topic's aliases array\n- Prevent duplicates\n- Check alias doesn't belong to another topic\n\n5. mergeTopics(sourceId: string, targetId: string): Promise<void>\n- Move all aliases from source to target\n- Update all source_items referencing source topic\n- Update all notebook_topic_pages\n- Delete source topic\n\nIPC HANDLERS:\n- topic:resolve, topic:createOrGet, topic:suggest, topic:addAlias, topic:merge\n\nUI AUTOCOMPLETE:\n- When user types topic name, show suggestions\n- Allow creating new topic if no match\n- Show alias count/common aliases in dropdown",
        "testStrategy": "Test 'HOCM' resolves to same topic as 'Hypertrophic Cardiomyopathy'. Test fuzzy search finds 'cardio' matches 'cardiology'. Test merge preserves all references. Test dedupe prevents same alias on multiple topics.",
        "priority": "high",
        "dependencies": [
          "35"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "37",
        "title": "[v2-T1.2] Metadata schema: minimal fields, AI templates per source type",
        "description": "Define minimal required metadata fields per source type and create AI prompt templates for extracting/suggesting metadata from raw content.",
        "details": "METADATA TEMPLATES per sourceType:\n\n1. qbank:\n- Required: questionId, sourceName (UWorld/AMBOSS/etc)\n- AI extract: topic, key concept, correct answer rationale\n\n2. article:\n- Required: title, sourceUrl or sourceName\n- AI extract: main topic, key takeaways, publication year\n\n3. pdf:\n- Required: title\n- AI extract: document type (textbook/guidelines/paper), main topics\n\n4. image:\n- Required: title (AI-suggested from context/filename)\n- AI extract: image type (diagram/photo/chart), labeled structures\n\n5. audio:\n- Required: title\n- AI extract: main topics from transcription (deferred)\n\n6. quickcapture:\n- Required: none (raw dump)\n- AI extract: auto-detect content type, suggest title, tags\n\n7. manual:\n- Required: title\n- AI extract: topic, format suggestions\n\nIMPLEMENTATION in electron/ai-service.ts:\n\n- Add function: suggestSourceMetadata(content: string, sourceType: SourceType): Promise<SuggestedMetadata>\n- SuggestedMetadata = { title, topics: string[], tags: string[], questionId?, sourceUrl? }\n- Use source-type-specific prompts\n- Return confidence scores for each suggestion\n\nUI INTEGRATION:\n- On paste/import, call suggestSourceMetadata\n- Pre-fill form with AI suggestions\n- User can edit before single Save",
        "testStrategy": "Test qbank paste extracts questionId and topic. Test article URL extracts title. Test quick capture with mixed content gets reasonable title suggestion. Verify confidence scores help user decide whether to accept.",
        "priority": "medium",
        "dependencies": [
          "35"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "38",
        "title": "[v2-T2] Capture pipeline: Quick Dump → SourceItem (inbox), text + image support",
        "description": "Implement capture flow that creates SourceItems in Knowledge Bank with inbox status. Support text paste and image paste/drop.",
        "details": "CAPTURE FLOW:\n\n1. QUICK DUMP (Ctrl+Shift+S):\n- Open modal from anywhere\n- Paste text or image\n- Optional: add tags\n- Save creates SourceItem with status='inbox', sourceType='quickcapture'\n- Zero decisions, process later\n\n2. PASTE HANDLER:\n- Detect content type (text, image, URL)\n- For text: store in rawContent\n- For image: save to mediaPath, store path reference\n- For URL: attempt to extract title/content, store sourceUrl\n\n3. AI METADATA SUGGESTION:\n- On save, async call suggestSourceMetadata\n- Pre-populate title, topics, tags\n- Status remains 'inbox' until user processes\n\nCOMPONENTS:\n\n- src/components/capture/QuickDumpModal.tsx (update existing)\n  - Change from notes.create to sourceItems.create\n  - Add image paste/drop support\n  - Add optional tags input\n\n- src/components/capture/SourceItemForm.tsx (new)\n  - Full form for editing SourceItem metadata\n  - Used when processing inbox items\n  - Single Save button (no separate metadata confirm)\n\nIPC HANDLERS:\n- sourceItems:create, sourceItems:update, sourceItems:getByStatus, sourceItems:delete\n\nIMAGE HANDLING:\n- Save images to app data folder\n- Generate UUID-based filename\n- Store relative path in mediaPath\n- Display thumbnail in UI",
        "testStrategy": "Quick Dump text → verify SourceItem in inbox. Quick Dump image → verify image saved and mediaPath populated. Verify AI suggestions populate on save. Verify status='inbox' for all new captures.",
        "priority": "high",
        "dependencies": [
          "35"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update QuickDumpModal to create SourceItems instead of QuickDumps",
            "description": "Refactor QuickDumpModal.tsx to use sourceItems.create API, mapping text content to SourceItem schema with sourceType='quickcapture' and status='inbox'.",
            "dependencies": [],
            "details": "Modify src/components/modals/QuickDumpModal.tsx: Change from window.api.quickDumps.create to window.api.sourceItems.create. Map fields: content → rawContent, generate title from first 50 chars of content, set sourceType='quickcapture', status='inbox', sourceName='Quick Dump', tags=[], canonicalTopicIds=[]. Update SourceItem import from @/types. Keep existing UI/UX (textarea, Save/Cancel buttons, keyboard shortcuts).",
            "status": "done",
            "testStrategy": "Manual test: Open Quick Dump modal (Ctrl+Shift+S), paste text, verify SourceItem created in source_items table with correct fields. Check toast message shows 'Saved to inbox'.",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T02:38:03.709Z"
          },
          {
            "id": 2,
            "title": "Add image paste/drop support to QuickDumpModal",
            "description": "Implement clipboard paste and drag-drop handlers for images in QuickDumpModal, with visual preview and content type detection.",
            "dependencies": [
              1
            ],
            "details": "In QuickDumpModal.tsx: Add state for contentType ('text' | 'image'), imagePreview (base64 data URL). Add onPaste handler to detect image/png, image/jpeg from clipboard. Add onDrop/onDragOver handlers for image files. Display image preview with max-height constraint when image detected. On save: if image, convert to blob and prepare for file system save (subtask 3 handles actual file I/O). Update placeholder text to 'Paste text or image...'.",
            "status": "done",
            "testStrategy": "Test paste image from clipboard, verify preview shown. Test drag-drop image file, verify preview. Test switching between text/image content. Verify UI adapts appropriately.",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T02:54:36.818Z"
          },
          {
            "id": 3,
            "title": "Implement image file storage and mediaPath handling",
            "description": "Create IPC handler for saving image files to app data directory and returning relative path for mediaPath field.",
            "dependencies": [
              2
            ],
            "details": "Add new IPC handler 'files:saveImage' in electron/ipc-handlers.ts: Accept base64 image data, generate UUID-based filename (e.g., {uuid}.png), save to app.getPath('userData')/images/ directory, return relative path 'images/{uuid}.png'. Add corresponding preload.ts and electron.d.ts entries. In QuickDumpModal, call this API before sourceItems.create when contentType='image', store returned path in mediaPath field, leave rawContent empty or with placeholder text.",
            "status": "done",
            "testStrategy": "Test image paste → verify file saved to userData/images/ with UUID filename. Verify SourceItem.mediaPath contains correct relative path. Check filesystem permissions and error handling.",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T03:15:27.734Z"
          },
          {
            "id": 4,
            "title": "Add inboxCount state management to useAppStore",
            "description": "Extend useAppStore with inboxCount state that queries source_items table for items with status='inbox', and refreshes after captures.",
            "dependencies": [
              1
            ],
            "details": "In src/stores/useAppStore.ts: Add inboxCount: number to AppState (default 0). Add loadInboxCount action that calls window.api.sourceItems.getByStatus('inbox') and updates state with result.data.length. Call loadInboxCount in initialize() after seeding. Create refreshInboxCount action to re-query count. Ensure QuickDumpModal success flow triggers refreshInboxCount. Export getInboxCount selector for Sidebar consumption.",
            "status": "done",
            "testStrategy": "After creating SourceItem via Quick Dump, verify inboxCount increments. Check count persists across app restarts. Verify count shown in Sidebar badge updates reactively.",
            "updatedAt": "2026-01-06T02:33:30.360Z",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Wire Sidebar to display inbox badge count from useAppStore",
            "description": "Connect Sidebar component to useAppStore.inboxCount and replace hardcoded inboxCount=0 with live state.",
            "dependencies": [
              4
            ],
            "details": "In src/components/layout/Sidebar.tsx line 61: Replace 'const inboxCount = 0;' with 'const inboxCount = useAppStore((state) => state.inboxCount);'. Remove TODO comment. Verify Badge component already handles inboxCount display correctly (it does - see line 68). No other changes needed since NavItem structure already supports badge counts.",
            "status": "done",
            "testStrategy": "Create Quick Dump items, verify Sidebar Inbox badge shows correct count. Delete inbox items, verify count decreases. Test with 0, 1, and multiple items. Verify badge visibility rules (only shows when count > 0).",
            "parentId": "undefined",
            "updatedAt": "2026-01-06T02:33:30.460Z"
          }
        ],
        "updatedAt": "2026-01-06T03:15:27.734Z"
      },
      {
        "id": "39",
        "title": "[v2-T2.1] Inbox UI: persistent indicator, count badge, batch triage actions",
        "description": "Add persistent inbox indicator in sidebar showing count of unprocessed items. Enable batch selection and triage actions.",
        "details": "SIDEBAR INBOX VIEW:\n\n1. INBOX COUNT BADGE:\n- Show count next to Inbox in Smart Views sidebar\n- Update in real-time when items added/processed\n- Visual emphasis when count > 0 (dot indicator)\n\n2. INBOX LIST VIEW:\n- Vertical list grouped by date (Today, Yesterday, Earlier)\n- Each row shows: icon, title, source type, tags, time ago\n- Primary action: 'Add to Notebook'\n- Secondary action: 'Open' (view full content)\n- Destructive: delete (icon + confirm)\n\n3. BATCH ACTIONS:\n- Checkbox selection for multiple items\n- Select all / deselect all\n- Batch 'Add to Notebook' (select topic for all)\n- Batch delete (with confirmation)\n- Batch tag assignment\n\n4. FILTER/SORT:\n- Filter by source type\n- Sort by date (newest/oldest)\n- Search within inbox\n\nCOMPONENTS:\n\n- src/components/knowledgebank/InboxView.tsx\n- src/components/knowledgebank/SourceItemRow.tsx\n- src/components/knowledgebank/BatchActions.tsx\n\nSTORE ACTIONS in useAppStore:\n- inboxCount: number (derived from sourceItems)\n- selectedInboxItems: Set<string>\n- toggleInboxSelection, selectAllInbox, clearInboxSelection\n- batchAddToNotebook, batchDeleteInbox",
        "testStrategy": "Add 5 items via Quick Dump, verify inbox badge shows 5. Select 3 items, batch add to notebook, verify badge shows 2. Test batch delete. Test filter by source type.",
        "priority": "high",
        "dependencies": [
          "38"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "40",
        "title": "[v2-T3] Knowledge Bank UI: vertical list grouped by status, search, filters",
        "description": "Build the main Knowledge Bank view showing all SourceItems organized by status with filtering and search capabilities.",
        "details": "KNOWLEDGE BANK VIEW:\n\n1. STATUS GROUPS:\n- Inbox (status='inbox') - top priority, most prominent\n- Processed (status='processed') - middle section\n- Curated (status='curated') - items added to Notebook\n\n2. LIST LAYOUT (Vertical):\n- Icon by source type (📄 text, 🖼️ image, 🎤 audio, ⚡ quick)\n- Title (AI-generated or user-edited)\n- Tags as pills\n- Topic link (if assigned)\n- Relative time\n- Actions: [Add to Notebook ▼] [Open] [🗑️]\n\n3. SEARCH:\n- Search box at top\n- Searches: title, rawContent, tags, topic names\n- Real-time filtering as user types\n- <200ms response time\n\n4. FILTERS:\n- By status (multi-select)\n- By source type (multi-select)\n- By topic (dropdown)\n- By tag (dropdown)\n- By date range\n\n5. SORT OPTIONS:\n- Newest first (default)\n- Oldest first\n- Alphabetical\n- By topic\n\nCOMPONENTS:\n\n- src/components/knowledgebank/KnowledgeBankView.tsx (main view)\n- src/components/knowledgebank/StatusGroup.tsx\n- src/components/knowledgebank/FilterBar.tsx\n- src/components/knowledgebank/SourceItemRow.tsx (reuse from T2.1)\n\nNO CARD CREATION HERE:\n- 'Add to Notebook' is the only path to cards\n- No 'Create Card' button in Knowledge Bank",
        "testStrategy": "Add items of various types and statuses. Verify grouping correct. Test search finds content in rawContent. Test filters narrow results correctly. Verify <200ms search response.",
        "priority": "high",
        "dependencies": [
          "39"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "41",
        "title": "[v2-T4] Notebook UI: topic pages, add blocks from sources, enforce card-creation here only",
        "description": "Build the Notebook layer with topic pages containing blocks from Knowledge Bank. This is the ONLY place where cards can be created.",
        "details": "NOTEBOOK VIEW:\n\n1. TOPIC PAGE LIST:\n- Sidebar or list showing all NotebookTopicPages\n- Show: topic name, aliases, card count, source count\n- Search/filter by topic name\n- 'New Topic Page' button\n\n2. TOPIC PAGE VIEW:\n- Header: Topic name, aliases, stats\n- Blocks section: excerpts from SourceItems\n- Each block shows:\n  - Source reference (clickable deep link)\n  - Content excerpt\n  - Annotations (user notes)\n  - Actions: [Edit] [→ Source] [Generate Card]\n- Footer: [+ Add from Knowledge Bank] [Generate All Cards]\n\n3. ADD BLOCK FLOW:\n- Click '+ Add from Knowledge Bank'\n- Modal shows Knowledge Bank filtered to curated/processed\n- Select source item\n- Select excerpt (highlight text) or use full content\n- Save creates NotebookBlock linked to source\n- SourceItem status → 'curated'\n\n4. BLOCK MANAGEMENT:\n- Drag to reorder blocks (position field)\n- Edit block content (excerpt)\n- Add annotations\n- Delete block (doesn't delete source)\n\nCARD CREATION ENFORCED:\n- 'Generate Card' button ONLY appears on blocks\n- No card creation in Knowledge Bank\n- No card creation from scratch\n- All cards have notebookTopicPageId (enforced)\n\nCOMPONENTS:\n\n- src/components/notebook/NotebookView.tsx\n- src/components/notebook/TopicPageList.tsx\n- src/components/notebook/TopicPageView.tsx\n- src/components/notebook/NotebookBlock.tsx\n- src/components/notebook/AddBlockModal.tsx",
        "testStrategy": "Create topic page, add block from source, verify deep link works. Generate card from block, verify card has notebookTopicPageId. Verify no way to create card outside Notebook.",
        "priority": "high",
        "dependencies": [
          "40",
          "36"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "42",
        "title": "[v2-T4.1] Card generation from notebook: AI suggests, card-worthiness gate",
        "description": "Implement AI-powered card generation from Notebook blocks with card-worthiness evaluation before creation.",
        "details": "CARD GENERATION FLOW:\n\n1. USER INITIATES:\n- Click 'Generate Card' on a block\n- Or 'Generate All Cards' for entire topic page\n\n2. AI CARD SUGGESTION:\n- Call AI to analyze block content\n- Suggest card format (Q&A, cloze, vignette)\n- Generate front/back content\n- Return multiple card suggestions if content-rich\n\n3. CARD-WORTHINESS GATE (T5):\n- Before showing card to user, evaluate:\n  - Board-relevant? (high-yield for Step 2/3)\n  - Testable? (clear right answer)\n  - Discriminative? (distinguishes from similar concepts)\n- Show AI assessment with checkmarks/warnings\n- Recommendation: CREATE CARD / KEEP AS NOTE ONLY\n\n4. USER DECISION:\n- [Create Card] - proceeds with creation\n- [Keep as Note Only] - doesn't create card, marks block\n- [Edit First] - allows editing before creation\n- [Discard] - cancels without saving\n\n5. CARD CREATION:\n- Create card with:\n  - notebookTopicPageId (required)\n  - sourceBlockId (specific block)\n  - tags from topic\n  - FSRS defaults\n- Update NotebookTopicPage.cardIds array\n\nCOMPONENTS:\n\n- src/components/notebook/CardGenerationModal.tsx\n- src/components/notebook/CardWorthinessGate.tsx\n- src/components/notebook/CardSuggestionPreview.tsx\n\nAI FUNCTIONS in electron/ai-service.ts:\n- generateCardFromBlock(blockContent: string, topicContext: string): Promise<CardSuggestion[]>\n- evaluateCardWorthiness(card: CardSuggestion): Promise<WorthinessAssessment>",
        "testStrategy": "Generate card from medical fact block, verify AI suggests appropriate format. Test worthiness gate shows correct assessments. Test 'Keep as Note Only' doesn't create card. Verify all created cards have required provenance fields.",
        "priority": "high",
        "dependencies": [
          "41"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "43",
        "title": "[v2-T5] Card-worthiness gate: rubric (board-relevant? testable? discriminative?), UI",
        "description": "Implement the full card-worthiness evaluation rubric that gates all card creation to minimize low-yield cards.",
        "details": "WORTHINESS RUBRIC:\n\n1. BOARD-RELEVANT:\n- Is this high-yield for Step 2/3 or specialty boards?\n- AI checks against known high-yield topics\n- Weight: High\n\n2. TESTABLE:\n- Does this have a clear, unambiguous answer?\n- Avoid opinion-based or 'it depends' content\n- Weight: High\n\n3. DISCRIMINATIVE:\n- Does this distinguish from similar concepts?\n- Avoid cards that could apply to multiple conditions\n- Weight: Medium\n\n4. MINIMUM INFORMATION:\n- Single concept per card?\n- Not testing multiple facts at once?\n- Weight: High\n\n5. NO PATTERN MATCHING:\n- Answer not giveaway from card format?\n- Could be rephrased and still work?\n- Weight: Medium\n\nSCORING:\n- Each criterion: Pass / Warning / Fail\n- Overall: CREATE / CONSIDER EDITING / NOT RECOMMENDED\n- Show reasoning for each criterion\n\nUI COMPONENT:\n\n```\n┌─ Card Quality Check ─────────────────────────────────┐\n│ \"What is the most specific marker for MI?\"           │\n│ → Troponin I                                         │\n│                                                      │\n│ ✓ Board-relevant (high-yield for Step 2/3)           │\n│ ✓ Testable (clear right answer)                      │\n│ ✓ Discriminative (distinguishes troponin vs CK-MB)   │\n│ ✓ Minimum information (single fact)                  │\n│ ⚠️ Consider: Add 'why' for deeper learning           │\n│                                                      │\n│ Recommendation: ✅ CREATE CARD                       │\n│                                                      │\n│ [Create] [Keep Note] [Edit] [Discard]                │\n└──────────────────────────────────────────────────────┘\n```\n\nAI IMPLEMENTATION:\n- evaluateCardWorthiness(card) returns WorthinessAssessment\n- WorthinessAssessment has criteria[], overallRecommendation, suggestions[]",
        "testStrategy": "Test high-yield card passes all criteria. Test vague card gets warnings. Test multi-fact card fails minimum info. Test pattern-matching card gets warning. Verify recommendations match rubric logic.",
        "priority": "high",
        "dependencies": [
          "42"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "44",
        "title": "[v2-T8] Smart Views: system views (Inbox, Today, Queue, Weak Topics), filter engine",
        "description": "Implement Smart Views system with preset system views and a reusable filter engine for dynamic list filtering.",
        "details": "SYSTEM SMART VIEWS:\n\n1. INBOX (📥):\n- Filter: status='inbox'\n- Badge: count\n- Default sort: newest first\n\n2. TODAY (📅):\n- Filter: due cards (due <= today) + recent captures (createdAt = today)\n- Badge: combined count\n- Sections: 'Due for Review', 'Captured Today'\n\n3. QUEUE (📋):\n- Filter: sourceType='quickcapture' AND status='inbox'\n- Badge: count\n- Quick dumps pending processing\n\n4. NOTEBOOK (📚):\n- Shows all NotebookTopicPages\n- No filter, just different view type\n\n5. TOPICS (🏷️):\n- CanonicalTopic browser\n- Hierarchical if parentTopicId used\n- Show card counts per topic\n\n6. STATS (📊):\n- Dashboard view (deferred content)\n\n7. WEAK TOPICS (⚠️):\n- Topics where cards have low ease (< 2.0 average)\n- Badge: count of weak topics\n- Helps identify knowledge gaps\n\nFILTER ENGINE:\n\n```typescript\ninterface SmartViewFilter {\n  status?: string[];           // ['inbox', 'processed']\n  sourceType?: string[];       // ['qbank', 'article']\n  topicIds?: string[];         // specific topics\n  tags?: string[];             // tag matches\n  hasLowEase?: boolean;        // cards with ease < 2.0\n  isBoardMiss?: boolean;       // flagged board questions\n  dateRange?: { start, end };  // created/modified date\n}\n\nfunction applyFilter(items: any[], filter: SmartViewFilter): any[]\n```\n\nSIDEBAR COMPONENT:\n\n- src/components/layout/SmartViewSidebar.tsx\n- Show all system views with badges\n- Click to switch main content view\n- Highlight active view",
        "testStrategy": "Test Inbox shows only inbox items. Test Today combines due cards and today's captures. Test Weak Topics finds low-ease topics. Test filter engine with complex multi-criteria filters.",
        "priority": "high",
        "dependencies": [
          "35"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "45",
        "title": "[v2-T6] FSRS integration: scheduling fields, review UI, response time tracking",
        "description": "Ensure FSRS integration works with new card provenance model. Track response times for future personalization.",
        "details": "FSRS FIELDS (already in cards table):\n- stability, difficulty, elapsedDays, scheduledDays\n- reps, lapses, state\n- dueDate, lastReview\n\nNEW TRACKING:\n- responseTimeMs in review_logs (already added in Task 1)\n- Track time from card shown to answer revealed\n- Track time from answer revealed to rating\n\nREVIEW UI UPDATES:\n\n1. PROVENANCE DISPLAY:\n- Show source: 'From: [Topic Name]'\n- Click to navigate to NotebookTopicPage\n- Show related cards count\n\n2. ZERO-DECISION FLOW:\n- Show Answer (Space)\n- Continue (Enter) = auto-grade based on response time\n- I Forgot (F) = grade as Again\n- Edit (E) = open card editor\n- Skip (S) = skip without grading\n\n3. LOW-EASE FLAGGING:\n- If card ease < 2.0 after review, flag for attention\n- Show in Weak Topics smart view\n- Suggest 'fix card' flow\n\nRESPONSE TIME LOGIC:\n- Fast correct (< 5s): increase ease slightly\n- Slow correct (> 15s): decrease ease slightly\n- Helps identify cards that need improvement\n\nCOMPONENTS:\n\n- Update src/components/review/ReviewInterface.tsx\n- Add response time tracking\n- Add provenance display\n- Connect to NotebookTopicPage navigation",
        "testStrategy": "Review card, verify response time logged. Test fast answer increases ease. Test slow answer decreases ease. Test low-ease card appears in Weak Topics. Test provenance link navigates correctly.",
        "priority": "high",
        "dependencies": [
          "41",
          "42"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "46",
        "title": "[v2-T6.1] Low-ease detection: flag repeatedly-hard cards, route to fix flow",
        "description": "Detect cards with consistently low ease scores and surface them for improvement through a 'fix card' workflow.",
        "details": "LOW-EASE DETECTION:\n\n1. CRITERIA:\n- Ease factor < 2.0 (default is 2.5)\n- OR lapses > 3 in last 30 days\n- OR response time consistently > 15s\n\n2. FLAGGING:\n- Add lowEaseFlag boolean to cards table (or compute dynamically)\n- Update after each review\n- Clear flag when ease recovers above threshold\n\n3. WEAK TOPICS AGGREGATION:\n- Group flagged cards by topic\n- Count flagged cards per CanonicalTopic\n- Sort topics by flagged card count\n- Show in Weak Topics smart view\n\nFIX CARD FLOW:\n\n1. USER ENTRY:\n- Click 'Fix' on low-ease card during review\n- Or browse Weak Topics and select card\n\n2. FIX OPTIONS:\n- Edit card (simplify, clarify)\n- Split card (if testing multiple concepts)\n- Add context (link to more detailed note)\n- Mark as leech (suspend temporarily)\n- Delete and recreate from Notebook\n\n3. AFTER FIX:\n- Reset FSRS parameters (optional)\n- Card re-enters learning phase\n- Track if fix was successful (ease improvement)\n\nCOMPONENTS:\n\n- src/components/review/FixCardModal.tsx\n- Add 'Fix' button to review UI for low-ease cards\n- Weak Topics view shows fix suggestions",
        "testStrategy": "Create card, fail it 4 times, verify low-ease flag set. Verify card appears in Weak Topics. Test fix flow resets card. Test ease recovery clears flag.",
        "priority": "medium",
        "dependencies": [
          "45"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "47",
        "title": "[v2-T9] UI/UX rules: button hierarchy, destructive confirmations, naming patterns",
        "description": "Establish and enforce consistent UI/UX patterns across all v2 components for button styling, confirmations, and naming.",
        "details": "BUTTON HIERARCHY:\n\n1. PRIMARY (Purple):\n- Main action on any screen\n- Examples: 'Save', 'Create Card', 'Add to Notebook'\n- shadcn: variant='default' with purple theme\n\n2. SECONDARY (Gray):\n- Alternative actions\n- Examples: 'Open', 'Edit', 'Skip'\n- shadcn: variant='outline' or 'secondary'\n\n3. DESTRUCTIVE (Icon + Confirm):\n- Delete operations\n- Icon-only button (🗑️) with hover tooltip\n- Click shows confirmation dialog\n- Never inline delete without confirm\n\n4. GHOST (Text only):\n- Subtle actions\n- Examples: 'Cancel', 'Back'\n- shadcn: variant='ghost'\n\nCONFIRMATION PATTERNS:\n\n- Delete SourceItem: 'Delete this source? Content will be lost.'\n- Delete Card: 'Delete this card? Review history will be lost.'\n- Delete Topic: 'Delete topic and X cards? This cannot be undone.'\n- Batch delete: 'Delete X items? This cannot be undone.'\n\nNAMING CONVENTIONS:\n\n- Use 'Knowledge Bank' not 'Sources' or 'Inbox'\n- Use 'Notebook' not 'Notes' or 'Pages'\n- Use 'Topic' not 'Subject' or 'Category'\n- Use 'Block' not 'Excerpt' or 'Snippet'\n- Use 'Generate Card' not 'Create Card' (implies AI assistance)\n\nKEYBOARD SHORTCUTS:\n\n- Ctrl+K: Command palette\n- Ctrl+Shift+S: Quick Dump\n- Ctrl+1-6: Navigate Smart Views\n- Space: Show answer / Continue\n- Enter: Confirm action\n- Escape: Cancel / Close modal\n- F: Mark as forgotten (review)\n- E: Edit current item\n- Delete/Backspace: Delete (with confirm)\n\nIMPLEMENTATION:\n\n- Create src/components/ui/button-variants.tsx with themed variants\n- Create src/components/ui/ConfirmDialog.tsx reusable component\n- Document patterns in src/styles/design-system.md",
        "testStrategy": "Audit all buttons across app for consistency. Test all delete actions show confirmation. Test keyboard shortcuts work globally. Verify naming consistency in UI.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "48",
        "title": "Research AI context relevance optimization (lost in the middle problem)",
        "description": "Studies show models perform better with 10 highly-relevant files than 50 loosely-related ones. Plan how to optimize file/context selection when querying AI: relevance scoring, chunking strategies, embeddings-based retrieval, quality over quantity.",
        "priority": "low",
        "status": "deferred",
        "tags": [],
        "dependencies": [],
        "subtasks": []
      },
      {
        "id": "49",
        "title": "Personalized response time baselines for FSRS auto-grading",
        "description": "Replace fixed time thresholds (<5s=Easy, 5-15s=Good, etc.) with personalized baselines. Grade relative to user's own response patterns rather than absolute time. Research shows response latency correlates with retrieval strength (Bjork 1994, PMC4480221).",
        "details": "EVIDENCE BASE:\n- Response latency is a valid proxy for retrieval difficulty (PMC4480221)\n- FSRS already uses response time in its 21-parameter personalization model\n- Items strengthen across retrieval attempts even when accuracy unchanged - latency reveals this\n\nIMPLEMENTATION:\n1. Track per-card response time history in review_logs (already storing responseTimeMs from Task 5)\n2. Calculate user-level baseline: median response time across last 100 reviews\n3. Calculate card-level baseline: median response time for this specific card\n4. Grade relative to baseline:\n   - <50% of baseline = Easy (fast for you)\n   - 50-100% of baseline = Good (normal for you)\n   - 100-200% of baseline = Hard (slow for you)\n   - >200% of baseline = Again (struggled)\n5. Cold start: use fixed thresholds until 20+ reviews exist\n6. Weighted recency: recent reviews weighted higher than old ones\n7. Add analytics dashboard showing personal response time trends\n\nFILES TO MODIFY:\n- electron/fsrs-service.ts: Add getPersonalizedBaseline(), calculateRelativeGrade()\n- electron/database.ts: Add reviewLogQueries.getRecentForUser(), getRecentForCard()\n- src/components/review/ReviewInterface.tsx: Use personalized grading when data available",
        "testStrategy": "Create mock review history with varying response times. Verify baseline calculation accuracy. Test cold start fallback to fixed thresholds. Test grade determination relative to baseline. Verify weighted recency favors recent data. Compare auto-grades between fixed and personalized modes.",
        "priority": "medium",
        "status": "pending",
        "tags": [],
        "dependencies": [
          "5"
        ],
        "subtasks": []
      },
      {
        "id": "50",
        "title": "Voice-based answer capture with AI grading (Exploratory)",
        "description": "Optional feature: user speaks answer aloud, speech-to-text converts to text, AI compares against expected answer. Research shows AI grading achieves QWK>0.93 for objective content but 70% disagreement on nuanced responses. Best for simple fact recall, not complex medical reasoning.",
        "details": "EVIDENCE BASE (from research):\n- AI grading accuracy varies by content type (MIT Sloan 2024, Flodén 2025)\n- Objective STEM content: QWK > 0.93 (highly accurate)\n- Nuanced/subjective content: 70.5% score differences vs human graders\n- Recommendation: 'AI best for formative feedback, not standalone grading'\n\nIMPLEMENTATION APPROACH:\n1. Use Web Speech API (browser-native, no external service) for speech-to-text\n2. AI comparison via existing ai-service.ts infrastructure\n3. Grading prompt: 'Compare user answer to expected answer. Rate semantic similarity 0-100. Flag if user missed key concepts.'\n4. Threshold: >80% similarity = recall successful, <50% = Again\n5. Show AI reasoning to user for transparency\n\nSCOPE LIMITATIONS:\n- ONLY enable for cards tagged 'objective-fact' or similar\n- Disable for cards with complex/nuanced answers\n- User can toggle feature on/off in settings\n- Store voice transcription for user review (privacy: local only)\n\nUSER FLOW:\n1. Card shown → User taps microphone icon (or hotkey)\n2. User speaks answer → STT converts to text\n3. Text shown to user for verification → User confirms or edits\n4. AI grades → Shows result with reasoning\n5. User can override if AI was wrong\n\nFILES TO CREATE/MODIFY:\n- src/components/review/VoiceCapture.tsx: New component for voice recording\n- electron/ai-service.ts: Add gradeVoiceAnswer() function\n- src/components/review/ReviewInterface.tsx: Integrate VoiceCapture\n- src/stores/useAppStore.ts: Add voiceModeEnabled setting",
        "testStrategy": "Test Web Speech API availability detection. Test transcription accuracy with sample medical terms. Test AI grading consistency across multiple attempts. Test user override flow. Verify privacy (no external transmission of voice). Test toggle on/off. Compare AI grades to human grades on test set.",
        "priority": "low",
        "status": "pending",
        "tags": [
          "exploratory"
        ],
        "dependencies": [
          "49"
        ],
        "subtasks": []
      },
      {
        "id": "51",
        "title": "AI Agent / Jarvis Mode for natural language card retrieval",
        "description": "Voice/text command interface to pull relevant flashcards using natural language. Example: 'Let's review ACLS protocol' → AI finds and queues all ACLS-related cards. Think Jarvis-style assistant for post-shift exhausted brain.",
        "details": "VISION:\nUser says or types natural language command, AI interprets intent and executes card operations without manual searching/filtering.\n\nEXAMPLE COMMANDS:\n- 'Let's review ACLS protocol' → Queue all cards tagged/related to ACLS\n- 'Show me my weak cards on intubation' → Filter low-ease cards + topic\n- 'What do I know about vasopressors?' → Surface all vasopressor cards for quick browse\n- 'Quiz me on yesterday's shift topics' → Pull cards from recently-added sources\n- 'Focus on boards content' → Filter to high-yield tagged cards\n\nTECHNICAL APPROACH:\n1. Command input: Text field or voice (Web Speech API)\n2. Intent parsing: AI extracts action + filters (topic, difficulty, recency, tags)\n3. Query generation: Convert intent to database query\n4. Execution: Populate review queue or browse view with results\n5. Confirmation: 'Found 23 cards about ACLS. Ready to review?'\n\nINTEGRATION POINTS:\n- CommandPalette (Ctrl+K): Add 'Ask Jarvis' option\n- Floating input: Always-available command bar\n- Voice activation: Optional hotkey for voice mode\n- Smart Views: Agent can create temporary filtered views\n\nDEPENDENCIES:\n- Robust tagging/topic system (CanonicalTopic)\n- Card search infrastructure\n- AI service integration\n\nFILES TO CREATE:\n- src/components/agent/JarvisInput.tsx\n- src/components/agent/JarvisResponse.tsx\n- electron/agent-service.ts: Intent parsing, query generation\n- src/stores/useAppStore.ts: agentMode state",
        "testStrategy": "Test 10 natural language commands, verify correct card retrieval. Test edge cases: no matches found, ambiguous commands. Test voice input transcription. Verify AI reasoning shown to user. Test command history/recall.",
        "priority": "low",
        "status": "deferred",
        "tags": [
          "future",
          "ai-agent"
        ],
        "dependencies": [],
        "subtasks": []
      },
      {
        "id": "52",
        "title": "Procedural Simulation Mode with equipment recall and sequential cloze",
        "description": "Interactive procedure review: User voices equipment needed (items appear in left panel as named), then walks through sequential cloze steps (right panel). Builds both equipment recall AND procedural memory in realistic simulation format.",
        "details": "VISION:\nSimulate procedure preparation and execution. Two-phase review:\n1. Equipment recall: Voice what you need, items appear visually\n2. Procedure steps: Sequential cloze walkthrough with equipment reference\n\nUI LAYOUT:\n┌─────────────────┬─────────────────┐\n│  EQUIPMENT      │  PROCEDURE      │\n│  (Left Panel)   │  (Right Panel)  │\n│                 │                 │\n│  [Laryngoscope] │  1. Position... │\n│  [ETT 7.5]      │  2. {{c1::___}} │\n│  [Stylet]       │  3. Advance...  │\n│  [BVM]          │  4. {{c2::___}} │\n│                 │                 │\n│  (items appear  │  (sequential    │\n│   as named)     │   cloze reveal) │\n└─────────────────┴─────────────────┘\n\nPHASE 1 - EQUIPMENT RECALL:\n- Procedure card shows: 'What equipment do you need for [Intubation]?'\n- User speaks items aloud (Web Speech API transcription)\n- As each item recognized, cutout PNG appears in left panel\n- Visual feedback: green checkmark for correct, amber for partial match\n- AI matches spoken terms to equipment database (fuzzy matching)\n- 'ETT' matches 'Endotracheal Tube', 'blade' matches 'Laryngoscope blade'\n- After equipment phase, proceed to steps\n\nPHASE 2 - SEQUENTIAL STEPS:\n- Right panel shows procedure steps as overlapping cloze\n- User speaks/reveals each step sequentially\n- Can reference equipment panel: click item to 'use' it\n- Step reveal: Space or voice continues to next cloze\n- Feedback: Each step graded, aggregate score at end\n\nCONTENT STRUCTURE:\n- New card type: 'procedure'\n- Fields: title, equipmentList[], steps[] (each step is cloze text)\n- Equipment items: name, imageUrl (cutout PNG), aliases[]\n- Source: Created from Notebook blocks tagged 'procedure'\n\nEQUIPMENT IMAGE LIBRARY:\n- Store cutout PNGs locally or reference CDN\n- Common medical equipment: airways, lines, monitors, meds\n- User can add custom equipment images\n\nFILES TO CREATE:\n- src/components/review/ProceduralSimulation.tsx\n- src/components/review/EquipmentPanel.tsx\n- src/components/review/SequentialClozePanel.tsx\n- src/types/procedure.ts: ProcedureCard, Equipment interfaces\n- electron/database.ts: procedure_cards table, equipment table\n- assets/equipment/: PNG library",
        "testStrategy": "Create intubation procedure card with 5 equipment items and 8 steps. Test equipment recognition via voice. Test visual feedback on correct/incorrect items. Test sequential cloze progression. Test equipment 'use' interaction. Verify aggregate scoring. Test with multiple procedures.",
        "priority": "low",
        "status": "deferred",
        "tags": [
          "future",
          "procedures",
          "simulation"
        ],
        "dependencies": [],
        "subtasks": []
      },
      {
        "id": "53",
        "title": "Review UI: Refine manual grade button color contrast",
        "description": "Adjust manual grade button colors for better accessibility and visual clarity. Current dark colors have low contrast.",
        "priority": "low",
        "status": "deferred",
        "tags": [
          "ux",
          "polish"
        ],
        "dependencies": [
          "5"
        ],
        "subtasks": []
      },
      {
        "id": "54",
        "title": "Review UI: Show response time and auto-grade reasoning",
        "description": "Add visual indicator showing how auto-grade was determined (e.g., 'Recalled (12.3s)'). User can't tell why card was graded certain way.",
        "priority": "medium",
        "status": "deferred",
        "tags": [
          "ux",
          "transparency"
        ],
        "dependencies": [
          "5"
        ],
        "subtasks": []
      },
      {
        "id": "55",
        "title": "Review UI: Add first-time interactive tutorial",
        "description": "New users confused about Continue vs. manual buttons. Add guided tutorial explaining zero-decision flow and when to override.",
        "priority": "high",
        "status": "deferred",
        "tags": [
          "ux",
          "onboarding"
        ],
        "dependencies": [
          "5",
          "54"
        ],
        "subtasks": []
      },
      {
        "id": "56",
        "title": "Review UI: Hide manual grade buttons until answer shown",
        "description": "Manual grade buttons appear before answer revealed, which is confusing. Should only show after Space pressed.",
        "priority": "high",
        "status": "deferred",
        "tags": [
          "ux",
          "bug-fix"
        ],
        "dependencies": [
          "5"
        ],
        "subtasks": []
      },
      {
        "id": "57",
        "title": "Review UI: Add persistent back button in header",
        "description": "Add global '← Capture' button in header so user can exit review without scrolling to bottom.",
        "priority": "medium",
        "status": "deferred",
        "tags": [
          "ux",
          "navigation"
        ],
        "dependencies": [
          "5"
        ],
        "subtasks": []
      },
      {
        "id": "58",
        "title": "Main screen (Capture): Improve onboarding and discoverability",
        "description": "Capture screen feels empty. Add hero section, multi-input tabs, examples, recent activity. Make it feel like a proper home screen.",
        "priority": "high",
        "status": "deferred",
        "tags": [
          "ux",
          "onboarding"
        ],
        "dependencies": [],
        "subtasks": []
      },
      {
        "id": "59",
        "title": "Capture UI: Auto-resize paste box based on content type",
        "description": "Paste box should adapt: single-line for quick text, multi-line for paragraphs, drop zone for PDFs/images.",
        "priority": "medium",
        "status": "deferred",
        "tags": [
          "ux",
          "smart-ui"
        ],
        "dependencies": [
          "58"
        ],
        "subtasks": []
      },
      {
        "id": "60",
        "title": "Quick Dump UI: Add explanation and visual distinction",
        "description": "Quick Dump is unclear. Add tooltip, visual design (lightning icon), and explanation of 'emergency capture' use case.",
        "priority": "medium",
        "status": "deferred",
        "tags": [
          "ux",
          "quick-dump"
        ],
        "dependencies": [
          "58"
        ],
        "subtasks": []
      },
      {
        "id": "61",
        "title": "Review UI: Add response time indicator in header",
        "description": "Show subtle timer in top-right (0-5s green, 5-15s blue, etc.) and/or expandable session stats panel.",
        "priority": "low",
        "status": "deferred",
        "tags": [
          "ux",
          "stats"
        ],
        "dependencies": [
          "5",
          "54",
          "57"
        ],
        "subtasks": []
      },
      {
        "id": "62",
        "title": "Capture UI: Clarify keyboard shortcuts and boost AI confidence",
        "description": "Remove confusing 'Tab to navigate, Space to toggle' hint. Replace with accurate shortcuts and better AI accuracy messaging.",
        "priority": "medium",
        "status": "deferred",
        "tags": [
          "ux",
          "onboarding"
        ],
        "dependencies": [
          "58"
        ],
        "subtasks": []
      },
      {
        "id": "71",
        "title": "Queue onboarding and guidance: explain what Queue is and how to access reviews",
        "description": "Users don't understand what the Queue view is or how it relates to reviews. Add onboarding explanation and UI guidance.",
        "status": "deferred",
        "priority": "medium",
        "dependencies": [],
        "details": "PROBLEM:\n- No clear explanation of Queue vs Review\n- Users don't know where to find due cards\n- Terminology confusion: 'Queue' is vague\n- No guidance on what to do when queue is empty\n\nSOLUTION:\n1. First-time Queue visit: Show tutorial modal explaining:\n   - Queue shows cards due for review today\n   - Auto-populated by FSRS scheduler\n   - Click 'Start Review' to begin session\n   - Visual diagram: Capture → Cards → Queue → Review\n\n2. Empty state guidance:\n   - When queue empty: 'No cards due. Next review: [timestamp]'\n   - Suggest: 'Create more cards' or 'Adjust daily limit in Settings'\n   - Show upcoming review schedule (next 7 days)\n   - Link to Capture view to create more cards\n\n3. Header/navigation clarity:\n   - Consider renaming 'Queue' to 'Due Cards' or 'Review Queue'\n   - Add badge with due count in sidebar/header\n   - Tooltip on hover: 'Cards scheduled for review today'\n\n4. Contextual help:\n   - Small '?' icon in Queue header\n   - Clicking shows: queue logic, scheduling explanation\n   - Link to Review to start session\n\n5. Visual improvements:\n   - Show preview of next 3-5 cards in queue\n   - Display estimated time to complete queue\n   - Show progress: \"5 of 23 cards reviewed today\"\n\nFILES TO CREATE/MODIFY:\n- src/components/review/QueueView.tsx (create or update)\n- src/components/modals/QueueTutorialModal.tsx\n- src/components/layout/Navigation.tsx (badge + tooltip)\n- src/stores/useAppStore.ts (track tutorial completion)",
        "testStrategy": "Test first-time Queue visit shows tutorial. Test empty state messaging. Verify badge shows correct due count. Test tooltip hover. Verify navigation works correctly.",
        "subtasks": [],
        "tags": [
          "onboarding",
          "ux",
          "post-mvp",
          "queue"
        ]
      },
      {
        "id": "72",
        "title": "Undo functionality (Ctrl+Z) for edits and review grades",
        "description": "Implement global undo system allowing users to reverse recent actions, especially card edits and review grades.",
        "status": "deferred",
        "priority": "high",
        "dependencies": [],
        "details": "PROBLEM:\n- Users accidentally grade cards incorrectly (fat-finger mistakes)\n- No way to undo card edits after saving\n- Medical content mistakes can't be quickly reversed\n- Causes anxiety about making mistakes during tired post-shift reviews\n\nKEY USE CASES:\n1. Review grade undo: User accidentally hits wrong grade button, Ctrl+Z immediately reverts\n2. Card edit undo: User edits card content, saves changes but realizes mistake, Ctrl+Z reverts\n3. Batch operation undo: User deletes multiple cards accidentally, Ctrl+Z restores\n\nIMPLEMENTATION:\n1. Action history stack: Store last 10-20 actions in memory\n   - Each action: { type, timestamp, before, after, revertFn }\n   - Types: 'review_grade', 'card_edit', 'card_delete', 'metadata_change'\n\n2. Database integration:\n   - For review grades: Update review_logs with new grade, restore FSRS state\n   - For edits: Keep edit_history table with timestamps, revert to previous version\n   - For deletes: Soft delete pattern (deleted_at column), restore on undo\n\n3. UI feedback:\n   - Toast notification: \"Undone: [action description]\"\n   - Keyboard shortcut: Ctrl+Z (Cmd+Z on Mac)\n   - Optional: Show \"Undo\" button in corner for 5 seconds after action\n   - Redo support: Ctrl+Shift+Z\n\nFILES TO CREATE/MODIFY:\n- src/stores/useAppStore.ts: Add undoStack, redoStack, undo(), redo() actions\n- src/hooks/useUndo.ts: Hook for keyboard shortcut and undo logic\n- electron/database.ts: Add edit_history table, soft delete support\n- src/components/layout/UndoToast.tsx: Undo notification component\n- src/components/review/ReviewInterface.tsx: Integrate undo for grades",
        "testStrategy": "Test undo after review grade. Test undo after card edit. Test undo after delete. Test redo functionality. Test undo stack limits. Test session persistence.",
        "subtasks": [],
        "tags": [
          "ux",
          "post-mvp",
          "undo",
          "quality-of-life"
        ]
      },
      {
        "id": "73",
        "title": "Anki import functionality (.apkg file support)",
        "description": "Allow users to import existing Anki decks (.apkg files) to migrate their medical flashcard collections into DougHub.",
        "status": "deferred",
        "priority": "critical",
        "dependencies": [],
        "details": "CRITICAL IMPORTANCE:\n- Medical students/residents have YEARS of Anki cards\n- Migration barrier is #1 blocker for adoption\n- Must preserve scheduling data (FSRS state, due dates)\n- Common use case: User has 5,000+ Step 1/2 cards in Anki\n\nANKI .APKG FORMAT:\n- SQLite database in ZIP archive\n- Tables: cards, notes, revlog (review history), decks\n- Anki uses SM-2 algorithm (older) or FSRS-4.5 (newer)\n- Media files (images) stored in separate folder in archive\n\nIMPORT REQUIREMENTS:\n1. File handling: Accept .apkg file via drag-drop or file picker, extract ZIP, read SQLite DB\n2. Data mapping: Anki notes → DougHub cards, preserve tags, convert decks to topics\n3. FSRS migration: Preserve or convert scheduling state, maintain ease/interval/due dates\n4. Media handling: Extract images, copy to DougHub storage, update paths\n5. UI workflow: Import modal with preview, options, progress bar, summary\n6. Conflict handling: Duplicate detection, merge options\n\nLIBRARIES TO USE:\n- adm-zip or yauzl: ZIP extraction\n- better-sqlite3: Read Anki SQLite database (already in project)\n- FSRS-rs or ts-fsrs: FSRS state conversion\n\nFILES TO CREATE:\n- electron/anki-importer.ts: Core import logic\n- electron/anki-parser.ts: Parse .apkg format\n- electron/media-migrator.ts: Handle image/media files\n- src/components/settings/AnkiImportModal.tsx: Import UI\n- src/components/settings/ImportPreview.tsx: Preview component\n- electron/ipc-handlers.ts: Add importAnkiDeck() handler",
        "testStrategy": "Test with real Anki decks from AnkiWeb (Zanki, AnKing). Verify FSRS state preservation. Check image rendering. Confirm tag import. Test with 10/100/1000/10000 card decks.",
        "subtasks": [],
        "tags": [
          "import",
          "anki",
          "migration",
          "critical",
          "post-mvp"
        ]
      },
      {
        "id": "74",
        "title": "Keyboard shortcut reference overlay (Ctrl+? or F1)",
        "description": "Global keyboard shortcut help overlay showing all available shortcuts, context-sensitive to current view.",
        "status": "deferred",
        "priority": "medium",
        "dependencies": [],
        "details": "PROBLEM:\n- Users don't know available keyboard shortcuts\n- Docs mention Ctrl+K, Space, Escape but no in-app reference\n- Power users want to learn shortcuts to speed up workflow\n\nSOLUTION:\n1. Shortcut overlay: Triggered by Ctrl+? or F1, modal with semi-transparent background\n2. Context-aware display: Show all shortcuts, highlight relevant ones for current view\n3. Shortcut categories: Global, Capture, Review, Navigation\n4. Interactive features: Search within shortcuts, click to execute, print button\n5. Visual design: <kbd> styled tags, monospace font, icons, clean layout\n\nSHORTCUT CATEGORIES:\nGLOBAL: Ctrl+K (command palette), Ctrl+F (search), Ctrl+Enter (floating capture), Ctrl+Z (undo)\nCAPTURE: Ctrl+S (save), Tab (next field), Ctrl+Shift+C (make cloze)\nREVIEW: Space (show answer/continue), 1-4 (manual grade), Escape (exit)\nNAVIGATION: Alt+1-4 (view switching)\n\nFILES TO CREATE:\n- src/components/modals/KeyboardShortcutsModal.tsx\n- src/constants/keyboard-shortcuts.ts: Centralized shortcut definitions\n- src/hooks/useKeyboardShortcut.ts: Reusable hook for registering shortcuts\n- src/components/ui/KeyboardKey.tsx: Styled <kbd> component",
        "testStrategy": "Test overlay triggers via Ctrl+? and F1. Test context highlighting. Test search functionality. Verify all shortcuts listed. Test keyboard navigation within modal.",
        "subtasks": [],
        "tags": [
          "ux",
          "keyboard",
          "shortcuts",
          "post-mvp",
          "onboarding"
        ]
      },
      {
        "id": "75",
        "title": "Statistics and analytics dashboard (FSRS metrics, retention, weak topics)",
        "description": "Comprehensive analytics view showing learning progress, FSRS metrics, retention curves, and performance insights.",
        "status": "deferred",
        "priority": "high",
        "dependencies": [],
        "details": "MEDICAL RESIDENT NEEDS:\n- \"Am I actually retaining this?\" - Confidence in learning\n- \"What are my weak areas?\" - Targeted review\n- \"How much do I know about X?\" - Topic mastery\n- \"Am I ready for boards?\" - Exam readiness metrics\n\nDASHBOARD SECTIONS:\n1. OVERVIEW STATS: Total cards, reviews today/week, retention rate, study streak\n2. RETENTION CURVE: Line graph showing retention % over time, FSRS accuracy\n3. TOPIC BREAKDOWN: Bar chart of cards per topic, color-coded by mastery level\n4. REVIEW HEATMAP: GitHub-style calendar showing review activity\n5. FSRS INSIGHTS: Average ease by topic, hardest cards, cards at risk\n6. PERFORMANCE TRENDS: Response time trends, auto-grade distribution, learning velocity\n7. STUDY GOALS: Achievements, milestones, progress toward goals\n\nUI/UX:\n- Accessible via sidebar: \"Stats\" icon\n- Responsive grid layout\n- Export data: CSV download\n- Date range filter: Last 7/30/90/365 days or All Time\n- Print-friendly view\n\nCHARTS LIBRARY: Recharts or Chart.js for React\n\nFILES TO CREATE:\n- src/components/stats/StatsDashboard.tsx\n- src/components/stats/RetentionCurve.tsx\n- src/components/stats/TopicBreakdown.tsx\n- src/components/stats/ReviewHeatmap.tsx\n- src/components/stats/PerformanceTrends.tsx\n- electron/stats-queries.ts: Aggregate queries for dashboard\n- src/hooks/useStats.ts: Data fetching hook",
        "testStrategy": "Test all dashboard sections render. Test date range filtering. Test export functionality. Verify calculations accurate. Test with empty data (new user). Test with large datasets (100k+ reviews).",
        "subtasks": [],
        "tags": [
          "analytics",
          "stats",
          "fsrs",
          "post-mvp",
          "retention"
        ]
      },
      {
        "id": "76",
        "title": "Tag management UI (create, rename, merge, delete tags manually)",
        "description": "Interface for manual tag management, separate from AI auto-suggestions. Allow users to organize, clean up, and maintain their tag taxonomy.",
        "status": "deferred",
        "priority": "medium",
        "dependencies": [],
        "details": "DISTINCTION FROM EXISTING:\n- Task #12 covers AI tag SUGGESTIONS during capture\n- THIS task covers manual tag MANAGEMENT post-creation\n- Think: Gmail labels management vs auto-labeling\n\nUSE CASES:\n1. Tag cleanup: Merge \"cardiology\", \"cards\", \"cardio\" into one\n2. Tag organization: Create parent-child relationships (optional hierarchy)\n3. Tag discovery: View all tags with card counts, sort, search\n\nFEATURES:\n1. TAG BROWSER: List view with card counts, search, sort options, color coding\n2. TAG EDITOR: Inline edit, validation, aliases, descriptions\n3. MERGE TAGS: Select multiple → merge, choose primary, update cards atomically\n4. DELETE TAGS: Delete unused tags, confirm for used tags, soft delete\n5. BULK OPERATIONS: Select cards → apply/remove tags\n6. TAG ANALYTICS: Most/least used, retention rates, orphaned tags\n\nUI LOCATION:\n- Settings → Tags\n- Or dedicated \"Tags\" view in sidebar (after MVP)\n- Quick access: Right-click tag anywhere → \"Manage tag\"\n\nDATABASE:\n- canonical_topics table (already exists in v2 schema)\n- Add: color, description, parent_id (for hierarchy)\n- topic_aliases table for merge functionality\n- deleted_tags table for soft delete\n\nFILES TO CREATE:\n- src/components/settings/TagManagement.tsx\n- src/components/settings/TagBrowser.tsx\n- src/components/settings/TagEditor.tsx\n- src/components/modals/MergeTagsModal.tsx\n- electron/tag-manager.ts: Tag CRUD operations\n- electron/ipc-handlers.ts: Add tag management handlers",
        "testStrategy": "Test tag creation, rename, delete. Test merge functionality updates all cards. Test bulk operations. Test tag analytics calculations. Test hierarchy (if implemented). Test soft delete recovery.",
        "subtasks": [],
        "tags": [
          "tags",
          "management",
          "ux",
          "post-mvp",
          "organization"
        ]
      },
      {
        "id": "77",
        "title": "Capture textarea auto-resize based on content length",
        "description": "Make the paste/input textarea dynamically resize based on content, eliminating need to scroll within small box.",
        "status": "deferred",
        "priority": "medium",
        "dependencies": [],
        "details": "CURRENT PROBLEM:\n- User pastes long clinical vignette or article\n- Textarea stays small fixed height\n- Must scroll within textarea to see content\n- Feels cramped, hard to review pasted content\n- UX doesn't convey \"dump everything here\"\n\nDESIRED BEHAVIOR:\n- Textarea starts at reasonable default (4-6 lines)\n- As user types/pastes, textarea grows vertically\n- Max height: ~70% of viewport (then scroll whole page, not textarea)\n- Smooth transition, no jarring jumps\n- Shrinks back down if user deletes content\n\nIMPLEMENTATION:\n1. Auto-resize logic: Use `react-textarea-autosize` package OR custom hook\n   - Listen to input events, measure scrollHeight\n   - Set height = max(minHeight, min(scrollHeight, maxHeight))\n   - Debounce resize during rapid typing\n\n2. Visual design:\n   - Min height: 120px (~4 lines)\n   - Max height: calc(100vh - 300px)\n   - Vertical resize handle (optional): User can manually adjust\n   - Smooth CSS transition: `transition: height 0.1s ease`\n\n3. Content type adaptation (BONUS):\n   - Single line paste (<80 chars): Keep small\n   - Paragraph paste (80-500 chars): Grow to ~8 lines\n   - Long article (500+ chars): Grow to max height\n   - Image/PDF drop: Collapse textarea, show preview\n\nLIBRARIES: react-textarea-autosize (lightweight, 2KB)\n\nFILES TO MODIFY:\n- src/components/capture/CaptureInterface.tsx\n- src/components/capture/AutoResizeTextarea.tsx (new component)\n- src/index.css: Add smooth transition styles",
        "testStrategy": "Test paste of varying lengths (short, medium, long). Test typing and deleting. Test smooth transitions. Test max height cap. Test mobile responsiveness. Test keyboard navigation preserved.",
        "subtasks": [],
        "tags": [
          "ux",
          "capture",
          "textarea",
          "post-mvp",
          "polish"
        ]
      },
      {
        "id": "78",
        "title": "Capture page UX redesign: better 'dump it here, I'll organize' messaging",
        "description": "Redesign Capture page to clearly convey 'paste anything, AI will organize it' workflow. Currently feels empty and unclear.",
        "status": "deferred",
        "priority": "high",
        "dependencies": [],
        "details": "CURRENT ISSUES (from user feedback):\n- Capture screen feels empty and uninviting\n- Not clear what to do: \"Just paste... anything?\"\n- No examples or guidance for first-time users\n- Doesn't convey AI-powered workflow\n- Feels like generic form, not intelligent assistant\n\nDESIRED FEELING:\n- \"This is a smart assistant that handles the hard parts\"\n- \"I can dump messy notes and it'll make sense of them\"\n- \"Low friction, zero decision paralysis\"\n- \"I trust this to extract what matters\"\n\nREDESIGN COMPONENTS:\n1. HERO SECTION: \"Drop your notes. We'll handle the rest.\" with animation\n2. MULTI-INPUT TABS: Text | Image | PDF | Quick Dump\n3. EXAMPLES SECTION: Clickable snippets to populate textarea\n4. RECENT ACTIVITY: Shows last 3-5 cards created\n5. SMART PLACEHOLDERS: Context-aware based on time/usage\n6. VISUAL HIERARCHY: Larger textarea, de-emphasize Quick Dump, emphasize Extract Cards\n7. ONBOARDING OVERLAY: Dismissible tour for first-time users\n8. CONFIDENCE INDICATORS: \"AI analyzing... found 5 key concepts\"\n9. STATS/MOTIVATION: \"You've created 247 cards this month\"\n10. LAYOUT OPTIONS: Test centered vs two-column vs full-width\n\nFILES TO CREATE/MODIFY:\n- src/components/capture/CaptureInterface.tsx (major refactor)\n- src/components/capture/CaptureHero.tsx (new)\n- src/components/capture/ExamplesPanel.tsx (new)\n- src/components/capture/RecentActivity.tsx (new)\n- src/components/modals/CaptureOnboardingModal.tsx (new)\n- src/index.css: Updated capture page styles\n\nINSPIRATION:\n- Notion's clean, inviting empty state\n- Readwise's \"highlight → card\" clear value prop\n- RemNote's \"drop anything\" file import\n- Linear's command-driven, frictionless input",
        "testStrategy": "User testing with medical residents. A/B test current vs redesigned. Measure: Time to first card creation, perceived clarity, user confidence ratings.",
        "subtasks": [],
        "tags": [
          "ux",
          "capture",
          "onboarding",
          "post-mvp",
          "redesign"
        ]
      },
      {
        "id": "79",
        "title": "Implement Superhuman-style split view for Notebook",
        "description": "Create a master-detail layout for the Notebook view with a narrow topic list (~280px) on the left and content display on the right, including resizable divider, search/filter, smooth transitions, and keyboard navigation",
        "details": "1. Create NotebookView component in src/components/notebook/NotebookView.tsx\n2. Use react-resizable-panels (already installed) for split layout:\n   - Import { ResizablePanelGroup, ResizablePanel, ResizableHandle } from '@/components/ui/resizable'\n   - Left panel: defaultSize={20} minSize={15} maxSize={35} for topic list\n   - Right panel: defaultSize={80} for content area\n3. Implement TopicList component:\n   - Vertical list of topics with search input at top\n   - Use ScrollArea from shadcn/ui for scrollable list\n   - Active topic highlighted with border-l-2 border-primary pattern (similar to Sidebar.tsx:98)\n   - Each item shows topic name, card count badge\n4. Implement TopicDetail component:\n   - Display selected topic content\n   - Use Card components for content blocks\n   - Show empty state when no topic selected\n5. Add keyboard navigation:\n   - Arrow up/down to navigate topic list\n   - Enter to focus detail pane\n   - Escape to return to topic list\n   - Use useEffect with addEventListener for keydown events\n6. State management:\n   - Add 'notebook' to AppView type in useAppStore.ts\n   - Add selectedTopicId state\n   - Add topics array to store (fetch via window.api.topics.getAll pattern)\n7. Smooth transitions:\n   - Use Tailwind transition-all duration-150 classes\n   - Framer Motion not needed (keep it simple)\n8. Update Sidebar.tsx line 73: set implemented: true for notebook item\n9. Update AppLayout.tsx to render NotebookView when currentView === 'notebook'",
        "testStrategy": "Manual testing: (1) Verify split view renders with ~280px left panel by default, (2) Test resizing divider and confirm min/max constraints, (3) Click topics and verify content updates smoothly, (4) Test keyboard navigation (arrows, enter, escape), (5) Search/filter topics and verify list updates, (6) Test with 0 topics (empty state), 1 topic, and 50+ topics (scrolling), (7) Verify prefers-reduced-motion respected for transitions, (8) Check accessibility with screen reader for ARIA labels on panels",
        "priority": "high",
        "dependencies": [],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T02:02:11.895Z"
      },
      {
        "id": "80",
        "title": "Add Notion-style breadcrumb navigation",
        "description": "Implement dynamic breadcrumb trail in content area header showing current location (e.g., 'Capture > New Note', 'Review > Cardiology') with clickable segments, truncation for long paths, and subtle styling",
        "details": "1. Create BreadcrumbNav component in src/components/layout/BreadcrumbNav.tsx\n2. Use existing shadcn/ui Breadcrumb components from '@/components/ui/breadcrumb':\n   - Import { Breadcrumb, BreadcrumbList, BreadcrumbItem, BreadcrumbLink, BreadcrumbPage, BreadcrumbSeparator, BreadcrumbEllipsis }\n3. Implement breadcrumb logic:\n   - Map currentView to base segment: 'capture' → 'Capture', 'review' → 'Review', etc.\n   - Add context-based second segment:\n     * Review view: show current topic name if available (from store)\n     * Notebook view: show selected topic name\n     * Capture view: show 'New Note' or note title if editing\n   - Maximum 3 segments, use BreadcrumbEllipsis for truncation\n4. Make segments clickable:\n   - Use BreadcrumbLink with onClick handlers\n   - First segment: navigate to main view (setCurrentView)\n   - Middle segments: navigate to parent context (e.g., clear topic selection)\n   - Last segment: use BreadcrumbPage (not clickable)\n5. Truncation with hover expansion:\n   - For segments > 25 chars, truncate to 22 chars + '...'\n   - Use Tooltip from '@/components/ui/tooltip' to show full text on hover\n   - Set delayDuration={300} for tooltip\n6. Styling:\n   - Use text-xs or text-sm for breadcrumb text\n   - Apply text-muted-foreground for non-current segments\n   - Use opacity-70 for separators\n   - Ensure doesn't compete with main content (subtle background if needed)\n7. Integration:\n   - Add to Header.tsx or create dedicated breadcrumb section in AppLayout\n   - Position below main header, above content area\n   - Use flex items-center gap-2 for layout\n8. Zustand store updates:\n   - Add getCurrentBreadcrumb() helper that returns { segments: Array<{ label: string, onClick?: () => void }> }\n   - Pull context from currentView, selectedTopicId, activeNote, etc.",
        "testStrategy": "Manual testing: (1) Navigate to each view and verify correct breadcrumb shows, (2) Review with topic selected: verify 'Review > [Topic Name]' appears, (3) Click first segment and verify navigation to base view, (4) Test with long topic names (>25 chars) and verify truncation + tooltip, (5) Verify breadcrumb updates immediately on view change, (6) Test visual hierarchy (should be subtle, not dominating), (7) Verify separators render correctly (ChevronRight icons), (8) Keyboard navigation: tab through breadcrumb links and activate with Enter",
        "priority": "medium",
        "dependencies": [
          "79"
        ],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T02:02:11.900Z"
      },
      {
        "id": "81",
        "title": "Add Things 3 today badge pulse animation",
        "description": "Implement subtle pulse animation for the Today count badge in sidebar when due cards exist (due > 0), respecting prefers-reduced-motion, and stopping after user interaction",
        "details": "1. Add custom keyframes to tailwind.config.ts:\n   - In theme.extend.keyframes, add:\n     'badge-pulse': {\n       '0%, 100%': { transform: 'scale(1)', opacity: '1' },\n       '50%': { transform: 'scale(1.05)', opacity: '0.9' }\n     }\n   - In theme.extend.animation, add:\n     'badge-pulse': 'badge-pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite'\n2. Update Sidebar.tsx (around line 108-117 where Badge is rendered):\n   - Add state: const [hasSeenDueCards, setHasSeenDueCards] = useState(false)\n   - Load from localStorage on mount: localStorage.getItem('sidebar-seen-due-cards')\n   - Save to localStorage when user clicks Today or Review: localStorage.setItem('sidebar-seen-due-cards', 'true')\n   - Reset hasSeenDueCards to false when new cards become due (useEffect watching dueCount)\n3. Apply animation conditionally:\n   - On Today badge (line 67): add className based on conditions\n   - Condition: dueCount > 0 && !hasSeenDueCards && !prefersReducedMotion\n   - className: cn('animate-badge-pulse', ...other classes)\n4. Respect prefers-reduced-motion:\n   - Add hook: const prefersReducedMotion = window.matchMedia('(prefers-reduced-motion: reduce)').matches\n   - Use useEffect to set state from matchMedia\n   - Only animate when prefersReducedMotion === false\n5. Stop pulsing triggers:\n   - When user clicks Today nav item: setHasSeenDueCards(true)\n   - When user clicks Review and reviews any card: setHasSeenDueCards(true)\n   - When user completes review session: setHasSeenDueCards(true)\n6. Animation parameters:\n   - Duration: 2s (gentle, not distracting)\n   - Easing: cubic-bezier(0.4, 0, 0.6, 1) for smooth pulse\n   - Scale: 1.0 to 1.05 (subtle)\n   - Opacity: 1.0 to 0.9 (very subtle)\n   - Infinite loop until stopped\n7. Consider edge cases:\n   - Don't pulse when sidebar collapsed (check collapsed state)\n   - Don't pulse when badge not visible (dueCount === 0)\n   - Reset animation when badge value changes",
        "testStrategy": "Manual testing: (1) Set system to prefers-reduced-motion and verify no pulse, (2) With due cards, verify Today badge pulses smoothly, (3) Click Today and verify pulse stops immediately, (4) Complete review session and verify pulse stops, (5) Add new cards with due date today, verify pulse resumes, (6) Test with sidebar collapsed (should not pulse or pulse icon only), (7) Verify animation performance (should not cause jank), (8) Test with 0 due cards (no pulse), then increment to 1 (pulse starts), (9) Verify localStorage persists 'seen' state across app restarts",
        "priority": "low",
        "dependencies": [
          "79"
        ],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T02:02:11.904Z"
      },
      {
        "id": "82",
        "title": "Implement Browser Extension with Local HTTP API for Web Content Capture",
        "description": "Create a browser extension (Chrome/Firefox) with local HTTP API server in Electron main process to enable direct web content capture into DougHub's Knowledge Bank inbox",
        "details": "# Implementation Details\n\n## Phase 1: Local HTTP API Server in Electron Main Process\n\n### 1.1 Create HTTP Server Module (electron/api-server.ts)\n```typescript\nimport { createServer, IncomingMessage, ServerResponse } from 'http';\nimport { randomUUID } from 'crypto';\nimport { sourceItemQueries, DbSourceItem } from './database';\n\nconst API_PORT = 31337;\nconst ALLOWED_ORIGINS = ['chrome-extension://*', 'moz-extension://*'];\n\ninterface CapturePayload {\n  content: string;\n  url: string;\n  title: string;\n  sourceType: 'webcapture';\n  tags?: string[];\n  selectedText?: string;\n  pageHtml?: string;\n}\n\nexport function startApiServer() {\n  const server = createServer(handleRequest);\n  \n  server.listen(API_PORT, 'localhost', () => {\n    console.log(`[API Server] Listening on localhost:${API_PORT}`);\n  });\n  \n  return server;\n}\n\nasync function handleRequest(req: IncomingMessage, res: ServerResponse) {\n  // CORS headers for browser extensions\n  res.setHeader('Access-Control-Allow-Origin', '*');\n  res.setHeader('Access-Control-Allow-Methods', 'POST, GET, OPTIONS');\n  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');\n  \n  if (req.method === 'OPTIONS') {\n    res.writeHead(200);\n    res.end();\n    return;\n  }\n  \n  // Health check endpoint\n  if (req.method === 'GET' && req.url === '/api/health') {\n    res.writeHead(200, { 'Content-Type': 'application/json' });\n    res.end(JSON.stringify({ status: 'ok', version: '1.0.0' }));\n    return;\n  }\n  \n  // Capture endpoint\n  if (req.method === 'POST' && req.url === '/api/capture') {\n    let body = '';\n    req.on('data', chunk => body += chunk);\n    req.on('end', () => {\n      try {\n        const payload: CapturePayload = JSON.parse(body);\n        const sourceItem: DbSourceItem = {\n          id: randomUUID(),\n          sourceType: 'webcapture',\n          sourceName: new URL(payload.url).hostname,\n          sourceUrl: payload.url,\n          title: payload.title || 'Untitled Capture',\n          rawContent: payload.content,\n          canonicalTopicIds: [],\n          tags: payload.tags || [],\n          status: 'inbox',\n          createdAt: new Date().toISOString(),\n        };\n        \n        sourceItemQueries.insert(sourceItem);\n        \n        res.writeHead(201, { 'Content-Type': 'application/json' });\n        res.end(JSON.stringify({ success: true, id: sourceItem.id }));\n      } catch (error) {\n        res.writeHead(400, { 'Content-Type': 'application/json' });\n        res.end(JSON.stringify({ success: false, error: String(error) }));\n      }\n    });\n    return;\n  }\n  \n  res.writeHead(404);\n  res.end('Not found');\n}\n```\n\n### 1.2 Integrate Server into Main Process (electron/main.ts)\n```typescript\nimport { startApiServer } from './api-server';\n\nlet apiServer: ReturnType<typeof startApiServer> | null = null;\n\napp.whenReady().then(() => {\n  // ... existing initialization ...\n  \n  // Start local API server for browser extension\n  apiServer = startApiServer();\n  \n  createWindow();\n});\n\napp.on('before-quit', () => {\n  if (apiServer) {\n    apiServer.close();\n    console.log('[API Server] Closed');\n  }\n  closeDatabase();\n});\n```\n\n## Phase 2: Browser Extension - Option A (Tampermonkey Script)\n\n### 2.1 Create Userscript (browser-extension/doughub-capture.user.js)\n```javascript\n// ==UserScript==\n// @name         DougHub Web Capture\n// @namespace    http://doughub.local/\n// @version      1.0.0\n// @description  Capture web content to DougHub\n// @match        https://*/*\n// @match        http://*/*\n// @grant        GM_xmlhttpRequest\n// @connect      localhost\n// ==/UserScript==\n\n(function() {\n    'use strict';\n    \n    const DOUGHUB_API = 'http://localhost:31337/api/capture';\n    const captureQueue = [];\n    \n    // Check if DougHub is running\n    async function checkHealth() {\n        try {\n            const response = await fetch('http://localhost:31337/api/health');\n            return response.ok;\n        } catch {\n            return false;\n        }\n    }\n    \n    // Send capture to DougHub\n    function sendCapture(content, selectedText = null) {\n        const payload = {\n            content: selectedText || content,\n            url: window.location.href,\n            title: document.title,\n            sourceType: 'webcapture',\n            tags: [],\n        };\n        \n        GM_xmlhttpRequest({\n            method: 'POST',\n            url: DOUGHUB_API,\n            data: JSON.stringify(payload),\n            headers: { 'Content-Type': 'application/json' },\n            onload: (response) => {\n                if (response.status === 201) {\n                    showNotification('✓ Saved to DougHub', 'success');\n                } else {\n                    captureQueue.push(payload);\n                    showNotification('⏳ Queued (DougHub offline)', 'warning');\n                }\n            },\n            onerror: () => {\n                captureQueue.push(payload);\n                showNotification('⏳ Queued (DougHub offline)', 'warning');\n            }\n        });\n    }\n    \n    // Visual feedback notification\n    function showNotification(message, type) {\n        const div = document.createElement('div');\n        div.textContent = message;\n        div.style.cssText = `\n            position: fixed; top: 20px; right: 20px; z-index: 999999;\n            padding: 12px 20px; border-radius: 6px; font-family: sans-serif;\n            background: ${type === 'success' ? '#10b981' : '#f59e0b'};\n            color: white; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n            animation: slideIn 0.3s ease-out;\n        `;\n        document.body.appendChild(div);\n        setTimeout(() => div.remove(), 3000);\n    }\n    \n    // Context menu handler\n    document.addEventListener('contextmenu', (e) => {\n        const selection = window.getSelection()?.toString();\n        if (selection && selection.length > 10) {\n            sessionStorage.setItem('doughub_selected', selection);\n        }\n    });\n    \n    // Keyboard shortcut: Ctrl+Shift+D\n    document.addEventListener('keydown', (e) => {\n        if (e.ctrlKey && e.shiftKey && e.key === 'D') {\n            e.preventDefault();\n            const selection = window.getSelection()?.toString();\n            if (selection) {\n                sendCapture(document.body.innerText, selection);\n            } else {\n                sendCapture(document.body.innerText);\n            }\n        }\n    });\n    \n    // Retry queue periodically\n    setInterval(async () => {\n        if (captureQueue.length > 0 && await checkHealth()) {\n            while (captureQueue.length > 0) {\n                const item = captureQueue.shift();\n                sendCapture(item.content);\n            }\n        }\n    }, 30000); // Check every 30s\n})();\n```\n\n## Phase 3: Browser Extension - Option B (Native Chrome/Firefox Extension)\n\n### 3.1 Extension Manifest (browser-extension/manifest.json)\n```json\n{\n  \"manifest_version\": 3,\n  \"name\": \"DougHub Web Capture\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Capture web content directly to DougHub\",\n  \"permissions\": [\n    \"contextMenus\",\n    \"activeTab\",\n    \"storage\"\n  ],\n  \"host_permissions\": [\n    \"http://localhost:31337/*\"\n  ],\n  \"background\": {\n    \"service_worker\": \"background.js\"\n  },\n  \"content_scripts\": [\n    {\n      \"matches\": [\"<all_urls>\"],\n      \"js\": [\"content.js\"]\n    }\n  ],\n  \"icons\": {\n    \"16\": \"icons/icon16.png\",\n    \"48\": \"icons/icon48.png\",\n    \"128\": \"icons/icon128.png\"\n  },\n  \"action\": {\n    \"default_popup\": \"popup.html\",\n    \"default_icon\": \"icons/icon48.png\"\n  }\n}\n```\n\n### 3.2 Background Service Worker (browser-extension/background.js)\n```javascript\nconst DOUGHUB_API = 'http://localhost:31337/api/capture';\nconst captureQueue = [];\n\n// Create context menu on install\nchrome.runtime.onInstalled.addListener(() => {\n  chrome.contextMenus.create({\n    id: 'doughub-capture',\n    title: 'Send to DougHub',\n    contexts: ['selection']\n  });\n});\n\n// Handle context menu clicks\nchrome.contextMenus.onClicked.addListener((info, tab) => {\n  if (info.menuItemId === 'doughub-capture') {\n    captureContent({\n      content: info.selectionText,\n      url: tab.url,\n      title: tab.title,\n      sourceType: 'webcapture'\n    });\n  }\n});\n\n// Capture content to DougHub\nasync function captureContent(data) {\n  try {\n    const response = await fetch(DOUGHUB_API, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify(data)\n    });\n    \n    if (response.ok) {\n      chrome.notifications.create({\n        type: 'basic',\n        iconUrl: 'icons/icon48.png',\n        title: 'DougHub',\n        message: 'Content saved successfully!'\n      });\n    } else {\n      throw new Error('API request failed');\n    }\n  } catch (error) {\n    // Queue for later\n    captureQueue.push(data);\n    chrome.storage.local.set({ captureQueue });\n    \n    chrome.notifications.create({\n      type: 'basic',\n      iconUrl: 'icons/icon48.png',\n      title: 'DougHub Offline',\n      message: 'Content queued for later sync'\n    });\n  }\n}\n\n// Retry queue every 30 seconds\nsetInterval(async () => {\n  if (captureQueue.length > 0) {\n    try {\n      const health = await fetch('http://localhost:31337/api/health');\n      if (health.ok) {\n        while (captureQueue.length > 0) {\n          const item = captureQueue.shift();\n          await captureContent(item);\n        }\n        chrome.storage.local.set({ captureQueue: [] });\n      }\n    } catch {}\n  }\n}, 30000);\n```\n\n### 3.3 Popup UI (browser-extension/popup.html)\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <style>\n    body { width: 300px; padding: 16px; font-family: system-ui; }\n    .status { padding: 8px; border-radius: 4px; margin-bottom: 12px; }\n    .status.online { background: #d1fae5; color: #065f46; }\n    .status.offline { background: #fee2e2; color: #991b1b; }\n    button { width: 100%; padding: 10px; background: #8b5cf6; color: white; border: none; border-radius: 6px; cursor: pointer; }\n    button:hover { background: #7c3aed; }\n  </style>\n</head>\n<body>\n  <div id=\"status\" class=\"status\">Checking connection...</div>\n  <button id=\"captureBtn\">Capture Current Page</button>\n  <script src=\"popup.js\"></script>\n</body>\n</html>\n```\n\n### 3.4 Popup Logic (browser-extension/popup.js)\n```javascript\nconst statusDiv = document.getElementById('status');\nconst captureBtn = document.getElementById('captureBtn');\n\nasync function checkStatus() {\n  try {\n    const response = await fetch('http://localhost:31337/api/health');\n    if (response.ok) {\n      statusDiv.textContent = '✓ Connected to DougHub';\n      statusDiv.className = 'status online';\n      captureBtn.disabled = false;\n    }\n  } catch {\n    statusDiv.textContent = '✗ DougHub not running';\n    statusDiv.className = 'status offline';\n    captureBtn.disabled = true;\n  }\n}\n\ncaptureBtn.addEventListener('click', async () => {\n  const [tab] = await chrome.tabs.query({ active: true, currentWindow: true });\n  chrome.tabs.sendMessage(tab.id, { action: 'getPageContent' });\n});\n\ncheckStatus();\nsetInterval(checkStatus, 5000);\n```\n\n## Phase 4: Type Definitions and IPC Integration\n\n### 4.1 Update SourceType (src/types/index.ts)\n```typescript\n// Already exists: export type SourceType = 'qbank' | 'article' | 'pdf' | 'image' | 'audio' | 'quickcapture' | 'manual';\n// Extend to:\nexport type SourceType = 'qbank' | 'article' | 'pdf' | 'image' | 'audio' | 'quickcapture' | 'manual' | 'webcapture';\n```\n\n## Phase 5: Testing Strategy\n- Unit tests for HTTP API endpoints (health, capture)\n- Integration tests for SourceItem creation via API\n- Manual testing with Tampermonkey script on UWorld, UpToDate\n- Manual testing with native extension\n- Test offline queueing and sync behavior\n- Test CORS handling for extension origins\n- Performance testing: API response time <100ms\n- Security testing: verify localhost-only binding",
        "testStrategy": "1. Unit Testing:\n   - Test HTTP server startup/shutdown\n   - Test /api/health endpoint returns 200 with correct JSON\n   - Test /api/capture endpoint creates SourceItem correctly\n   - Test CORS headers are set properly\n   - Test invalid JSON payload returns 400\n\n2. Integration Testing:\n   - Test end-to-end flow: extension → API → database → Knowledge Bank UI\n   - Verify SourceItem appears in inbox with status='inbox'\n   - Test queuing mechanism when DougHub is offline\n   - Test queue sync when DougHub comes back online\n\n3. Manual Testing:\n   - Install Tampermonkey script and test on medical sites (UWorld, UpToDate)\n   - Install native extension and test context menu capture\n   - Test keyboard shortcut (Ctrl+Shift+D)\n   - Verify visual feedback notifications\n   - Test with DougHub closed, then open and verify sync\n\n4. Security Testing:\n   - Verify server only binds to localhost (not 0.0.0.0)\n   - Test that requests from remote IPs are rejected\n   - Verify no XSS vulnerabilities in captured content\n\n5. Performance Testing:\n   - Measure API response time (target <100ms)\n   - Test with large content payloads (>1MB)\n   - Verify no memory leaks in long-running server\n\n6. Cross-browser Testing:\n   - Test extension in Chrome\n   - Test extension in Firefox (may need manifest.json v2 variant)\n   - Test Tampermonkey script in both browsers",
        "priority": "medium",
        "dependencies": [],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T02:17:22.219Z"
      },
      {
        "id": "83",
        "title": "OCR Image Text Search Integration",
        "description": "Extract text from images using Tesseract.js and index in SQLite FTS5 for full-text search",
        "details": "1. Install tesseract.js: `npm install tesseract.js`\n2. Create `electron/ocr-service.ts`:\n   - Export `extractTextFromImage(imagePath: string): Promise<string>`\n   - Use Tesseract.createWorker() with eng language model\n   - Handle worker lifecycle (create, load, recognize, terminate)\n   - Return extracted text with confidence scores\n3. Extend database schema migration (v4):\n   - Create FTS5 virtual table: `CREATE VIRTUAL TABLE search_index USING fts5(itemId UNINDEXED, content, tokenize='porter unicode61')`\n   - Add `extractedText` column to source_items table\n4. Update `electron/ipc-handlers.ts`:\n   - Add handler `sourceItems:extractText` that calls OCR service\n   - Trigger OCR on image upload/import\n   - Store extracted text in source_items.extractedText\n   - Insert into search_index FTS5 table\n5. Add search handler `search:query`:\n   - Query FTS5 table with MATCH operator: `SELECT itemId FROM search_index WHERE content MATCH ?`\n   - Return ranked results with snippet() and highlight()\n   - Support BM25 ranking for relevance\n6. Update QuickDumpModal to support image OCR:\n   - Show OCR progress indicator\n   - Display extracted text preview\n   - Allow manual correction of OCR results\n7. Performance targets:\n   - OCR processing: <10s per image\n   - Search query: <200ms\n   - Index updates: async, non-blocking",
        "testStrategy": "1. Unit tests (vitest):\n   - Test OCR service with sample medical images (prescription, handwritten notes)\n   - Mock Tesseract worker, verify text extraction\n   - Test FTS5 query building and result ranking\n2. Integration tests:\n   - Upload image → verify extractedText populated\n   - Search for OCR'd content → verify results returned\n   - Test search highlighting and snippets\n3. Performance benchmarks:\n   - Measure OCR time on various image sizes (100KB - 5MB)\n   - Verify search query < 200ms on 1000+ items\n   - Test concurrent OCR operations (queue management)\n4. Manual testing:\n   - Import medical textbook screenshots\n   - Search for specific terms (drug names, diagnoses)\n   - Verify accuracy on handwritten notes (lower expectations)\n5. Edge cases:\n   - Test with corrupted images\n   - Test with non-text images (charts, graphs)\n   - Verify graceful degradation on OCR failure",
        "priority": "medium",
        "dependencies": [],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T04:02:38.830Z"
      },
      {
        "id": "84",
        "title": "Video/Audio Transcription Search with Whisper",
        "description": "Transcribe video and audio content using Whisper API/local model for searchable text index",
        "details": "1. Evaluate Whisper implementation options:\n   - Option A: OpenAI Whisper API (requires API key, 25MB limit)\n   - Option B: whisper.cpp Node bindings for local inference\n   - Recommendation: Start with OpenAI API, add whisper.cpp option for privacy\n2. Install dependencies:\n   - `npm install @openai/whisper` (if using API)\n   - OR compile whisper.cpp bindings for local\n3. Create `electron/transcription-service.ts`:\n   - Export `transcribeAudio(filePath: string): Promise<{text: string, segments: Array<{start: number, end: number, text: string}>}>`\n   - Support formats: mp3, mp4, m4a, wav, webm\n   - Implement chunking for files >25MB (split audio)\n   - Cache transcriptions to avoid re-processing\n4. Update source_items schema:\n   - `transcription` column already exists (verified in database.ts:209)\n   - Add `transcriptionSegments` JSON column for timestamp-based search\n5. Add IPC handlers:\n   - `sourceItems:transcribe` - trigger transcription\n   - `sourceItems:searchTranscription` - search within timestamps\n   - Show transcription progress (0-100%) in UI\n6. Extend FTS5 index from Task 83:\n   - Insert transcription text into search_index\n   - Link segments to source video timestamp for playback jumping\n7. UI updates in QuickDumpModal:\n   - Show transcription status indicator\n   - Display transcript with timestamps\n   - Click timestamp → jump to video position (future enhancement)\n8. Background processing:\n   - Use electron background workers to avoid blocking UI\n   - Queue transcription jobs (FIFO)\n   - Persist queue state across app restarts",
        "testStrategy": "1. Unit tests:\n   - Mock Whisper API responses\n   - Test audio chunking logic for large files\n   - Verify segment timestamp parsing\n2. Integration tests:\n   - Upload sample medical lecture audio (5 min)\n   - Verify transcription completes successfully\n   - Search transcription → verify results with timestamps\n   - Test FTS5 integration with transcribed content\n3. Performance tests:\n   - Measure transcription time vs audio duration (expect 1:1 ratio for API, 1:3 for local)\n   - Test queue handling with multiple concurrent jobs\n   - Verify memory usage stays <500MB during transcription\n4. Manual validation:\n   - Import medical podcast episode\n   - Search for specific medical terms\n   - Verify transcript accuracy (medical terminology)\n5. Edge cases:\n   - Test with corrupted audio files\n   - Handle API rate limits gracefully\n   - Test with non-English audio (future i18n)\n6. Cost monitoring (if using API):\n   - Track API usage and costs\n   - Warn user when approaching limits",
        "priority": "medium",
        "dependencies": [
          "83"
        ],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T04:02:38.835Z"
      },
      {
        "id": "85",
        "title": "Voice Search Interface (Web Speech API)",
        "description": "Implement voice-to-text search using Web Speech API for hands-free query input",
        "details": "1. Create `src/components/search/VoiceSearchButton.tsx`:\n   - Use Web Speech API (SpeechRecognition)\n   - Microphone button with pulsing animation during listening\n   - Display interim results in real-time\n   - Auto-submit on silence detection (1s timeout)\n2. Implement speech recognition service:\n   - Check browser support: `'SpeechRecognition' in window || 'webkitSpeechRecognition' in window`\n   - Configure: continuous=false, interimResults=true, maxAlternatives=1\n   - Handle language: default to 'en-US', detect medical terminology\n3. Integrate with existing search:\n   - Convert speech to text query\n   - Trigger existing FTS5 search (from Tasks 83/84)\n   - Show confidence indicator for voice recognition\n4. UI/UX considerations:\n   - Keyboard shortcut: Ctrl+Shift+V to activate voice search\n   - Visual feedback: waveform animation while listening\n   - Error handling: \"Microphone access denied\" message\n   - Fallback to text input if API unavailable\n5. Add to QuickDumpModal and global search:\n   - Small microphone icon next to search input\n   - Tooltip: \"Voice search (Ctrl+Shift+V)\"\n6. Medical vocabulary optimization:\n   - Pre-load medical term dictionary\n   - Use custom grammar hints if supported\n   - Implement fuzzy matching for misrecognized terms\n7. Privacy considerations:\n   - All processing happens client-side (browser API)\n   - No audio sent to external servers\n   - Add privacy notice in settings",
        "testStrategy": "1. Unit tests:\n   - Mock SpeechRecognition API\n   - Test interim result handling\n   - Verify query submission on final result\n2. Browser compatibility tests:\n   - Chrome/Edge (WebKit Speech API)\n   - Firefox (limited support, test graceful degradation)\n   - Safari (WebKit Speech API)\n3. Manual testing:\n   - Speak common medical terms (\"hypertension\", \"myocardial infarction\")\n   - Test with background noise (verify accuracy degradation)\n   - Test voice search in Review mode (hands-free workflow)\n4. Accessibility testing:\n   - Verify keyboard navigation works\n   - Screen reader announces voice search mode\n   - Test with high contrast themes\n5. Edge cases:\n   - Test microphone permission denial\n   - Handle speech recognition timeout\n   - Test rapid consecutive voice searches\n6. Performance:\n   - Verify no memory leaks from SpeechRecognition instances\n   - Test microphone cleanup on component unmount",
        "priority": "low",
        "dependencies": [
          "83"
        ],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T04:02:38.839Z"
      },
      {
        "id": "86",
        "title": "Semantic Intent Search with Local Embeddings",
        "description": "Implement AI-powered semantic search using Ollama embeddings for intent-based querying beyond keyword matching",
        "details": "1. Leverage existing Ollama integration (electron/ai-service.ts):\n   - Add embedding model support: `nomic-embed-text` (137M params, 768 dims)\n   - Create `generateEmbedding(text: string): Promise<number[]>` function\n   - Use OpenAI-compatible embeddings endpoint: `POST /api/embeddings`\n2. Extend database schema (migration v5):\n   - Add `embeddings` table: `CREATE TABLE embeddings (itemId TEXT PRIMARY KEY, vector BLOB, updatedAt TEXT)`\n   - Store embeddings as BLOB (efficient storage for 768 float32 values = 3KB per item)\n   - Index on itemId for fast lookup\n3. Implement vector similarity search:\n   - Cosine similarity function in SQLite: `SELECT itemId, (dot_product / (norm1 * norm2)) AS similarity FROM embeddings`\n   - Use better-sqlite3 user-defined functions for vector math\n   - Pre-compute vector norms for optimization\n4. Create hybrid search pipeline:\n   - Step 1: FTS5 keyword search (fast, high precision)\n   - Step 2: Semantic search on query embedding (slower, high recall)\n   - Merge results with weighted scoring: 0.7 * keyword_score + 0.3 * semantic_score\n   - Return top 20 results ranked by combined score\n5. Add IPC handlers:\n   - `search:semantic` - perform semantic search\n   - `embeddings:generate` - batch generate embeddings for existing items\n   - `embeddings:status` - show indexing progress\n6. Background embedding generation:\n   - Process source_items in batches (10 items at a time)\n   - Rate limit to avoid overwhelming Ollama (1 req/sec)\n   - Show progress in UI: \"Indexing for semantic search: 45/120 items\"\n7. Query enhancement:\n   - Expand user queries with synonyms using embeddings\n   - Example: \"chest pain\" → also match \"angina\", \"myocardial infarction\"\n   - Use embedding similarity threshold: >0.8 for high confidence matches\n8. Configuration:\n   - Add setting to enable/disable semantic search\n   - Allow user to select embedding model\n   - Fallback to keyword-only if Ollama unavailable",
        "testStrategy": "1. Unit tests:\n   - Test vector similarity calculations (cosine distance)\n   - Mock Ollama embedding API responses\n   - Verify embedding storage/retrieval from SQLite BLOB\n2. Integration tests:\n   - Generate embeddings for 100 sample medical notes\n   - Query: \"heart attack\" → verify \"MI\", \"infarction\" matched\n   - Compare semantic vs keyword-only recall metrics\n3. Performance benchmarks:\n   - Embedding generation: <5s per item (Ollama local)\n   - Similarity search: <500ms on 1000 items\n   - Measure index size: expect ~3KB per item for 768-dim embeddings\n4. Quality evaluation:\n   - Test semantic matching accuracy on medical synonyms\n   - Example queries: \"HTN\" should match \"hypertension\", \"high blood pressure\"\n   - Measure precision@10 and recall@10 metrics\n5. Manual testing:\n   - Query: \"What causes shortness of breath?\" (intent-based)\n   - Verify results include \"dyspnea\", \"respiratory distress\" content\n   - Test cross-lingual search if multi-language content exists\n6. Fallback testing:\n   - Verify graceful degradation when Ollama offline\n   - Test with embedding model not installed (auto-download prompt)\n7. Resource monitoring:\n   - Verify Ollama memory usage stays <2GB during embedding generation\n   - Test concurrent embedding requests don't block UI",
        "priority": "medium",
        "dependencies": [
          "83"
        ],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T04:02:38.843Z"
      },
      {
        "id": "87",
        "title": "Context-Aware Search Suggestions",
        "description": "AI-powered search suggestions like 'Cards comparing X and Y' based on user study patterns and context",
        "details": "1. Create suggestion generation service `electron/search-suggestions-service.ts`:\n   - Analyze user's study patterns from review_logs table\n   - Identify weak topics (high difficulty, low retention)\n   - Generate contextual suggestions using AI (Ollama/OpenAI)\n2. Suggestion types to implement:\n   - Comparative: \"Cards comparing hypertension and heart failure\"\n   - Weak topics: \"Review cards on pharmacology (60% accuracy)\"\n   - Related: \"Cards related to what you studied yesterday\"\n   - Time-based: \"Cards due in next 3 days on cardiology\"\n   - Similarity: \"More cards like [recently reviewed card]\"\n3. Context gathering:\n   - Current time of day (morning = new content, evening = review)\n   - Recent review performance (last 24h accuracy)\n   - Upcoming due cards by topic\n   - User's domain focus (cardiology, neurology, etc.)\n4. AI prompt template:\n   ```\n   Given:\n   - User's weak topics: {weakTopics}\n   - Recently reviewed: {recentCards}\n   - Due today: {dueCards}\n   - Study patterns: {patterns}\n   \n   Generate 5 actionable search suggestions in natural language.\n   Format: [Action] [Content] [Context]\n   Example: \"Review weak cards on renal physiology (45% accuracy this week)\"\n   ```\n5. Implement suggestion UI:\n   - Add SuggestionChip component below search bar\n   - Display 3-5 rotating suggestions\n   - Click suggestion → auto-populate search + execute\n   - Refresh suggestions every 30 minutes or on user action\n6. Add IPC handlers:\n   - `search:getSuggestions` - generate context-aware suggestions\n   - `search:trackClick` - track which suggestions are useful (analytics)\n7. Caching and optimization:\n   - Cache suggestions for 30 minutes (avoid repeated AI calls)\n   - Pre-generate suggestions in background during idle time\n   - Use cheaper/faster model for suggestions (gpt-3.5-turbo or llama3:8b)\n8. Analytics tracking:\n   - Track suggestion click-through rate\n   - Learn which suggestion types are most useful\n   - Adapt suggestion generation based on user preferences\n9. Fallback mechanism:\n   - If AI unavailable, use rule-based suggestions:\n     - \"Cards due today\"\n     - \"Review weak topics\"\n     - \"Continue yesterday's session\"",
        "testStrategy": "1. Unit tests:\n   - Mock review_logs and card data\n   - Test suggestion generation with various contexts\n   - Verify AI prompt construction\n2. Integration tests:\n   - Simulate user study pattern (e.g., 10 reviews, 60% accuracy)\n   - Generate suggestions → verify relevance to weak topics\n   - Test suggestion caching (verify AI not called on duplicate requests)\n3. Quality evaluation:\n   - Manual review of 50 generated suggestions\n   - Rate each suggestion: relevant (1), somewhat relevant (0.5), irrelevant (0)\n   - Target: >80% relevance score\n4. Performance tests:\n   - Suggestion generation time: <3s (with AI), <500ms (rule-based)\n   - Verify background pre-generation doesn't block UI\n   - Test with 1000+ cards and 5000+ review logs\n5. A/B testing framework:\n   - Track metrics: suggestion CTR, study session length, retention improvement\n   - Compare AI suggestions vs rule-based suggestions\n6. Manual validation:\n   - Test suggestions at different times of day\n   - Verify suggestions adapt after reviewing suggested cards\n   - Test with new user (no history) → verify graceful degradation\n7. Edge cases:\n   - User with no review history (show generic suggestions)\n   - User with perfect retention (suggest advanced topics)\n   - AI service timeout (fall back to rule-based)",
        "priority": "low",
        "dependencies": [
          "83",
          "86"
        ],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T04:02:38.847Z"
      },
      {
        "id": "88",
        "title": "Add Vision API Support with Camera/Webcam Capture",
        "description": "Extend QuickDumpModal with camera/webcam capture and integrate multimodal AI vision analysis for images with text context",
        "details": "1. Add camera capture UI to QuickDumpModal.tsx:\n   - Add camera button alongside existing Browse image button\n   - Implement webcam stream using navigator.mediaDevices.getUserMedia API\n   - Create camera preview modal with capture/retake/cancel controls\n   - Handle camera permissions and error states gracefully\n   - Convert captured frame to base64 for consistency with existing image flow\n\n2. Extend ai-service.ts with vision capabilities:\n   - Add new multimodal prompt: 'imageAnalysis' for vision + text context\n   - Create extractImageConcepts(imageData: string, textContext?: string) function\n   - Use OpenAI Vision API (gpt-4o-mini supports vision) or Claude Vision\n   - Send both image (as base64 or URL) and accompanying text in single request\n   - Parse response to extract: text from image, concept identification, and context understanding\n   - Return ConceptExtractionResult with image-specific metadata\n\n3. IPC Layer updates:\n   - Add ai:extractImageConcepts handler in ipc-handlers.ts\n   - Update preload.ts to expose window.api.ai.extractImageConcepts\n   - Update electron.d.ts type definitions\n\n4. Integration in QuickDumpModal:\n   - When image saved, optionally trigger AI vision analysis\n   - Show loading state during analysis\n   - Store analysis results in SourceItem.rawContent or new field\n   - Maintain existing flow: save image → inbox → process later\n\n5. Research and implement:\n   - Cost optimization: Use Claude Haiku for vision (cheaper than GPT-4V)\n   - Image similarity search: Investigate OpenAI Embeddings API for images or third-party services (Google Vision API reverse image search)\n   - Add caching for vision results using existing aiCache infrastructure\n\nPseudo-code for camera capture:\n```typescript\nconst stream = await navigator.mediaDevices.getUserMedia({ video: true });\nvideoRef.current.srcObject = stream;\nconst canvas = document.createElement('canvas');\ncanvas.getContext('2d').drawImage(videoRef.current, 0, 0);\nconst imageData = canvas.toDataURL('image/png');\nstream.getTracks().forEach(track => track.stop());\n```\n\nPseudo-code for vision API:\n```typescript\nawait client.chat.completions.create({\n  model: 'gpt-4o-mini', // or claude-3-haiku-20240307\n  messages: [\n    { role: 'system', content: PROMPTS.imageAnalysis },\n    { role: 'user', content: [\n      { type: 'text', text: textContext || 'Analyze this medical image' },\n      { type: 'image_url', image_url: { url: imageData } }\n    ]}\n  ]\n});\n```",
        "testStrategy": "Unit tests:\n- Mock navigator.mediaDevices.getUserMedia for camera capture flow\n- Test vision API with sample medical images (diagrams, charts)\n- Verify multimodal request format matches OpenAI/Anthropic specs\n- Test error handling for unsupported models, API failures, timeout\n\nIntegration tests:\n- E2E test: Open QuickDump → Enable camera → Capture → Verify image saved\n- Test vision analysis with image + text context, verify concepts extracted\n- Test cost optimization: Verify Haiku used for vision when Anthropic selected\n- Test caching: Same image + context should hit cache on second call\n\nManual testing:\n- Test camera permissions prompt and denial handling\n- Capture medical diagram, verify AI identifies components correctly\n- Test with different image types (photo, screenshot, diagram)\n- Verify vision analysis respects provider config (Anthropic vs OpenAI)",
        "priority": "high",
        "dependencies": [],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T04:07:32.984Z"
      },
      {
        "id": "89",
        "title": "Design and Implement Unified Capture Pathway UX",
        "description": "Audit and redesign capture UX to clearly guide users through three pathways: Quick Dump → Inbox, Main Page → Extract Concepts, and Learning Pipeline with visual cues and progressive disclosure",
        "details": "1. UX Audit of current pathways:\n   - Quick Dump (Ctrl+Q): Emergency capture → inbox (status='inbox')\n   - Main Page concept extraction: Immediate AI processing\n   - Learning Pipeline: Structured card creation from Notebook\n   - Identify confusion points: When to use which pathway? What happens after capture?\n\n2. Design visual pathway indicators:\n   - Add subtle color coding: Quick Dump (gray/muted), Extract Concepts (purple/primary), Learning Pipeline (green/success)\n   - Use icons consistently: Quick Dump (Upload), Extract Concepts (Sparkles/AI), Learning Pipeline (GraduationCap)\n   - Add progress indicators showing where content goes: Inbox → Notebook → Cards\n\n3. Implement progressive disclosure onboarding:\n   - First-time user: Show tooltip on Quick Dump explaining \"Capture now, process later\"\n   - After 3 Quick Dumps: Suggest \"Process inbox items to extract concepts\"\n   - After first concept extraction: Highlight Learning Pipeline for card creation\n   - Use localStorage to track onboarding state, avoid annoying repeat users\n   - Add \"Show tips\" toggle in settings for users who want help\n\n4. Create pathway decision helper:\n   - Add \"?\" button in capture modals explaining when to use each pathway\n   - Modal content: \n     * Quick Dump: \"Urgent capture during rounds? Save for later processing\"\n     * Extract Concepts: \"Have 2 minutes? Let AI identify key concepts immediately\"\n     * Learning Pipeline: \"Ready to study? Convert concepts to flashcards\"\n\n5. Update UI components:\n   - QuickDumpModal: Add subtle header text \"Capture for later\" with inbox icon\n   - Main page capture area: Add \"Extract concepts with AI\" header\n   - Add breadcrumb/stepper component showing: Capture → Process → Review\n\n6. User testing plan:\n   - Recruit 3-5 medical residents from target audience\n   - Task 1: \"You're on rounds and see something important. Capture it.\"\n   - Task 2: \"You have study time. Create flashcards from a topic.\"\n   - Task 3: \"Review your inbox and process items.\"\n   - Measure: Time to complete, errors, verbalized confusion\n   - Iterate based on feedback\n\nResearch:\n- Review Notion's capture UX (Quick Add vs Database entry)\n- Study Anki's note type selection UX\n- Analyze Readwise Reader's triage workflow (Later/Archive/Discard)\n\nPseudo-code for onboarding:\n```typescript\nconst onboardingState = {\n  quickDumpTooltipSeen: false,\n  conceptExtractionPromptSeen: false,\n  learningPipelineHighlighted: false\n};\n\nif (!onboardingState.quickDumpTooltipSeen && isFirstQuickDump) {\n  showTooltip('Quick Dump saves to inbox for later processing');\n  updateOnboardingState({ quickDumpTooltipSeen: true });\n}\n```",
        "testStrategy": "UX Testing:\n- A/B test current vs redesigned UI with 10 medical residents\n- Measure task completion time for each pathway\n- Track error rate (users choosing wrong pathway for task)\n- Survey: \"Which pathway would you use for [scenario]?\" (5 scenarios)\n\nFunctional tests:\n- Test onboarding tooltips appear at correct triggers\n- Verify localStorage persistence of onboarding state\n- Test pathway decision helper modal renders correctly\n- Verify color coding and icons display consistently\n\nAccessibility tests:\n- Screen reader announces pathway differences clearly\n- Keyboard navigation through pathway options\n- Color contrast meets WCAG AA for pathway indicators\n\nUsability metrics:\n- Track pathway selection frequency via analytics\n- Measure inbox processing rate (are users using Quick Dump correctly?)\n- Monitor help modal open rate (high = confusion)\n- Post-launch survey: \"Do you understand when to use each capture method?\"",
        "priority": "medium",
        "dependencies": [],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T04:07:32.989Z"
      },
      {
        "id": "90",
        "title": "Enhance AI Service for Image Context Understanding",
        "description": "Extend AI service to accept multimodal inputs (image + text) and generate appropriate card formats based on image type (diagrams → cloze, algorithms → vignettes)",
        "details": "1. Create new AI prompt for image context understanding:\n```typescript\nconst PROMPTS = {\n  ...existing,\n  imageContextAnalysis: `You are a medical education AI with vision capabilities.\nAnalyze the image and accompanying text to:\n1. Extract visible text/labels from the image (OCR)\n2. Identify the image type: anatomical diagram, flow chart, algorithm, table, clinical photo, radiograph, etc.\n3. Cross-reference visual elements with text context to understand medical relevance\n4. Suggest optimal card format:\n   - Labeled diagrams → multiple cloze deletions (one per label)\n   - Algorithms/flowcharts → clinical vignettes testing decision points\n   - Tables → Q&A cards for each row/relationship\n   - Clinical photos → image occlusion or vignette\n5. Generate concepts that test understanding, not just memorization\n\nRespond with JSON: { imageType, extractedText, concepts[], suggestedCardFormat }`\n};\n```\n\n2. Implement smart card format selection:\n   - detectImageType(imageData, textContext) → 'diagram' | 'algorithm' | 'table' | 'photo' | 'radiograph'\n   - generateCardsFromImage(imageData, textContext, imageType) → Card[]\n   - For diagrams: Create overlapping cloze cards (test same diagram with different deletions)\n   - For algorithms: Convert decision nodes to vignettes using existing convertToVignette\n   - For tables: Extract rows as Q&A pairs\n\n3. Integrate image similarity search:\n   - Research options:\n     * OpenAI Embeddings API: Generate image embeddings, compare cosine similarity\n     * Google Vision API: Reverse image search for medical diagrams\n     * Open source: CLIP embeddings (local, free, privacy-preserving)\n   - Implement findSimilarImages(imageData) → SimilarImage[]\n   - Use results to enrich concept extraction: \"This appears to be [diagnosis] based on similar images\"\n   - Cache embeddings to avoid redundant API calls\n\n4. Add multimodal concept extraction:\n   - Update extractConcepts to accept optional imageData parameter\n   - When both text and image present:\n     * Send multimodal request to vision-capable model\n     * Use text as context to disambiguate image content\n     * Example: \"Heart murmur timing\" + image of cardiac cycle → identifies systolic vs diastolic\n   - Fallback: If vision API unavailable, extract text concepts only and save image separately\n\n5. Optimize for cost and performance:\n   - Resize images before sending (max 1024px for vision APIs)\n   - Use Claude Haiku for vision (cheapest vision model as of 2025)\n   - Implement tiered processing:\n     * Local CLIP for image type detection (free)\n     * Cloud vision API only for concept extraction (paid)\n   - Add cost tracking: Log tokens + image count per request\n\n6. Update learning pipeline integration:\n   - When SourceItem has mediaPath + rawContent, trigger image analysis\n   - Store results in new field: aiAnalysis: { imageType, concepts, suggestedFormat }\n   - Surface in Notebook UI: \"AI suggests creating 5 cloze cards from this diagram\"\n\nPseudo-code:\n```typescript\nasync function extractConceptsMultimodal(\n  textContent: string,\n  imageData?: string\n): Promise<ConceptExtractionResult> {\n  if (!imageData) return extractConcepts(textContent);\n  \n  const imageType = await detectImageType(imageData, textContent);\n  const response = await callVisionAPI(\n    PROMPTS.imageContextAnalysis,\n    textContent,\n    imageData\n  );\n  \n  const parsed = parseAIResponse<ImageAnalysisResponse>(response);\n  \n  return {\n    concepts: parsed.concepts.map(c => ({\n      ...c,\n      suggestedFormat: inferCardFormat(imageType, c)\n    })),\n    imageMetadata: {\n      type: imageType,\n      extractedText: parsed.extractedText,\n      suggestedCardFormat: parsed.suggestedCardFormat\n    }\n  };\n}\n```",
        "testStrategy": "Unit tests:\n- Mock vision API responses for different image types\n- Test card format selection logic:\n  * Diagram image → returns multiple cloze cards\n  * Algorithm image → returns vignette cards\n  * Table image → returns Q&A cards\n- Test image resizing before API submission\n- Verify cost tracking logs tokens and image count\n\nIntegration tests:\n- Test with real medical images:\n  * Anatomical diagram (e.g., cardiac cycle) → verify cloze cards with correct labels\n  * Clinical algorithm (e.g., ACLS) → verify vignettes test decision points\n  * Lab values table → verify Q&A pairs extract relationships\n- Test multimodal extraction with image + question text\n- Test fallback when vision API unavailable\n\nPerformance tests:\n- Measure image resize time (should be <100ms)\n- Test vision API latency (track p50, p95, p99)\n- Verify caching prevents duplicate vision API calls\n- Load test: 10 concurrent image analyses\n\nCost validation:\n- Calculate cost per image analysis (target: <$0.01/image with Haiku)\n- Verify image resizing reduces API costs\n- Test CLIP local detection saves API calls\n\nManual validation:\n- Medical resident reviews AI-generated cards from images\n- Verify suggested card formats are pedagogically sound\n- Test with edge cases: blurry images, handwritten notes, complex diagrams",
        "priority": "high",
        "dependencies": [
          "88"
        ],
        "status": "deferred",
        "subtasks": [],
        "updatedAt": "2026-01-06T04:07:32.993Z"
      },
      {
        "id": 91,
        "title": "Debug and Fix Ollama AI Extraction Pipeline",
        "description": "Troubleshoot and resolve AI extraction issues with Ollama integration, focusing on JSON parsing, model compatibility, and response handling in the extractConcepts() function",
        "details": "**Current State Analysis:**\n- AI extraction connected to Ollama via OpenAI-compatible SDK\n- Model: qwen2.5:7b-instruct (ai-service.ts:117)\n- JSON response parsing with markdown code block handling (ai-service.ts:569-583)\n- Debug logging added (ai-service.ts:629-633)\n- IPC caching layer implemented (ipc-handlers.ts:793-810)\n- Unit tests exist showing expected behavior (tests/unit/ai-service.test.ts:90-111)\n\n**Root Cause Investigation Steps:**\n\n1. **Verify Ollama Model Response Format**\n   - Test extractConcepts() with actual Ollama instance\n   - Compare Ollama response structure vs expected ConceptExtractionResponse\n   - Check if model returns markdown-wrapped JSON or raw JSON\n   - Verify model outputs valid JSON matching schema: {concepts: [{text, conceptType, confidence, suggestedFormat}]}\n\n2. **Test Alternative Ollama Models**\n   - Current: qwen2.5:7b-instruct\n   - Alternatives to test: llama3.2:3b, mistral:7b, phi3:medium\n   - Evaluate JSON formatting reliability across models\n   - Update PROVIDER_PRESETS.ollama.model if better model found\n\n3. **Enhance JSON Parsing Robustness**\n   - Review parseAIResponse() (ai-service.ts:569-583)\n   - Add handling for common Ollama formatting issues:\n     * Extra whitespace/newlines\n     * Mixed text+JSON responses\n     * Non-standard markdown blocks\n   - Add validation that parsed.concepts is array before mapping\n   - Log raw response before parsing for debugging\n\n4. **Validate Response Structure**\n   - Ensure extractConcepts() validates all required fields exist\n   - Add defensive checks in ai-service.ts:636-654:\n     * Verify concept.text exists and is string\n     * Verify conceptType is valid string\n     * Verify confidence is numeric (already clamped to 0-1)\n     * Verify suggestedFormat is 'qa' or 'cloze'\n   - Filter out malformed concepts instead of failing entire extraction\n\n5. **Test End-to-End Flow**\n   - Paste medical content in CaptureInterface\n   - Verify IPC call to ai:extractConcepts (CaptureInterface.tsx:163)\n   - Check console logs for response structure\n   - Verify concepts display in ConceptCheckbox components\n   - Test with various content types: definitions, lists, mechanisms\n\n**Implementation Approach:**\n\n```typescript\n// Enhanced parseAIResponse with better error handling\nfunction parseAIResponse<T>(text: string): T {\n  let cleaned = text.trim();\n  \n  // Remove markdown code blocks (supports ```json, ```JSON, ```)\n  const jsonBlockMatch = cleaned.match(/```(?:json)?\\s*([\\s\\S]*?)```/);\n  if (jsonBlockMatch) {\n    cleaned = jsonBlockMatch[1].trim();\n  }\n  \n  // Find first { to last } to extract JSON from mixed content\n  const firstBrace = cleaned.indexOf('{');\n  const lastBrace = cleaned.lastIndexOf('}');\n  if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {\n    cleaned = cleaned.slice(firstBrace, lastBrace + 1);\n  }\n  \n  try {\n    return JSON.parse(cleaned) as T;\n  } catch (e) {\n    console.error('[AI Service] JSON parse failed. Raw text:', text);\n    throw new Error(`Failed to parse AI response: ${e.message}`);\n  }\n}\n\n// Enhanced extractConcepts with validation\nexport async function extractConcepts(content: string): Promise<ConceptExtractionResult> {\n  // ... existing code ...\n  \n  console.log('[AI Service] Raw concept extraction response:', conceptsResponse);\n  const parsed = parseAIResponse<ConceptExtractionResponse>(conceptsResponse);\n  console.log('[AI Service] Parsed response:', parsed);\n  \n  // Validate and filter concepts\n  if (!parsed || !Array.isArray(parsed.concepts)) {\n    console.error('[AI Service] Invalid response structure:', parsed);\n    throw new Error('Invalid response: missing concepts array');\n  }\n  \n  const validConcepts = parsed.concepts.filter(c => {\n    const isValid = \n      typeof c.text === 'string' && c.text.trim().length > 0 &&\n      typeof c.conceptType === 'string' &&\n      typeof c.confidence === 'number' &&\n      (c.suggestedFormat === 'qa' || c.suggestedFormat === 'cloze');\n    \n    if (!isValid) {\n      console.warn('[AI Service] Filtered invalid concept:', c);\n    }\n    return isValid;\n  });\n  \n  // ... rest of existing code ...\n}\n```\n\n**Environment Variables to Test:**\n- AI_MODEL=llama3.2:3b (test alternative models)\n- AI_BASE_URL=http://localhost:11434/v1 (verify correct endpoint)\n\n**Files to Modify:**\n- electron/ai-service.ts (parseAIResponse, extractConcepts)\n- Potentially electron/ai-service.ts:117 (update default model)\n\n**Debugging Checklist:**\n- [ ] Verify Ollama is running on localhost:11434\n- [ ] Test Ollama API directly: curl http://localhost:11434/api/generate\n- [ ] Check if model returns consistent JSON format\n- [ ] Add console.log before/after JSON.parse()\n- [ ] Test with simple input: \"Pneumonia is an infection of the lungs\"\n- [ ] Verify concepts array populates in CaptureInterface state\n- [ ] Check for errors in Electron dev console",
        "testStrategy": "**Unit Testing:**\n1. Extend tests/unit/ai-service.test.ts:\n   - Test parseAIResponse with markdown-wrapped JSON\n   - Test parseAIResponse with mixed text+JSON\n   - Test parseAIResponse with malformed responses\n   - Test extractConcepts filters invalid concepts\n   - Test extractConcepts with Ollama-style responses\n\n**Integration Testing:**\n1. Manual testing with real Ollama instance:\n   - Start Ollama: ollama serve\n   - Pull test models: ollama pull qwen2.5:7b-instruct\n   - Run npm run dev\n   - Test extraction with:\n     * Simple definition: \"Pneumonia is lung infection\"\n     * Medical list: \"Causes of cough: 1. Asthma 2. COPD\"\n     * Complex mechanism: \"Heart failure occurs when...\"\n   - Verify concepts appear in UI\n   - Check console logs for parsing errors\n\n2. Model comparison testing:\n   - Test same content with 3+ models\n   - Measure: response time, JSON validity, concept quality\n   - Document best-performing model\n\n**End-to-End Testing:**\n1. Complete capture workflow:\n   - Paste content → Extract Concepts button → concepts display\n   - Select concepts → Create Cards → verify save success\n   - Check database for created cards\n   - Verify no errors in console\n\n**Success Criteria:**\n- [ ] extractConcepts() returns valid ExtractedConcept[] array\n- [ ] No JSON parsing errors in console logs\n- [ ] Concepts display correctly in CaptureInterface\n- [ ] All unit tests pass: npm run test:unit\n- [ ] End-to-end flow works: paste → extract → create → save\n- [ ] Works with at least 2 different Ollama models\n- [ ] Gracefully handles malformed AI responses\n- [ ] <30s extraction time for typical medical content (ai-service.ts:118 timeout)",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2026-01-06T04:10:30.659Z",
      "updated": "2026-01-06T04:10:30.659Z",
      "description": "Tasks for master context"
    }
  }
}